{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b47d1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from telescope_torch import *\n",
    "import time\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a29d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange =  np.array([28,29,30,31,0,4,8,12,\n",
    "                     24,25,26,27,1,5,9,13,\n",
    "                     20,21,22,23,2,6,10,14,\n",
    "                     16,17,18,19,3,7,11,15,\n",
    "                     47,43,39,35,35,34,33,32,\n",
    "                     46,42,38,34,39,38,37,36,\n",
    "                     45,41,37,33,43,42,41,40,\n",
    "                     44,40,36,32,47,46,45,44])\n",
    "\n",
    "arrMask= np.array([1,1,1,1,1,1,1,1,\n",
    "                   1,1,1,1,1,1,1,1,\n",
    "                   1,1,1,1,1,1,1,1,\n",
    "                   1,1,1,1,1,1,1,1,\n",
    "                   1,1,1,1,0,0,0,0,\n",
    "                   1,1,1,1,0,0,0,0,\n",
    "                   1,1,1,1,0,0,0,0,\n",
    "                   1,1,1,1,0,0,0,0,])\n",
    "    \n",
    "calQMask=np.array([1,1,1,1,1,1,1,1,\n",
    "                    1,1,1,1,1,1,1,1,\n",
    "                    1,1,1,1,1,1,1,1,\n",
    "                    1,1,1,1,1,1,1,1,\n",
    "                    1,1,1,1,0,0,0,0,\n",
    "                    1,1,1,1,0,0,0,0,\n",
    "                    1,1,1,1,0,0,0,0,\n",
    "                    1,1,1,1,0,0,0,0,])\n",
    "undo = [4,12,20,28,\n",
    " 5,13,21,29,6,14,\n",
    " 22,30,7,15,23,31,\n",
    " 24,25,26,27,16,17,\n",
    " 18,19,8,9,10,11,0,\n",
    " 1,2,3,59,51,43,35,\n",
    " 58,50,42,34,57,49,\n",
    " 41,33,56,48,40,32]\n",
    "\n",
    "\n",
    "def prepInput(normData):\n",
    "    \n",
    "    shape = (1,8,8)\n",
    "    inputdata = normData[:,arrange]\n",
    "\n",
    "    inputdata[:,arrMask==0]=0  #zeros out repeated entries\n",
    "\n",
    "    shaped_data = inputdata.reshape(len(inputdata),shape[0],shape[1],shape[2])\n",
    "    \n",
    "    return shaped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e379fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types = [1,\n",
    "#          2,\n",
    "#         3]\n",
    "# max_size = 75000\n",
    "# for ty in types:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "562d2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data,cur_model):\n",
    "        cur_data = []\n",
    "        for d in data:\n",
    "            if d[1] == cur_model:\n",
    "                cur_data = d[0]\n",
    "                break\n",
    "        return cur_data\n",
    "def prepare_dataloader(ty, \n",
    "                       train_batch_size=128,\n",
    "                       eval_batch_size=256,\n",
    "                       max_size = 1000,\n",
    "                      threshold = 0,device = 'cuda'):\n",
    "    \n",
    "    print(ty)\n",
    "    \n",
    "\n",
    "    path = 'MIT_TTbar'\n",
    "    prefixed = [filename for filename in os.listdir(path) if filename.startswith(\"dt_norm\")]\n",
    "\n",
    "    data = []\n",
    "    for p in prefixed:\n",
    "        data.append([torch.load(f'{path}/{p}'),p])\n",
    "    path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models/wafer_layer_split_mip_std_1_mean_nonzero'\n",
    "\n",
    "    models = os.listdir(path)[9:]\n",
    "\n",
    "    #all past 29\n",
    "    cur_train = []\n",
    "    \n",
    "    for m in models:\n",
    "        if int(m[8]) == ty:\n",
    "            cur_train.append(m)\n",
    "    print(cur_train)\n",
    "    \n",
    "    #Calculating MSE of trained models\n",
    "    path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models/wafer_layer_split_mip_std_1_mean_nonzero'\n",
    "\n",
    "    cur_model = models[1]\n",
    "\n",
    "    \n",
    "\n",
    "    test_set = []\n",
    "    train_set = []\n",
    "    pred_set = []\n",
    "    \n",
    "    for m in cur_train:\n",
    "        cur_model = torch.load(os.path.join(path,m)).to('cuda')\n",
    "        cur_data = get_data(data,m).to('cuda')\n",
    "        if len(cur_data)< max_size:\n",
    "            train_set.append(cur_data[0:int(0.8*len(cur_data))])\n",
    "            test_data = cur_data[-int(0.19*len(cur_data)):]\n",
    "            test_set.append(test_data)\n",
    "        else:\n",
    "            train_set.append(cur_data[0:int(0.8*max_size)])\n",
    "\n",
    "            test_data = cur_data[-int(0.19*max_size):]\n",
    "            test_set.append(test_data)\n",
    "\n",
    "    \n",
    "    device = 'cuda'\n",
    "    \n",
    "    train_dt = torch.vstack(train_set).to(device)\n",
    "    test_dt = torch.vstack(test_set).to(device)    \n",
    "    mean = 0\n",
    "    std = 1\n",
    "    size_1_train =len(train_dt)\n",
    "    size_1_test = len(test_dt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loc = train_dt\n",
    "    test_loc = test_dt\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "    test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "    train_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_loc)),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_loc)),\n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train_1 = train_dt[:,0:48]\n",
    "    test_1 = test_dt[:,0:48]\n",
    "    train_1=train_1[torch.randperm(train_1.size()[0])]\n",
    "    test_1=test_1[torch.randperm(test_1.size()[0])]\n",
    "    print(len(train_1))\n",
    "\n",
    "    train_tc_sum = torch.unsqueeze(torch.sum(train_1[:,0:48],dim = 1),dim=1)\n",
    "    test_tc_sum = torch.unsqueeze(torch.sum(test_1[:,0:48],dim = 1),dim=1)\n",
    "\n",
    "\n",
    "    train_1 = train_1[torch.squeeze(train_tc_sum >threshold,dim=1)]\n",
    "    test_1 = test_1[torch.squeeze(test_tc_sum >threshold,dim=1)]\n",
    "    \n",
    "    sum_TC_test = torch.unsqueeze(torch.sum(test_1[:,0:48],dim=1),dim=1)\n",
    "\n",
    "    train_tc_sum = torch.unsqueeze(train_tc_sum[train_tc_sum >threshold],dim=1)\n",
    "    test_tc_sum = torch.unsqueeze(test_tc_sum[test_tc_sum >threshold],dim=1)\n",
    "\n",
    "    train_1_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_1),train_tc_sum),\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_1_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_1),test_tc_sum),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    return train_1_d1_flat, test_1_d1_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "022703ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['dt_norm_1_1', 'dt_norm_1_3', 'dt_norm_1_5', 'dt_norm_1_7', 'dt_norm_1_9', 'dt_norm_1_11', 'dt_norm_1_13', 'dt_norm_1_15', 'dt_norm_1_17', 'dt_norm_1_19', 'dt_norm_1_21', 'dt_norm_1_23', 'dt_norm_1_25', 'dt_norm_1_27', 'dt_norm_1_29', 'dt_norm_1_30', 'dt_norm_1_31', 'dt_norm_1_32', 'dt_norm_1_33']\n",
      "15200\n"
     ]
    }
   ],
   "source": [
    "print(ty)\n",
    "num_workers = 1 \n",
    "train_batch_size=128\n",
    "eval_batch_size=256\n",
    "max_size = 1000\n",
    "threshold = 0\n",
    "device = 'cuda'\n",
    "path = 'MIT_TTbar'\n",
    "prefixed = [filename for filename in os.listdir(path) if filename.startswith(\"dt_norm\")]\n",
    "\n",
    "data = []\n",
    "for p in prefixed:\n",
    "    data.append([torch.load(f'{path}/{p}'),p])\n",
    "path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models/wafer_layer_split_mip_std_1_mean_nonzero'\n",
    "\n",
    "models = os.listdir(path)[9:]\n",
    "\n",
    "#all past 29\n",
    "cur_train = []\n",
    "\n",
    "for m in models:\n",
    "    if int(m[8]) == ty:\n",
    "        cur_train.append(m)\n",
    "print(cur_train)\n",
    "\n",
    "#Calculating MSE of trained models\n",
    "path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models/wafer_layer_split_mip_std_1_mean_nonzero'\n",
    "\n",
    "cur_model = models[1]\n",
    "\n",
    "\n",
    "\n",
    "test_set = []\n",
    "train_set = []\n",
    "pred_set = []\n",
    "\n",
    "for m in cur_train:\n",
    "    cur_model = torch.load(os.path.join(path,m)).to('cuda')\n",
    "    cur_data = get_data(data,m).to('cuda')\n",
    "    if len(cur_data)< max_size:\n",
    "        train_set.append(cur_data[0:int(0.8*len(cur_data))])\n",
    "        test_data = cur_data[-int(0.19*len(cur_data)):]\n",
    "        test_set.append(test_data)\n",
    "    else:\n",
    "        train_set.append(cur_data[0:int(0.8*max_size)])\n",
    "\n",
    "        test_data = cur_data[-int(0.19*max_size):]\n",
    "        test_set.append(test_data)\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "train_dt = torch.vstack(train_set).to(device)\n",
    "test_dt = torch.vstack(test_set).to(device)    \n",
    "mean = 0\n",
    "std = 1\n",
    "size_1_train =len(train_dt)\n",
    "size_1_test = len(test_dt)\n",
    "\n",
    "\n",
    "\n",
    "train_loc = train_dt\n",
    "test_loc = test_dt\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "train_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_loc)),\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_1 = train_dt[:,0:48]\n",
    "test_1 = test_dt[:,0:48]\n",
    "train_1=train_1[torch.randperm(train_1.size()[0])]\n",
    "test_1=test_1[torch.randperm(test_1.size()[0])]\n",
    "print(len(train_1))\n",
    "\n",
    "train_tc_sum = torch.unsqueeze(torch.sum(train_1[:,0:48],dim = 1),dim=1)\n",
    "test_tc_sum = torch.unsqueeze(torch.sum(test_1[:,0:48],dim = 1),dim=1)\n",
    "\n",
    "\n",
    "train_1 = train_1[torch.squeeze(train_tc_sum >threshold,dim=1)]\n",
    "test_1 = test_1[torch.squeeze(test_tc_sum >threshold,dim=1)]\n",
    "\n",
    "sum_TC_test = torch.unsqueeze(torch.sum(test_1[:,0:48],dim=1),dim=1)\n",
    "\n",
    "train_tc_sum = torch.unsqueeze(train_tc_sum[train_tc_sum >threshold],dim=1)\n",
    "test_tc_sum = torch.unsqueeze(test_tc_sum[test_tc_sum >threshold],dim=1)\n",
    "\n",
    "train_1_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_1),train_tc_sum),\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_1_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_1),test_tc_sum),\n",
    "    batch_size=eval_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bff48959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    model.encoder.eval()\n",
    "    model.encoder.to(device)\n",
    "    model.encoder.device = device\n",
    "    model.decoder.eval()\n",
    "    model.decoder.device = device\n",
    "    model.decoder.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "\n",
    "    for i, (data,sum_TC) in enumerate(test_loader):\n",
    "\n",
    "        outputs = model(prepInput(data[:,0:48]),sum_TC)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, prepInput(data)).item()\n",
    "        \n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7676829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "    criterion = telescopeMSE8x8\n",
    "\n",
    "    model.to(device)\n",
    "    model.to(device)\n",
    "    model.encoder.eval()\n",
    "    model.encoder.to(device)\n",
    "    model.encoder.device = device\n",
    "    model.decoder.eval()\n",
    "    model.decoder.device = device\n",
    "    model.decoder.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learning_rate,weight_decay=0)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_loss = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "    print(\"Epoch: {:02d} Eval Loss: {:.3f}\".format(-1, eval_loss))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, (data,sum_TC) in enumerate(train_loader):\n",
    "\n",
    "            data = data.to(device)\n",
    "            sum_TC = sum_TC.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            masked_dt = prepInput(data[:,0:48])\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(masked_dt,sum_TC)\n",
    "            loss = criterion(outputs, masked_dt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() \n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        print(\"Epoch: {:03d} Train Loss: {:.3f} Eval Loss: {:.3f} \".format(epoch, train_loss, eval_loss))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7bd12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for data,sum_TC in loader:\n",
    "        data = data.to(device)\n",
    "        sum_TC = sum_TC.to(device)\n",
    "        masked_dt = prepInput(data[:,0:48])\n",
    "        _ = model(masked_dt,sum_TC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48ae3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1,8,8),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "    sum_x = t\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave\n",
    "\n",
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a39167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=10):\n",
    "\n",
    "    # The number of channels in ResNet18 is divisible by 8.\n",
    "    # This is required for fast GEMM integer matrix multiplication.\n",
    "    # model = torchvision.models.resnet18(pretrained=False)\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "\n",
    "    # We would use the pretrained ResNet18 as a feature extractor.\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Modify the last FC layer\n",
    "    # num_features = model.fc.in_features\n",
    "    # model.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "    return model\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedResNet18, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fa76314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ty = 1\n",
    "out_dir = 'models/CAE_Quant'\n",
    "model_name = f'CAE_Quant_large_{ty}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8242a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model.state_dict(), os.path.join(out_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "66a87150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['dt_norm_1_1', 'dt_norm_1_3', 'dt_norm_1_5', 'dt_norm_1_7', 'dt_norm_1_9', 'dt_norm_1_11', 'dt_norm_1_13', 'dt_norm_1_15', 'dt_norm_1_17', 'dt_norm_1_19', 'dt_norm_1_21', 'dt_norm_1_23', 'dt_norm_1_25', 'dt_norm_1_27', 'dt_norm_1_29', 'dt_norm_1_30', 'dt_norm_1_31', 'dt_norm_1_32', 'dt_norm_1_33']\n",
      "1057701\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Training QAT Model...\n",
      "Epoch: -1 Eval Loss: 3.075\n",
      "Epoch: 000 Train Loss: 0.814 Eval Loss: 0.301 \n",
      "Epoch: 001 Train Loss: 0.249 Eval Loss: 0.218 \n",
      "Epoch: 002 Train Loss: 0.208 Eval Loss: 0.198 \n",
      "Epoch: 003 Train Loss: 0.191 Eval Loss: 0.184 \n",
      "Epoch: 004 Train Loss: 0.180 Eval Loss: 0.174 \n",
      "Epoch: 005 Train Loss: 0.172 Eval Loss: 0.170 \n",
      "Epoch: 006 Train Loss: 0.167 Eval Loss: 0.165 \n",
      "Epoch: 007 Train Loss: 0.164 Eval Loss: 0.161 \n",
      "Epoch: 008 Train Loss: 0.160 Eval Loss: 0.157 \n",
      "Epoch: 009 Train Loss: 0.157 Eval Loss: 0.155 \n",
      "Epoch: 010 Train Loss: 0.154 Eval Loss: 0.153 \n",
      "Epoch: 011 Train Loss: 0.151 Eval Loss: 0.148 \n",
      "Epoch: 012 Train Loss: 0.146 Eval Loss: 0.144 \n",
      "Epoch: 013 Train Loss: 0.143 Eval Loss: 0.141 \n",
      "Epoch: 014 Train Loss: 0.141 Eval Loss: 0.139 \n",
      "Epoch: 015 Train Loss: 0.139 Eval Loss: 0.138 \n",
      "Epoch: 016 Train Loss: 0.137 Eval Loss: 0.138 \n",
      "Epoch: 017 Train Loss: 0.131 Eval Loss: 0.129 \n",
      "Epoch: 018 Train Loss: 0.127 Eval Loss: 0.126 \n",
      "Epoch: 019 Train Loss: 0.126 Eval Loss: 0.126 \n",
      "Epoch: 020 Train Loss: 0.124 Eval Loss: 0.123 \n",
      "Epoch: 021 Train Loss: 0.115 Eval Loss: 0.110 \n",
      "Epoch: 022 Train Loss: 0.110 Eval Loss: 0.109 \n",
      "Epoch: 023 Train Loss: 0.109 Eval Loss: 0.108 \n",
      "Epoch: 024 Train Loss: 0.108 Eval Loss: 0.108 \n",
      "Epoch: 025 Train Loss: 0.107 Eval Loss: 0.107 \n",
      "Epoch: 026 Train Loss: 0.106 Eval Loss: 0.106 \n",
      "Epoch: 027 Train Loss: 0.105 Eval Loss: 0.105 \n",
      "Epoch: 028 Train Loss: 0.105 Eval Loss: 0.104 \n",
      "Epoch: 029 Train Loss: 0.104 Eval Loss: 0.104 \n",
      "Epoch: 030 Train Loss: 0.104 Eval Loss: 0.103 \n",
      "Epoch: 031 Train Loss: 0.103 Eval Loss: 0.103 \n",
      "Epoch: 032 Train Loss: 0.103 Eval Loss: 0.103 \n",
      "Epoch: 033 Train Loss: 0.103 Eval Loss: 0.102 \n",
      "Epoch: 034 Train Loss: 0.102 Eval Loss: 0.102 \n",
      "Epoch: 035 Train Loss: 0.102 Eval Loss: 0.102 \n",
      "Epoch: 036 Train Loss: 0.102 Eval Loss: 0.101 \n",
      "Epoch: 037 Train Loss: 0.102 Eval Loss: 0.102 \n",
      "Epoch: 038 Train Loss: 0.101 Eval Loss: 0.101 \n",
      "Epoch: 039 Train Loss: 0.101 Eval Loss: 0.101 \n",
      "Epoch: 040 Train Loss: 0.101 Eval Loss: 0.100 \n",
      "Epoch: 041 Train Loss: 0.101 Eval Loss: 0.100 \n",
      "Epoch: 042 Train Loss: 0.100 Eval Loss: 0.099 \n",
      "Epoch: 043 Train Loss: 0.100 Eval Loss: 0.099 \n",
      "Epoch: 044 Train Loss: 0.100 Eval Loss: 0.100 \n",
      "Epoch: 045 Train Loss: 0.100 Eval Loss: 0.099 \n",
      "Epoch: 046 Train Loss: 0.099 Eval Loss: 0.099 \n",
      "Epoch: 047 Train Loss: 0.099 Eval Loss: 0.099 \n",
      "Epoch: 048 Train Loss: 0.099 Eval Loss: 0.099 \n",
      "Epoch: 049 Train Loss: 0.099 Eval Loss: 0.098 \n",
      "Epoch: 050 Train Loss: 0.099 Eval Loss: 0.098 \n",
      "Epoch: 051 Train Loss: 0.098 Eval Loss: 0.098 \n",
      "Epoch: 052 Train Loss: 0.098 Eval Loss: 0.097 \n",
      "Epoch: 053 Train Loss: 0.098 Eval Loss: 0.097 \n",
      "Epoch: 054 Train Loss: 0.098 Eval Loss: 0.098 \n",
      "Epoch: 055 Train Loss: 0.098 Eval Loss: 0.097 \n",
      "Epoch: 056 Train Loss: 0.098 Eval Loss: 0.097 \n",
      "Epoch: 057 Train Loss: 0.098 Eval Loss: 0.097 \n",
      "Epoch: 058 Train Loss: 0.097 Eval Loss: 0.098 \n",
      "Epoch: 059 Train Loss: 0.097 Eval Loss: 0.098 \n",
      "Epoch: 060 Train Loss: 0.097 Eval Loss: 0.098 \n",
      "Epoch: 061 Train Loss: 0.097 Eval Loss: 0.097 \n",
      "Epoch: 062 Train Loss: 0.097 Eval Loss: 0.099 \n",
      "Epoch: 063 Train Loss: 0.097 Eval Loss: 0.098 \n",
      "Epoch: 064 Train Loss: 0.097 Eval Loss: 0.097 \n",
      "Epoch: 065 Train Loss: 0.097 Eval Loss: 0.099 \n",
      "Epoch: 066 Train Loss: 0.096 Eval Loss: 0.099 \n",
      "Epoch: 067 Train Loss: 0.096 Eval Loss: 0.098 \n",
      "Epoch: 068 Train Loss: 0.096 Eval Loss: 0.096 \n",
      "Epoch: 069 Train Loss: 0.096 Eval Loss: 0.096 \n",
      "Epoch: 070 Train Loss: 0.096 Eval Loss: 0.096 \n",
      "Epoch: 071 Train Loss: 0.096 Eval Loss: 0.096 \n",
      "Epoch: 072 Train Loss: 0.096 Eval Loss: 0.096 \n",
      "Epoch: 073 Train Loss: 0.096 Eval Loss: 0.096 \n",
      "Epoch: 074 Train Loss: 0.096 Eval Loss: 0.095 \n",
      "Epoch: 075 Train Loss: 0.096 Eval Loss: 0.095 \n",
      "Epoch: 076 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 077 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 078 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 079 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 080 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 081 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 082 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 083 Train Loss: 0.095 Eval Loss: 0.095 \n",
      "Epoch: 084 Train Loss: 0.095 Eval Loss: 0.094 \n",
      "Epoch: 085 Train Loss: 0.095 Eval Loss: 0.094 \n",
      "Epoch: 086 Train Loss: 0.095 Eval Loss: 0.094 \n",
      "Epoch: 087 Train Loss: 0.095 Eval Loss: 0.094 \n",
      "Epoch: 088 Train Loss: 0.094 Eval Loss: 0.094 \n",
      "Epoch: 089 Train Loss: 0.094 Eval Loss: 0.094 \n",
      "Epoch: 090 Train Loss: 0.094 Eval Loss: 0.094 \n",
      "Epoch: 091 Train Loss: 0.094 Eval Loss: 0.094 \n",
      "Epoch: 092 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 093 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 094 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 095 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 096 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 097 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 098 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 099 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 100 Train Loss: 0.094 Eval Loss: 0.093 \n",
      "Epoch: 101 Train Loss: 0.093 Eval Loss: 0.093 \n",
      "Epoch: 102 Train Loss: 0.093 Eval Loss: 0.093 \n",
      "Epoch: 103 Train Loss: 0.093 Eval Loss: 0.093 \n",
      "Epoch: 104 Train Loss: 0.093 Eval Loss: 0.093 \n",
      "Epoch: 105 Train Loss: 0.093 Eval Loss: 0.093 \n",
      "Epoch: 106 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 107 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 108 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 109 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 110 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 111 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 112 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 113 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 114 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 115 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 116 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 117 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 118 Train Loss: 0.093 Eval Loss: 0.092 \n",
      "Epoch: 119 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 120 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 121 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 122 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 123 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 124 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 125 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 126 Train Loss: 0.092 Eval Loss: 0.092 \n",
      "Epoch: 127 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 128 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 129 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 130 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 131 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 132 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 133 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 134 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 135 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 136 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 137 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 138 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 139 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 140 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 141 Train Loss: 0.092 Eval Loss: 0.091 \n",
      "Epoch: 142 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 143 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 144 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 145 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 146 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 147 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 148 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 149 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 150 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 151 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 152 Train Loss: 0.091 Eval Loss: 0.091 \n",
      "Epoch: 153 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 154 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 155 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 156 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 157 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 158 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 159 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 160 Train Loss: 0.091 Eval Loss: 0.090 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 162 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 163 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 164 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 165 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 166 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 167 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 168 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 169 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 170 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 171 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 172 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 173 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 174 Train Loss: 0.091 Eval Loss: 0.090 \n",
      "Epoch: 175 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 176 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 177 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 178 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 179 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 180 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 181 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 182 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 183 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 184 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 185 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 186 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 187 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 188 Train Loss: 0.090 Eval Loss: 0.090 \n",
      "Epoch: 189 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 190 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 191 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 192 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 193 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 194 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 195 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 196 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 197 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 198 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "Epoch: 199 Train Loss: 0.090 Eval Loss: 0.089 \n",
      "2\n",
      "['dt_norm_2_1', 'dt_norm_2_3', 'dt_norm_2_5', 'dt_norm_2_7', 'dt_norm_2_9', 'dt_norm_2_11', 'dt_norm_2_13', 'dt_norm_2_15', 'dt_norm_2_17', 'dt_norm_2_19', 'dt_norm_2_21', 'dt_norm_2_23', 'dt_norm_2_25', 'dt_norm_2_27', 'dt_norm_2_29', 'dt_norm_2_30', 'dt_norm_2_31', 'dt_norm_2_32', 'dt_norm_2_34', 'dt_norm_2_35', 'dt_norm_2_36', 'dt_norm_2_37', 'dt_norm_2_38', 'dt_norm_2_39', 'dt_norm_2_40', 'dt_norm_2_41', 'dt_norm_2_42', 'dt_norm_2_43', 'dt_norm_2_44', 'dt_norm_2_45']\n",
      "852861\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Training QAT Model...\n",
      "Epoch: -1 Eval Loss: 1.809\n",
      "Epoch: 000 Train Loss: 0.619 Eval Loss: 0.219 \n",
      "Epoch: 001 Train Loss: 0.200 Eval Loss: 0.182 \n",
      "Epoch: 002 Train Loss: 0.169 Eval Loss: 0.154 \n",
      "Epoch: 003 Train Loss: 0.144 Eval Loss: 0.130 \n",
      "Epoch: 004 Train Loss: 0.124 Eval Loss: 0.114 \n",
      "Epoch: 005 Train Loss: 0.111 Eval Loss: 0.103 \n",
      "Epoch: 006 Train Loss: 0.102 Eval Loss: 0.095 \n",
      "Epoch: 007 Train Loss: 0.095 Eval Loss: 0.089 \n",
      "Epoch: 008 Train Loss: 0.089 Eval Loss: 0.084 \n",
      "Epoch: 009 Train Loss: 0.085 Eval Loss: 0.080 \n",
      "Epoch: 010 Train Loss: 0.082 Eval Loss: 0.077 \n",
      "Epoch: 011 Train Loss: 0.080 Eval Loss: 0.075 \n",
      "Epoch: 012 Train Loss: 0.077 Eval Loss: 0.074 \n",
      "Epoch: 013 Train Loss: 0.076 Eval Loss: 0.072 \n",
      "Epoch: 014 Train Loss: 0.074 Eval Loss: 0.071 \n",
      "Epoch: 015 Train Loss: 0.074 Eval Loss: 0.070 \n",
      "Epoch: 016 Train Loss: 0.072 Eval Loss: 0.069 \n",
      "Epoch: 017 Train Loss: 0.072 Eval Loss: 0.068 \n",
      "Epoch: 018 Train Loss: 0.071 Eval Loss: 0.067 \n",
      "Epoch: 019 Train Loss: 0.070 Eval Loss: 0.066 \n",
      "Epoch: 020 Train Loss: 0.070 Eval Loss: 0.066 \n",
      "Epoch: 021 Train Loss: 0.069 Eval Loss: 0.065 \n",
      "Epoch: 022 Train Loss: 0.069 Eval Loss: 0.065 \n",
      "Epoch: 023 Train Loss: 0.068 Eval Loss: 0.064 \n",
      "Epoch: 024 Train Loss: 0.067 Eval Loss: 0.064 \n",
      "Epoch: 025 Train Loss: 0.067 Eval Loss: 0.063 \n",
      "Epoch: 026 Train Loss: 0.067 Eval Loss: 0.063 \n",
      "Epoch: 027 Train Loss: 0.065 Eval Loss: 0.062 \n",
      "Epoch: 028 Train Loss: 0.066 Eval Loss: 0.062 \n",
      "Epoch: 029 Train Loss: 0.064 Eval Loss: 0.061 \n",
      "Epoch: 030 Train Loss: 0.064 Eval Loss: 0.061 \n",
      "Epoch: 031 Train Loss: 0.063 Eval Loss: 0.060 \n",
      "Epoch: 032 Train Loss: 0.063 Eval Loss: 0.060 \n",
      "Epoch: 033 Train Loss: 0.063 Eval Loss: 0.060 \n",
      "Epoch: 034 Train Loss: 0.062 Eval Loss: 0.059 \n",
      "Epoch: 035 Train Loss: 0.062 Eval Loss: 0.059 \n",
      "Epoch: 036 Train Loss: 0.062 Eval Loss: 0.059 \n",
      "Epoch: 037 Train Loss: 0.062 Eval Loss: 0.059 \n",
      "Epoch: 038 Train Loss: 0.062 Eval Loss: 0.058 \n",
      "Epoch: 039 Train Loss: 0.061 Eval Loss: 0.058 \n",
      "Epoch: 040 Train Loss: 0.061 Eval Loss: 0.058 \n",
      "Epoch: 041 Train Loss: 0.061 Eval Loss: 0.057 \n",
      "Epoch: 042 Train Loss: 0.060 Eval Loss: 0.060 \n",
      "Epoch: 043 Train Loss: 0.060 Eval Loss: 0.057 \n",
      "Epoch: 044 Train Loss: 0.060 Eval Loss: 0.057 \n",
      "Epoch: 045 Train Loss: 0.060 Eval Loss: 0.057 \n",
      "Epoch: 046 Train Loss: 0.060 Eval Loss: 0.056 \n",
      "Epoch: 047 Train Loss: 0.059 Eval Loss: 0.056 \n",
      "Epoch: 048 Train Loss: 0.059 Eval Loss: 0.056 \n",
      "Epoch: 049 Train Loss: 0.058 Eval Loss: 0.056 \n",
      "Epoch: 050 Train Loss: 0.059 Eval Loss: 0.056 \n",
      "Epoch: 051 Train Loss: 0.058 Eval Loss: 0.056 \n",
      "Epoch: 052 Train Loss: 0.058 Eval Loss: 0.055 \n",
      "Epoch: 053 Train Loss: 0.058 Eval Loss: 0.055 \n",
      "Epoch: 054 Train Loss: 0.058 Eval Loss: 0.055 \n",
      "Epoch: 055 Train Loss: 0.058 Eval Loss: 0.055 \n",
      "Epoch: 056 Train Loss: 0.058 Eval Loss: 0.055 \n",
      "Epoch: 057 Train Loss: 0.058 Eval Loss: 0.055 \n",
      "Epoch: 058 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 059 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 060 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 061 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 062 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 063 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 064 Train Loss: 0.056 Eval Loss: 0.054 \n",
      "Epoch: 065 Train Loss: 0.057 Eval Loss: 0.054 \n",
      "Epoch: 066 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 067 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 068 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 069 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 070 Train Loss: 0.055 Eval Loss: 0.053 \n",
      "Epoch: 071 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 072 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 073 Train Loss: 0.056 Eval Loss: 0.053 \n",
      "Epoch: 074 Train Loss: 0.055 Eval Loss: 0.053 \n",
      "Epoch: 075 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 076 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 077 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 078 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 079 Train Loss: 0.056 Eval Loss: 0.052 \n",
      "Epoch: 080 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 081 Train Loss: 0.056 Eval Loss: 0.052 \n",
      "Epoch: 082 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 083 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 084 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 085 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 086 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 087 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 088 Train Loss: 0.055 Eval Loss: 0.052 \n",
      "Epoch: 089 Train Loss: 0.055 Eval Loss: 0.051 \n",
      "Epoch: 090 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 091 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 092 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 093 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 094 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 095 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 096 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 097 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 098 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 099 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 100 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 101 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 102 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 103 Train Loss: 0.053 Eval Loss: 0.051 \n",
      "Epoch: 104 Train Loss: 0.054 Eval Loss: 0.051 \n",
      "Epoch: 105 Train Loss: 0.053 Eval Loss: 0.051 \n",
      "Epoch: 106 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 107 Train Loss: 0.053 Eval Loss: 0.051 \n",
      "Epoch: 108 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 109 Train Loss: 0.054 Eval Loss: 0.050 \n",
      "Epoch: 110 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 111 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 112 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 113 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 114 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 115 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 116 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 117 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 118 Train Loss: 0.053 Eval Loss: 0.050 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 120 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 121 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 122 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 123 Train Loss: 0.052 Eval Loss: 0.050 \n",
      "Epoch: 124 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 125 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 126 Train Loss: 0.053 Eval Loss: 0.050 \n",
      "Epoch: 127 Train Loss: 0.052 Eval Loss: 0.050 \n",
      "Epoch: 128 Train Loss: 0.052 Eval Loss: 0.050 \n",
      "Epoch: 129 Train Loss: 0.052 Eval Loss: 0.050 \n",
      "Epoch: 130 Train Loss: 0.053 Eval Loss: 0.049 \n",
      "Epoch: 131 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 132 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 133 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 134 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 135 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 136 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 137 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 138 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 139 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 140 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 141 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 142 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 143 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 144 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 145 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 146 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 147 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 148 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 149 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 150 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 151 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 152 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 153 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 154 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 155 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 156 Train Loss: 0.051 Eval Loss: 0.049 \n",
      "Epoch: 157 Train Loss: 0.051 Eval Loss: 0.049 \n",
      "Epoch: 158 Train Loss: 0.051 Eval Loss: 0.049 \n",
      "Epoch: 159 Train Loss: 0.051 Eval Loss: 0.049 \n",
      "Epoch: 160 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 161 Train Loss: 0.051 Eval Loss: 0.049 \n",
      "Epoch: 162 Train Loss: 0.052 Eval Loss: 0.049 \n",
      "Epoch: 163 Train Loss: 0.051 Eval Loss: 0.049 \n",
      "Epoch: 164 Train Loss: 0.052 Eval Loss: 0.048 \n",
      "Epoch: 165 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 166 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 167 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 168 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 169 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 170 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 171 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 172 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 173 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 174 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 175 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 176 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 177 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 178 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 179 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 180 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 181 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 182 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 183 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 184 Train Loss: 0.051 Eval Loss: 0.048 \n",
      "Epoch: 185 Train Loss: 0.042 Eval Loss: 0.028 \n",
      "Epoch: 186 Train Loss: 0.030 Eval Loss: 0.026 \n",
      "Epoch: 187 Train Loss: 0.028 Eval Loss: 0.026 \n",
      "Epoch: 188 Train Loss: 0.028 Eval Loss: 0.023 \n",
      "Epoch: 189 Train Loss: 0.024 Eval Loss: 0.022 \n",
      "Epoch: 190 Train Loss: 0.023 Eval Loss: 0.022 \n",
      "Epoch: 191 Train Loss: 0.023 Eval Loss: 0.021 \n",
      "Epoch: 192 Train Loss: 0.023 Eval Loss: 0.021 \n",
      "Epoch: 193 Train Loss: 0.022 Eval Loss: 0.021 \n",
      "Epoch: 194 Train Loss: 0.022 Eval Loss: 0.021 \n",
      "Epoch: 195 Train Loss: 0.022 Eval Loss: 0.020 \n",
      "Epoch: 196 Train Loss: 0.022 Eval Loss: 0.020 \n",
      "Epoch: 197 Train Loss: 0.021 Eval Loss: 0.020 \n",
      "Epoch: 198 Train Loss: 0.021 Eval Loss: 0.020 \n",
      "Epoch: 199 Train Loss: 0.021 Eval Loss: 0.020 \n",
      "3\n",
      "['dt_norm_3_36', 'dt_norm_3_30', 'dt_norm_3_31', 'dt_norm_3_33', 'dt_norm_3_35', 'dt_norm_3_37', 'dt_norm_3_27', 'dt_norm_3_29', 'dt_norm_3_32', 'dt_norm_3_34', 'dt_norm_3_38', 'dt_norm_3_1', 'dt_norm_3_3', 'dt_norm_3_5', 'dt_norm_3_7', 'dt_norm_3_9', 'dt_norm_3_11', 'dt_norm_3_13', 'dt_norm_3_15', 'dt_norm_3_17', 'dt_norm_3_19', 'dt_norm_3_21', 'dt_norm_3_23', 'dt_norm_3_25', 'dt_norm_3_39', 'dt_norm_3_40', 'dt_norm_3_41', 'dt_norm_3_42', 'dt_norm_3_43', 'dt_norm_3_44', 'dt_norm_3_45', 'dt_norm_3_46', 'dt_norm_3_47']\n",
      "1330687\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Training QAT Model...\n",
      "Epoch: -1 Eval Loss: 7.399\n",
      "Epoch: 000 Train Loss: 2.292 Eval Loss: 0.998 \n",
      "Epoch: 001 Train Loss: 0.877 Eval Loss: 0.780 \n",
      "Epoch: 002 Train Loss: 0.690 Eval Loss: 0.601 \n",
      "Epoch: 003 Train Loss: 0.574 Eval Loss: 0.552 \n",
      "Epoch: 004 Train Loss: 0.532 Eval Loss: 0.534 \n",
      "Epoch: 005 Train Loss: 0.507 Eval Loss: 0.512 \n",
      "Epoch: 006 Train Loss: 0.489 Eval Loss: 0.478 \n",
      "Epoch: 007 Train Loss: 0.477 Eval Loss: 0.470 \n",
      "Epoch: 008 Train Loss: 0.468 Eval Loss: 0.460 \n",
      "Epoch: 009 Train Loss: 0.461 Eval Loss: 0.454 \n",
      "Epoch: 010 Train Loss: 0.456 Eval Loss: 0.454 \n",
      "Epoch: 011 Train Loss: 0.452 Eval Loss: 0.446 \n",
      "Epoch: 012 Train Loss: 0.448 Eval Loss: 0.441 \n",
      "Epoch: 013 Train Loss: 0.446 Eval Loss: 0.449 \n",
      "Epoch: 014 Train Loss: 0.442 Eval Loss: 0.479 \n",
      "Epoch: 015 Train Loss: 0.440 Eval Loss: 0.434 \n",
      "Epoch: 016 Train Loss: 0.438 Eval Loss: 0.432 \n",
      "Epoch: 017 Train Loss: 0.436 Eval Loss: 0.446 \n",
      "Epoch: 018 Train Loss: 0.435 Eval Loss: 0.431 \n",
      "Epoch: 019 Train Loss: 0.434 Eval Loss: 0.428 \n",
      "Epoch: 020 Train Loss: 0.432 Eval Loss: 0.427 \n",
      "Epoch: 021 Train Loss: 0.432 Eval Loss: 0.426 \n",
      "Epoch: 022 Train Loss: 0.430 Eval Loss: 0.425 \n",
      "Epoch: 023 Train Loss: 0.429 Eval Loss: 0.441 \n",
      "Epoch: 024 Train Loss: 0.428 Eval Loss: 0.424 \n",
      "Epoch: 025 Train Loss: 0.428 Eval Loss: 0.424 \n",
      "Epoch: 026 Train Loss: 0.426 Eval Loss: 0.439 \n",
      "Epoch: 027 Train Loss: 0.422 Eval Loss: 0.423 \n",
      "Epoch: 028 Train Loss: 0.415 Eval Loss: 0.412 \n",
      "Epoch: 029 Train Loss: 0.413 Eval Loss: 0.424 \n",
      "Epoch: 030 Train Loss: 0.412 Eval Loss: 0.408 \n",
      "Epoch: 031 Train Loss: 0.411 Eval Loss: 0.414 \n",
      "Epoch: 032 Train Loss: 0.411 Eval Loss: 0.411 \n",
      "Epoch: 033 Train Loss: 0.410 Eval Loss: 0.423 \n",
      "Epoch: 034 Train Loss: 0.410 Eval Loss: 0.410 \n",
      "Epoch: 035 Train Loss: 0.409 Eval Loss: 0.407 \n",
      "Epoch: 036 Train Loss: 0.409 Eval Loss: 0.417 \n",
      "Epoch: 037 Train Loss: 0.408 Eval Loss: 0.404 \n",
      "Epoch: 038 Train Loss: 0.408 Eval Loss: 0.411 \n",
      "Epoch: 039 Train Loss: 0.407 Eval Loss: 0.415 \n",
      "Epoch: 040 Train Loss: 0.407 Eval Loss: 0.405 \n",
      "Epoch: 041 Train Loss: 0.406 Eval Loss: 0.406 \n",
      "Epoch: 042 Train Loss: 0.406 Eval Loss: 0.421 \n",
      "Epoch: 043 Train Loss: 0.406 Eval Loss: 0.402 \n",
      "Epoch: 044 Train Loss: 0.405 Eval Loss: 0.401 \n",
      "Epoch: 045 Train Loss: 0.405 Eval Loss: 0.405 \n",
      "Epoch: 046 Train Loss: 0.404 Eval Loss: 0.403 \n",
      "Epoch: 047 Train Loss: 0.404 Eval Loss: 0.401 \n",
      "Epoch: 048 Train Loss: 0.404 Eval Loss: 0.402 \n",
      "Epoch: 049 Train Loss: 0.404 Eval Loss: 0.404 \n",
      "Epoch: 050 Train Loss: 0.403 Eval Loss: 0.406 \n",
      "Epoch: 051 Train Loss: 0.403 Eval Loss: 0.402 \n",
      "Epoch: 052 Train Loss: 0.403 Eval Loss: 0.412 \n",
      "Epoch: 053 Train Loss: 0.403 Eval Loss: 0.401 \n",
      "Epoch: 054 Train Loss: 0.402 Eval Loss: 0.402 \n",
      "Epoch: 055 Train Loss: 0.402 Eval Loss: 0.401 \n",
      "Epoch: 056 Train Loss: 0.402 Eval Loss: 0.401 \n",
      "Epoch: 057 Train Loss: 0.402 Eval Loss: 0.399 \n",
      "Epoch: 058 Train Loss: 0.402 Eval Loss: 0.399 \n",
      "Epoch: 059 Train Loss: 0.401 Eval Loss: 0.398 \n",
      "Epoch: 060 Train Loss: 0.401 Eval Loss: 0.398 \n",
      "Epoch: 061 Train Loss: 0.401 Eval Loss: 0.402 \n",
      "Epoch: 062 Train Loss: 0.401 Eval Loss: 0.399 \n",
      "Epoch: 063 Train Loss: 0.401 Eval Loss: 0.399 \n",
      "Epoch: 064 Train Loss: 0.401 Eval Loss: 0.398 \n",
      "Epoch: 065 Train Loss: 0.400 Eval Loss: 0.399 \n",
      "Epoch: 066 Train Loss: 0.400 Eval Loss: 0.398 \n",
      "Epoch: 067 Train Loss: 0.401 Eval Loss: 0.399 \n",
      "Epoch: 068 Train Loss: 0.400 Eval Loss: 0.397 \n",
      "Epoch: 069 Train Loss: 0.400 Eval Loss: 0.397 \n",
      "Epoch: 070 Train Loss: 0.400 Eval Loss: 0.397 \n",
      "Epoch: 071 Train Loss: 0.400 Eval Loss: 0.397 \n",
      "Epoch: 072 Train Loss: 0.400 Eval Loss: 0.411 \n",
      "Epoch: 073 Train Loss: 0.400 Eval Loss: 0.397 \n",
      "Epoch: 074 Train Loss: 0.400 Eval Loss: 0.401 \n",
      "Epoch: 075 Train Loss: 0.399 Eval Loss: 0.398 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 076 Train Loss: 0.399 Eval Loss: 0.397 \n",
      "Epoch: 077 Train Loss: 0.399 Eval Loss: 0.396 \n",
      "Epoch: 078 Train Loss: 0.399 Eval Loss: 0.396 \n",
      "Epoch: 079 Train Loss: 0.399 Eval Loss: 0.405 \n",
      "Epoch: 080 Train Loss: 0.399 Eval Loss: 0.399 \n",
      "Epoch: 081 Train Loss: 0.399 Eval Loss: 0.397 \n",
      "Epoch: 082 Train Loss: 0.399 Eval Loss: 0.400 \n",
      "Epoch: 083 Train Loss: 0.398 Eval Loss: 0.395 \n",
      "Epoch: 084 Train Loss: 0.399 Eval Loss: 0.404 \n",
      "Epoch: 085 Train Loss: 0.398 Eval Loss: 0.397 \n",
      "Epoch: 086 Train Loss: 0.398 Eval Loss: 0.396 \n",
      "Epoch: 087 Train Loss: 0.398 Eval Loss: 0.396 \n",
      "Epoch: 088 Train Loss: 0.398 Eval Loss: 0.396 \n",
      "Epoch: 089 Train Loss: 0.398 Eval Loss: 0.395 \n",
      "Epoch: 090 Train Loss: 0.398 Eval Loss: 0.397 \n",
      "Epoch: 091 Train Loss: 0.398 Eval Loss: 0.396 \n",
      "Epoch: 092 Train Loss: 0.398 Eval Loss: 0.396 \n",
      "Epoch: 093 Train Loss: 0.398 Eval Loss: 0.395 \n",
      "Epoch: 094 Train Loss: 0.397 Eval Loss: 0.402 \n",
      "Epoch: 095 Train Loss: 0.397 Eval Loss: 0.395 \n",
      "Epoch: 096 Train Loss: 0.397 Eval Loss: 0.396 \n",
      "Epoch: 097 Train Loss: 0.397 Eval Loss: 0.404 \n",
      "Epoch: 098 Train Loss: 0.397 Eval Loss: 0.397 \n",
      "Epoch: 099 Train Loss: 0.397 Eval Loss: 0.396 \n",
      "Epoch: 100 Train Loss: 0.397 Eval Loss: 0.396 \n",
      "Epoch: 101 Train Loss: 0.397 Eval Loss: 0.394 \n",
      "Epoch: 102 Train Loss: 0.397 Eval Loss: 0.396 \n",
      "Epoch: 103 Train Loss: 0.397 Eval Loss: 0.400 \n",
      "Epoch: 104 Train Loss: 0.397 Eval Loss: 0.399 \n",
      "Epoch: 105 Train Loss: 0.397 Eval Loss: 0.396 \n",
      "Epoch: 106 Train Loss: 0.397 Eval Loss: 0.395 \n",
      "Epoch: 107 Train Loss: 0.397 Eval Loss: 0.394 \n",
      "Epoch: 108 Train Loss: 0.397 Eval Loss: 0.397 \n",
      "Epoch: 109 Train Loss: 0.396 Eval Loss: 0.394 \n",
      "Epoch: 110 Train Loss: 0.396 Eval Loss: 0.395 \n",
      "Epoch: 111 Train Loss: 0.396 Eval Loss: 0.393 \n",
      "Epoch: 112 Train Loss: 0.396 Eval Loss: 0.394 \n",
      "Epoch: 113 Train Loss: 0.396 Eval Loss: 0.394 \n",
      "Epoch: 114 Train Loss: 0.396 Eval Loss: 0.394 \n",
      "Epoch: 115 Train Loss: 0.396 Eval Loss: 0.393 \n",
      "Epoch: 116 Train Loss: 0.396 Eval Loss: 0.393 \n",
      "Epoch: 117 Train Loss: 0.396 Eval Loss: 0.397 \n",
      "Epoch: 118 Train Loss: 0.396 Eval Loss: 0.400 \n",
      "Epoch: 119 Train Loss: 0.396 Eval Loss: 0.394 \n",
      "Epoch: 120 Train Loss: 0.396 Eval Loss: 0.394 \n",
      "Epoch: 121 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 122 Train Loss: 0.395 Eval Loss: 0.394 \n",
      "Epoch: 123 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 124 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 125 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 126 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 127 Train Loss: 0.395 Eval Loss: 0.392 \n",
      "Epoch: 128 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 129 Train Loss: 0.395 Eval Loss: 0.392 \n",
      "Epoch: 130 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 131 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 132 Train Loss: 0.395 Eval Loss: 0.393 \n",
      "Epoch: 133 Train Loss: 0.395 Eval Loss: 0.394 \n",
      "Epoch: 134 Train Loss: 0.395 Eval Loss: 0.395 \n",
      "Epoch: 135 Train Loss: 0.394 Eval Loss: 0.392 \n",
      "Epoch: 136 Train Loss: 0.393 Eval Loss: 0.389 \n",
      "Epoch: 137 Train Loss: 0.390 Eval Loss: 0.388 \n",
      "Epoch: 138 Train Loss: 0.390 Eval Loss: 0.388 \n",
      "Epoch: 139 Train Loss: 0.390 Eval Loss: 0.389 \n",
      "Epoch: 140 Train Loss: 0.390 Eval Loss: 0.388 \n",
      "Epoch: 141 Train Loss: 0.390 Eval Loss: 0.387 \n",
      "Epoch: 142 Train Loss: 0.390 Eval Loss: 0.388 \n",
      "Epoch: 143 Train Loss: 0.390 Eval Loss: 0.387 \n",
      "Epoch: 144 Train Loss: 0.390 Eval Loss: 0.388 \n",
      "Epoch: 145 Train Loss: 0.389 Eval Loss: 0.389 \n",
      "Epoch: 146 Train Loss: 0.389 Eval Loss: 0.387 \n",
      "Epoch: 147 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 148 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 149 Train Loss: 0.389 Eval Loss: 0.387 \n",
      "Epoch: 150 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 151 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 152 Train Loss: 0.389 Eval Loss: 0.389 \n",
      "Epoch: 153 Train Loss: 0.389 Eval Loss: 0.387 \n",
      "Epoch: 154 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 155 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 156 Train Loss: 0.389 Eval Loss: 0.387 \n",
      "Epoch: 157 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 158 Train Loss: 0.389 Eval Loss: 0.387 \n",
      "Epoch: 159 Train Loss: 0.389 Eval Loss: 0.389 \n",
      "Epoch: 160 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 161 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 162 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 163 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 164 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 165 Train Loss: 0.389 Eval Loss: 0.388 \n",
      "Epoch: 166 Train Loss: 0.389 Eval Loss: 0.387 \n",
      "Epoch: 167 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 168 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 169 Train Loss: 0.388 Eval Loss: 0.388 \n",
      "Epoch: 170 Train Loss: 0.388 Eval Loss: 0.388 \n",
      "Epoch: 171 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 172 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 173 Train Loss: 0.389 Eval Loss: 0.386 \n",
      "Epoch: 174 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 175 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 176 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 177 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 178 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 179 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 180 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 181 Train Loss: 0.388 Eval Loss: 0.387 \n",
      "Epoch: 182 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 183 Train Loss: 0.388 Eval Loss: 0.389 \n",
      "Epoch: 184 Train Loss: 0.388 Eval Loss: 0.385 \n",
      "Epoch: 185 Train Loss: 0.388 Eval Loss: 0.387 \n",
      "Epoch: 186 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 187 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 188 Train Loss: 0.388 Eval Loss: 0.390 \n",
      "Epoch: 189 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 190 Train Loss: 0.388 Eval Loss: 0.386 \n",
      "Epoch: 191 Train Loss: 0.388 Eval Loss: 0.385 \n",
      "Epoch: 192 Train Loss: 0.388 Eval Loss: 0.385 \n",
      "Epoch: 193 Train Loss: 0.382 Eval Loss: 0.378 \n",
      "Epoch: 194 Train Loss: 0.380 Eval Loss: 0.378 \n",
      "Epoch: 195 Train Loss: 0.380 Eval Loss: 0.378 \n",
      "Epoch: 196 Train Loss: 0.380 Eval Loss: 0.378 \n",
      "Epoch: 197 Train Loss: 0.380 Eval Loss: 0.378 \n",
      "Epoch: 198 Train Loss: 0.380 Eval Loss: 0.377 \n",
      "Epoch: 199 Train Loss: 0.380 Eval Loss: 0.379 \n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import QuantizedAE\n",
    "import CAE_encoders\n",
    "import CAE_decoders\n",
    "importlib.reload(QuantizedAE)\n",
    "lr = 5e-5\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "max_size = 70000\n",
    "\n",
    "for ty in [1,2,3]:\n",
    "    \n",
    "    out_dir = 'models/CAE_Quant'\n",
    "    model_name = f'CAE_Quant_large_{ty}'\n",
    "    \n",
    "    cuda_device = torch.device(\"cuda:0\")\n",
    "    cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "    model_dir = \"models/CAE_Quant\"\n",
    "    model_filename = \"resnet18_cifar10.pt\"\n",
    "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "\n",
    "    train_loader, test_loader = prepare_dataloader(ty,max_size = max_size,  train_batch_size=500, eval_batch_size=500)\n",
    "\n",
    "    enc = CAE_encoders.CAE_conv_encoder_arch(16)\n",
    "\n",
    "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    enc.qconfig = quantization_config\n",
    "    print(enc.qconfig)\n",
    "    torch.quantization.prepare_qat(enc, inplace=True)\n",
    "    quantized_model = QuantizedAE.QuantizedAE(enc, CAE_decoders.CAE_conv_large_decoder_arch(16))\n",
    "    # # Use training data for calibration.\n",
    "    print(\"Training QAT Model...\")\n",
    "    quantized_model.train()\n",
    "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=lr, num_epochs=num_epochs)\n",
    "    quantized_model.to(cpu_device)\n",
    "    torch.save(quantized_model.state_dict(), os.path.join(out_dir, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5ad9bbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAE_Quant_large_1  CAE_Quant_large_2  CAE_Quant_large_3\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/CAE_Quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "280f2f0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/quantization/observer.py:1204: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [104], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Using high-level static quantization wrapper\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m quantized_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print quantized model.\u001b[39;00m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:535\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    534\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 535\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    539\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:573\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    572\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 573\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[1;32m    574\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:573\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    572\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 573\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[1;32m    574\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:575\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    572\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    573\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    574\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 575\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    578\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/quantization/quantize.py:608\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    606\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:279\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    275\u001b[0m qweight \u001b[38;5;241m=\u001b[39m _quantize_weight(mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat(), weight_post_process)\n\u001b[1;32m    276\u001b[0m qlinear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(mod\u001b[38;5;241m.\u001b[39min_features,\n\u001b[1;32m    277\u001b[0m               mod\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m    278\u001b[0m               dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 279\u001b[0m \u001b[43mqlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(act_scale)\n\u001b[1;32m    281\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mzero_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_zp)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:240\u001b[0m, in \u001b[0;36mLinear.set_weight_bias\u001b[0;34m(self, w, b)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor, bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Using high-level static quantization wrapper\n",
    "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
    "# quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
    "\n",
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "quantized_model.eval()\n",
    "\n",
    "# Print quantized model.\n",
    "print(quantized_model)\n",
    "\n",
    "# Save quantized model.\n",
    "save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
    "\n",
    "# Load quantized model.\n",
    "quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
    "\n",
    "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37d85c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 109\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINT8 JIT CPU Inference Latency: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m ms / sample\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(int8_jit_cpu_inference_latency \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [29], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     quantized_model_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, quantized_model_filename)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     set_random_seeds(random_seed=random_seed)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Create an untrained model.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#model = create_model(num_classes=num_classes)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Train model.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [4], line 22\u001b[0m, in \u001b[0;36mprepare_dataloader\u001b[0;34m(ty, num_workers, train_batch_size, eval_batch_size, max_size, threshold)\u001b[0m\n\u001b[1;32m     20\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prefixed:\n\u001b[0;32m---> 22\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mp\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,p])\n\u001b[1;32m     23\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models/wafer_layer_split_mip_std_1_mean_nonzero\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m models \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(path)[\u001b[38;5;241m9\u001b[39m:]\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1130\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1100\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/serialization.py:1079\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1077\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1079\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39muntyped()\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m   1083\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1084\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ty = 1\n",
    "    random_seed = 0\n",
    "    num_classes = 10\n",
    "    cuda_device = torch.device(\"cuda:0\")\n",
    "    cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "    model_dir = \"models/CAE_Quant\"\n",
    "    model_filename = \"resnet18_cifar10.pt\"\n",
    "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "\n",
    "#     set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "    # Create an untrained model.\n",
    "    #model = create_model(num_classes=num_classes)\n",
    "\n",
    "    train_loader, test_loader = prepare_dataloader(ty, num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
    "    if False:\n",
    "        # Train model.\n",
    "        print(\"Training Model...\")\n",
    "        model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=200)\n",
    "        # Save model.\n",
    "        save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
    "        # Load a pretrained model.\n",
    "        model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
    "        # Move the model to CPU since static quantization does not support CUDA currently.\n",
    "        model.to(cpu_device)\n",
    "        # Make a copy of the model for layer fusion\n",
    "        fused_model = copy.deepcopy(model)\n",
    "\n",
    "        model.train()\n",
    "        # The model has to be switched to training mode before any layer fusion.\n",
    "        # Otherwise the quantization aware training will not work correctly.\n",
    "        fused_model.train()\n",
    "\n",
    "        # Fuse the model in place rather manually.\n",
    "        fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "        for module_name, module in fused_model.named_children():\n",
    "            if \"layer\" in module_name:\n",
    "                for basic_block_name, basic_block in module.named_children():\n",
    "                    torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "                    for sub_block_name, sub_block in basic_block.named_children():\n",
    "                        if sub_block_name == \"downsample\":\n",
    "                            torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
    "\n",
    "        # Print FP32 model.\n",
    "        print(model)\n",
    "        # Print fused model.\n",
    "        print(fused_model)\n",
    "\n",
    "        # Model and fused model should be equivalent.\n",
    "        model.eval()\n",
    "        fused_model.eval()\n",
    "        assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "        # Prepare the model for quantization aware training. This inserts observers in\n",
    "        # the model that will observe activation tensors during calibration.\n",
    "    quantized_model = QuantizedAE.QuantizedAE(CAE_encoders.CAE_conv_encoder_arch(16), CAE_decoders.CAE_conv_large_decoder_arch(16))\n",
    "    \n",
    "    prepare_dataloader\n",
    "\n",
    "    # # Use training data for calibration.\n",
    "    print(\"Training QAT Model...\")\n",
    "    quantized_model.train()\n",
    "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)\n",
    "    quantized_model.to(cpu_device)\n",
    "\n",
    "    # Using high-level static quantization wrapper\n",
    "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
    "    # quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
    "\n",
    "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "    quantized_model.eval()\n",
    "\n",
    "    # Print quantized model.\n",
    "    print(quantized_model)\n",
    "\n",
    "    # Save quantized model.\n",
    "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
    "\n",
    "    # Load quantized model.\n",
    "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
    "\n",
    "    _, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    \n",
    "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "    print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "    print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
