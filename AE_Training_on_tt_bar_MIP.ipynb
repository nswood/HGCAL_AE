{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f4bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from Naive_DAE import Naive_DAE,Dropout_DAE\n",
    "import AE_Stats\n",
    "from load_data_fn import load_data,load_data_no_filter\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import importlib\n",
    "import time\n",
    "from ae_train import *\n",
    "from losses import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee90434",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4882cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ee0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "\n",
    "arr = torch.load('MIT_TTbar/ntuple_1_greater_1_sim')\n",
    "\n",
    "if n > 0:\n",
    "    for i in range(n):\n",
    "        arr = torch.vstack([arr,torch.load(f'MIT_TTbar/ntuple_{int(n+2)}_greater_1_sim')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e35c06a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9721958 wafers with sim energy > 1 \n",
      "mean MIP is: 1.0061618816751072\n",
      "max MIP is: 633.880615234375\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(arr)} wafers with sim energy > 1 ')\n",
    "print(f'mean MIP is: {torch.mean(arr[:,0:48])}')\n",
    "print(f'max MIP is: {torch.max(arr[:,0:48])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e52171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'MIT_TTbar'\n",
    "prefixed = [filename for filename in os.listdir(path) if filename.startswith(\"dt_norm\")]\n",
    "\n",
    "data = []\n",
    "for p in prefixed:\n",
    "    data.append([torch.load(f'{path}/{p}'),p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64be236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model by taking 5% of each of the wafer/layer slices\n",
    "ratio = 0.05\n",
    "all_data = []\n",
    "for d in data: \n",
    "    cur_data = d[0]\n",
    "    all_data.append(cur_data[0:int(ratio*len(cur_data))])\n",
    "all_data = torch.vstack(all_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c2b430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "std = arr[:,0:48].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b195dc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5280, dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc0c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_1 = []\n",
    "dt_2 = []\n",
    "dt_3 = []\n",
    "\n",
    "for a in arr[0:4000000]:\n",
    "    \n",
    "    if a[-2].item() ==0.: \n",
    "        dt_1.append(a)\n",
    "    elif a[-2].item() ==1.: \n",
    "        dt_2.append(a)\n",
    "    else: \n",
    "        dt_3.append(a)\n",
    "    \n",
    "dt_1 = torch.vstack(dt_1).float()\n",
    "dt_2 = torch.vstack(dt_2).float()\n",
    "dt_3 = torch.vstack(dt_3).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e32fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440000 wafers of type 0\n",
      "mean MIP: 1.217030644416809\n",
      "std MIP: 3.12253475189209\n"
     ]
    }
   ],
   "source": [
    "dt_1 = dt_1[0:440000].float()\n",
    "print(f'{len(dt_1)} wafers of type 0')\n",
    "print(f'mean MIP: {torch.mean(dt_1[:,0:48])}')\n",
    "print(f'std MIP: {torch.std(dt_1[:,0:48])}')\n",
    "# mean_dt_1 = torch.mean(dt_1[:,0:48])\n",
    "# std_dt_1 = torch.std(dt_1[:,0:48])\n",
    "dt_1[:,0:48] = (dt_1[:,0:48])/std\n",
    "dt_1 = dt_1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cdc4ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440000 wafers of type 0\n",
      "mean MIP: 0.6976751089096069\n",
      "std MIP: 3.1896018981933594\n"
     ]
    }
   ],
   "source": [
    "dt_2 = dt_2[0:440000].float()\n",
    "print(f'{len(dt_2)} wafers of type 0')\n",
    "print(f'mean MIP: {torch.mean(dt_2[:,0:48])}')\n",
    "print(f'std MIP: {torch.std(dt_2[:,0:48])}')\n",
    "# mean_dt_2 = torch.mean(dt_2[:,0:48])\n",
    "# std_dt_2 = torch.std(dt_2[:,0:48])\n",
    "dt_2[:,0:48] = (dt_2[:,0:48])/std\n",
    "dt_2 = dt_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d685bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440000 wafers of type 0\n",
      "mean MIP: 0.6197049617767334\n",
      "std MIP: 4.525939464569092\n"
     ]
    }
   ],
   "source": [
    "dt_3 = dt_3[0:440000].float()\n",
    "print(f'{len(dt_3)} wafers of type 0')\n",
    "print(f'mean MIP: {torch.mean(dt_3[:,0:48])}')\n",
    "print(f'std MIP: {torch.std(dt_3[:,0:48])}')\n",
    "# mean_dt_3 = torch.mean(dt_3[:,0:48])\n",
    "# std_dt_3 = torch.std(dt_3[:,0:48])\n",
    "dt_3[:,0:48] = (dt_3[:,0:48])/std\n",
    "dt_3 = dt_3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb97fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_1_train =400000\n",
    "size_1_test = 40000\n",
    "train_loc = dt_1[0:size_1_train]\n",
    "test_loc = dt_1[-size_1_test:]\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "train_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_1 = dt_1[0:size_1_train,0:48]\n",
    "test_1 = dt_1[-size_1_test:,0:48]\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_1=train_1[torch.randperm(train_1.size()[0])]\n",
    "test_1=test_1[torch.randperm(test_1.size()[0])]\n",
    "train_1_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_1)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_1_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_1)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd66ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_2_train =400000\n",
    "size_2_test = 40000\n",
    "train_loc = dt_2[0:size_2_train]\n",
    "test_loc = dt_2[-size_2_test:]\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "train_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_2 = dt_2[0:size_2_train,0:48]\n",
    "test_2 = dt_2[-size_2_test:,0:48]\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_2=train_2[torch.randperm(train_2.size()[0])]\n",
    "test_2=test_2[torch.randperm(test_2.size()[0])]\n",
    "train_2_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_2)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_2_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_2)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54690927",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_3_train =400000\n",
    "size_3_test = 40000\n",
    "train_loc = dt_3[0:size_3_train]\n",
    "test_loc = dt_3[-size_3_test:]\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "train_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loc_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_loc)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_3 = dt_3[0:size_3_train,0:48]\n",
    "test_3 = dt_3[-size_3_test:,0:48]\n",
    "# train = torch.vstack([train,largest[0:1000]] )\n",
    "train_3=train_3[torch.randperm(train_3.size()[0])]\n",
    "test_3=test_3[torch.randperm(test_3.size()[0])]\n",
    "train_3_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(train_3)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_3_d1_flat = DataLoader(\n",
    "    TensorDataset(torch.Tensor(test_3)),\n",
    "    batch_size=100,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de74755",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = new_loss\n",
    "mean = 0\n",
    "std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3123c3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr 0.00035\n",
      "Epoch 0: Train 71.92153782224655, Test 37.87287902832031\n",
      "Epoch 1, lr 0.00035\n",
      "Epoch 1: Train 33.43827845859528, Test 27.53294563293457\n",
      "Epoch 2, lr 0.00035\n",
      "Epoch 2: Train 28.096705271959305, Test 25.41302490234375\n",
      "Epoch 3, lr 0.00035\n",
      "Epoch 3: Train 25.7880955657959, Test 23.385772705078125\n",
      "Epoch 4, lr 0.00035\n",
      "Epoch 4: Train 24.15454931497574, Test 23.374448776245117\n",
      "Epoch 5, lr 0.00035\n",
      "Epoch 5: Train 23.023343055725096, Test 21.255319595336914\n",
      "Epoch 6, lr 0.00035\n",
      "Epoch 6: Train 22.02134324455261, Test 21.338253021240234\n",
      "Epoch 7, lr 0.00035\n",
      "Epoch 7: Train 21.287689282655716, Test 20.532363891601562\n",
      "Epoch 8, lr 0.00035\n",
      "Epoch 8: Train 20.722652888059617, Test 20.577909469604492\n",
      "Epoch 9, lr 0.00035\n",
      "Epoch 9: Train 20.036146723031997, Test 19.95856475830078\n",
      "Epoch 10, lr 0.00035\n",
      "Epoch 10: Train 19.63224494290352, Test 18.73879623413086\n",
      "Epoch 11, lr 0.00035\n",
      "Epoch 11: Train 19.07103521323204, Test 18.628890991210938\n",
      "Epoch 12, lr 0.00035\n",
      "Epoch 12: Train 18.88171202468872, Test 17.87520980834961\n",
      "Epoch 13, lr 0.00035\n",
      "Epoch 13: Train 18.455150196552278, Test 17.681703567504883\n",
      "Epoch 14, lr 0.00035\n",
      "Epoch 14: Train 18.191695229053497, Test 17.17227554321289\n",
      "Epoch 15, lr 0.00035\n",
      "Epoch 15: Train 18.039416244983673, Test 17.538272857666016\n",
      "Epoch 16, lr 0.00035\n",
      "Epoch 16: Train 17.776114456892014, Test 17.749099731445312\n",
      "Epoch 17, lr 0.00035\n",
      "Epoch 17: Train 17.655790739297867, Test 17.321887969970703\n",
      "Epoch 18, lr 0.00035\n",
      "Epoch 18: Train 17.479311081171037, Test 16.880741119384766\n",
      "Epoch 19, lr 0.00035\n",
      "Epoch 19: Train 17.30160349392891, Test 16.934123992919922\n",
      "Epoch 20, lr 0.00035\n",
      "Epoch 20: Train 17.11321016192436, Test 16.483924865722656\n",
      "Epoch 21, lr 0.00035\n",
      "Epoch 21: Train 17.059531032562255, Test 16.616695404052734\n",
      "Epoch 22, lr 0.00035\n",
      "Epoch 22: Train 16.977455536603927, Test 16.33728790283203\n",
      "Epoch 23, lr 0.00035\n",
      "Epoch 23: Train 16.832723989725114, Test 16.1409854888916\n",
      "Epoch 24, lr 0.00035\n",
      "Epoch 24: Train 16.774669159412383, Test 16.496501922607422\n",
      "Epoch 25, lr 0.00035\n",
      "Epoch 25: Train 16.670862815380097, Test 16.402692794799805\n",
      "Epoch 26, lr 0.00035\n",
      "Epoch 26: Train 16.57687583708763, Test 16.493885040283203\n",
      "Epoch 27, lr 0.00035\n",
      "Epoch 27: Train 16.57987294793129, Test 16.263717651367188\n",
      "Epoch 28, lr 0.00035\n",
      "Epoch 28: Train 16.474894825458527, Test 15.954767227172852\n",
      "Epoch 29, lr 0.00035\n",
      "Epoch 29: Train 16.437444275379182, Test 15.834732055664062\n",
      "Epoch 30, lr 0.00035\n",
      "Epoch 30: Train 16.29458937191963, Test 16.078907012939453\n",
      "Epoch 31, lr 0.00035\n",
      "Epoch 31: Train 16.18818692159653, Test 15.86083984375\n",
      "Epoch 32, lr 0.00035\n",
      "Epoch 32: Train 15.999864727258682, Test 15.901774406433105\n",
      "Epoch 33, lr 0.00035\n",
      "Epoch 33: Train 15.799875341415405, Test 15.94929313659668\n",
      "Epoch 34, lr 0.00035\n",
      "Epoch 34: Train 15.775571558237075, Test 15.88160514831543\n",
      "Epoch 35, lr 0.00035\n",
      "Epoch 35: Train 15.73294751214981, Test 15.425529479980469\n",
      "Epoch 36, lr 0.00035\n",
      "Epoch 36: Train 15.557818168401718, Test 15.862332344055176\n",
      "Epoch 37, lr 0.00035\n",
      "Epoch 37: Train 15.502001794576644, Test 15.434673309326172\n",
      "Epoch 38, lr 0.00035\n",
      "Epoch 38: Train 15.458237110853196, Test 15.384004592895508\n",
      "Epoch 39, lr 0.00035\n",
      "Epoch 39: Train 15.404548836231232, Test 15.34608268737793\n",
      "Epoch 40, lr 0.00035\n",
      "Epoch 40: Train 15.341146471738815, Test 15.448596954345703\n",
      "Epoch 41, lr 0.00035\n",
      "Epoch 41: Train 15.308810806512833, Test 14.946510314941406\n",
      "Epoch 42, lr 0.00035\n",
      "Epoch 42: Train 15.237543231725693, Test 15.375469207763672\n",
      "Epoch 43, lr 0.00035\n",
      "Epoch 43: Train 15.198255215406418, Test 15.309557914733887\n",
      "Epoch 44, lr 0.00035\n",
      "Epoch 44: Train 15.226560150384904, Test 15.142565727233887\n",
      "Epoch 45, lr 0.00035\n",
      "Epoch 45: Train 15.135606806755066, Test 15.369121551513672\n",
      "Epoch 46, lr 0.00035\n",
      "Epoch 46: Train 15.160783453702926, Test 15.175769805908203\n",
      "Epoch 47, lr 0.00035\n",
      "Epoch 47: Train 15.024435294628143, Test 14.986865997314453\n",
      "Epoch 48, lr 0.00035\n",
      "Epoch 48: Train 15.049657971143722, Test 15.020097732543945\n",
      "Epoch 49, lr 0.00035\n",
      "Epoch 49: Train 15.028658448457717, Test 14.852087020874023\n",
      "Epoch 50, lr 0.00035\n",
      "Epoch 50: Train 14.915290804624558, Test 15.113658905029297\n",
      "Epoch 51, lr 0.00035\n",
      "Epoch 51: Train 15.065373011350632, Test 15.15742015838623\n",
      "Epoch 52, lr 0.00035\n",
      "Epoch 52: Train 14.93240605378151, Test 14.99707317352295\n",
      "Epoch 53, lr 0.00035\n",
      "Epoch 53: Train 14.913490617275238, Test 14.78437328338623\n",
      "Epoch 54, lr 0.00035\n",
      "Epoch 54: Train 14.873750289201736, Test 15.003437995910645\n",
      "Epoch 55, lr 0.00035\n",
      "Epoch 55: Train 14.909886347532272, Test 14.844182014465332\n",
      "Epoch 56, lr 0.00035\n",
      "Epoch 56: Train 14.785930344581605, Test 14.791770935058594\n",
      "Epoch 57, lr 0.00035\n",
      "Epoch 57: Train 14.838631880521774, Test 14.704726219177246\n",
      "Epoch 58, lr 0.00035\n",
      "Epoch 58: Train 14.818336612939834, Test 14.530450820922852\n",
      "Epoch 59, lr 0.00035\n",
      "Epoch 59: Train 14.774366382598878, Test 14.800617218017578\n",
      "Epoch 60, lr 0.00035\n",
      "Epoch 60: Train 14.727260479688644, Test 14.821013450622559\n",
      "Epoch 61, lr 0.00035\n",
      "Epoch 61: Train 14.707775609970092, Test 14.668241500854492\n",
      "Epoch 62, lr 0.00035\n",
      "Epoch 62: Train 14.723809360265731, Test 14.574908256530762\n",
      "Epoch 63, lr 0.00035\n",
      "Epoch 63: Train 14.70389247584343, Test 14.557916641235352\n",
      "Epoch 64, lr 0.00035\n",
      "Epoch 64: Train 14.727158201217652, Test 14.629608154296875\n",
      "Epoch 65, lr 0.00035\n",
      "Epoch 65: Train 14.673315151929856, Test 14.537050247192383\n",
      "Epoch 66, lr 0.00035\n",
      "Epoch 66: Train 14.719737731933593, Test 14.51017951965332\n",
      "Epoch 67, lr 0.00035\n",
      "Epoch 67: Train 14.663698327064514, Test 14.453399658203125\n",
      "Epoch 68, lr 0.00035\n",
      "Epoch 68: Train 14.652391186952592, Test 14.50014877319336\n",
      "Epoch 69, lr 0.00035\n",
      "Epoch 69: Train 14.60574756526947, Test 14.380340576171875\n",
      "Epoch 70, lr 0.00035\n",
      "Epoch 70: Train 14.585916381120681, Test 14.675302505493164\n",
      "Epoch 71, lr 0.00035\n",
      "Epoch 71: Train 14.604531525611877, Test 14.636859893798828\n",
      "Epoch 72, lr 0.00035\n",
      "Epoch 72: Train 14.555968513965606, Test 14.693376541137695\n",
      "Epoch 73, lr 0.00035\n",
      "Epoch 73: Train 14.535460297346114, Test 14.40302848815918\n",
      "Epoch 74, lr 0.00035\n",
      "Epoch 74: Train 14.556025084257126, Test 14.538198471069336\n",
      "Epoch 75, lr 0.00035\n",
      "Epoch 75: Train 14.528580905914307, Test 14.829540252685547\n",
      "Epoch 76, lr 0.00035\n",
      "Epoch 76: Train 14.505218107700347, Test 14.487834930419922\n",
      "Epoch 77, lr 0.00035\n",
      "Epoch 77: Train 14.512670987606048, Test 14.396549224853516\n",
      "Epoch 78, lr 0.0002625\n",
      "Epoch 78: Train 14.497452072143554, Test 14.563545227050781\n",
      "Epoch 79, lr 0.0002625\n",
      "Epoch 79: Train 13.967325482606888, Test 13.8507080078125\n",
      "Epoch 80, lr 0.0002625\n",
      "Epoch 80: Train 13.967639738559724, Test 13.95656967163086\n",
      "Epoch 81, lr 0.0002625\n",
      "Epoch 81: Train 13.968502173900605, Test 13.928167343139648\n",
      "Epoch 82, lr 0.0002625\n",
      "Epoch 82: Train 13.94677981376648, Test 13.868417739868164\n",
      "Epoch 83, lr 0.0002625\n",
      "Epoch 83: Train 13.950891261577606, Test 14.137024879455566\n",
      "Epoch 84, lr 0.0002625\n",
      "Epoch 84: Train 13.943836092710495, Test 14.064201354980469\n",
      "Epoch 85, lr 0.0002625\n",
      "Epoch 85: Train 13.913115077018738, Test 13.900546073913574\n",
      "Epoch 86, lr 0.0002625\n",
      "Epoch 86: Train 13.902626381158829, Test 13.95854663848877\n",
      "Epoch 87, lr 0.0002625\n",
      "Epoch 87: Train 13.915213721752167, Test 13.93716049194336\n",
      "Epoch 88, lr 0.0002625\n",
      "Epoch 88: Train 13.88574731373787, Test 14.022666931152344\n",
      "Epoch 89, lr 0.0002625\n",
      "Epoch 89: Train 13.887488704919814, Test 13.942752838134766\n",
      "Epoch 90, lr 0.0002625\n",
      "Epoch 90: Train 13.872590964078903, Test 13.879020690917969\n",
      "Epoch 91, lr 0.0002625\n",
      "Epoch 91: Train 13.86279484128952, Test 13.85435676574707\n",
      "Epoch 92, lr 0.0002625\n",
      "Epoch 92: Train 13.87144580411911, Test 13.779834747314453\n",
      "Epoch 93, lr 0.0002625\n",
      "Epoch 93: Train 13.854966628789901, Test 13.788623809814453\n",
      "Epoch 94, lr 0.0002625\n",
      "Epoch 94: Train 13.85224496626854, Test 13.686861038208008\n",
      "Epoch 95, lr 0.0002625\n",
      "Epoch 95: Train 13.85269224023819, Test 13.845438003540039\n",
      "Epoch 96, lr 0.0002625\n",
      "Epoch 96: Train 13.847333128213883, Test 13.873102188110352\n",
      "Epoch 97, lr 0.0002625\n",
      "Epoch 97: Train 13.846562205553054, Test 13.976828575134277\n",
      "Epoch 98, lr 0.0002625\n",
      "Epoch 98: Train 13.836645877122878, Test 13.838441848754883\n",
      "Epoch 99, lr 0.0002625\n",
      "Epoch 99: Train 13.828012676477432, Test 13.839716911315918\n",
      "Epoch 100, lr 0.0002625\n",
      "Epoch 100: Train 13.824210711240768, Test 13.850211143493652\n",
      "Epoch 101, lr 0.0002625\n",
      "Epoch 101: Train 13.821241624832153, Test 13.843679428100586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102, lr 0.0002625\n",
      "Epoch 102: Train 13.81549849176407, Test 13.842377662658691\n",
      "Epoch 103, lr 0.0002625\n",
      "Epoch 103: Train 13.81179276895523, Test 13.922008514404297\n",
      "Epoch 104, lr 0.0002625\n",
      "Epoch 104: Train 13.793210770130157, Test 13.80235481262207\n",
      "Epoch 105, lr 0.0002625\n",
      "Epoch 105: Train 13.801254521608353, Test 13.650233268737793\n",
      "Epoch 106, lr 0.0002625\n",
      "Epoch 106: Train 13.79210722398758, Test 13.744707107543945\n",
      "Epoch 107, lr 0.0002625\n",
      "Epoch 107: Train 13.79857439494133, Test 13.813979148864746\n",
      "Epoch 108, lr 0.0002625\n",
      "Epoch 108: Train 13.782887930631638, Test 13.76481819152832\n",
      "Epoch 109, lr 0.0002625\n",
      "Epoch 109: Train 13.764538625717163, Test 13.887353897094727\n",
      "Epoch 110, lr 0.0002625\n",
      "Epoch 110: Train 13.76703505897522, Test 13.836326599121094\n",
      "Epoch 111, lr 0.0002625\n",
      "Epoch 111: Train 13.752795780658722, Test 13.802047729492188\n",
      "Epoch 112, lr 0.0002625\n",
      "Epoch 112: Train 13.754935623645782, Test 13.773839950561523\n",
      "Epoch 113, lr 0.0002625\n",
      "Epoch 113: Train 13.7249323220253, Test 13.654563903808594\n",
      "Epoch 114, lr 0.0002625\n",
      "Epoch 114: Train 13.721372707128525, Test 13.797237396240234\n",
      "Epoch 115, lr 0.0002625\n",
      "Epoch 115: Train 13.729487761497497, Test 13.800966262817383\n",
      "Epoch 116, lr 0.000196875\n",
      "Epoch 116: Train 13.717382862567902, Test 13.67580795288086\n",
      "Epoch 117, lr 0.000196875\n",
      "Epoch 117: Train 13.368482029676438, Test 13.316091537475586\n",
      "Epoch 118, lr 0.000196875\n",
      "Epoch 118: Train 13.360372121334075, Test 13.299001693725586\n",
      "Epoch 119, lr 0.000196875\n",
      "Epoch 119: Train 13.329944623947144, Test 13.215485572814941\n",
      "Epoch 120, lr 0.000196875\n",
      "Epoch 120: Train 13.278362506866456, Test 13.142005920410156\n",
      "Epoch 121, lr 0.000196875\n",
      "Epoch 121: Train 13.225203430652618, Test 13.105125427246094\n",
      "Epoch 122, lr 0.000196875\n",
      "Epoch 122: Train 13.18219268131256, Test 13.054656982421875\n",
      "Epoch 123, lr 0.000196875\n",
      "Epoch 123: Train 13.147533617973327, Test 13.035682678222656\n",
      "Epoch 124, lr 0.000196875\n",
      "Epoch 124: Train 13.117935820817948, Test 12.968318939208984\n",
      "Epoch 125, lr 0.000196875\n",
      "Epoch 125: Train 13.085730291128158, Test 13.017478942871094\n",
      "Epoch 126, lr 0.000196875\n",
      "Epoch 126: Train 13.077788620948791, Test 12.983945846557617\n",
      "Epoch 127, lr 0.000196875\n",
      "Epoch 127: Train 13.043374538183212, Test 12.949357986450195\n",
      "Epoch 128, lr 0.000196875\n",
      "Epoch 128: Train 13.027768522500992, Test 12.926498413085938\n",
      "Epoch 129, lr 0.000196875\n",
      "Epoch 129: Train 13.003679718255997, Test 12.88017749786377\n",
      "Epoch 130, lr 0.000196875\n",
      "Epoch 130: Train 12.99556584906578, Test 12.867063522338867\n",
      "Epoch 131, lr 0.000196875\n",
      "Epoch 131: Train 12.978823343753815, Test 12.93447208404541\n",
      "Epoch 132, lr 0.000196875\n",
      "Epoch 132: Train 12.971709357500076, Test 12.80257511138916\n",
      "Epoch 133, lr 0.000196875\n",
      "Epoch 133: Train 12.945410220384598, Test 12.75252628326416\n",
      "Epoch 134, lr 0.000196875\n",
      "Epoch 134: Train 12.946086574077606, Test 12.843608856201172\n",
      "Epoch 135, lr 0.000196875\n",
      "Epoch 135: Train 12.923685546398163, Test 12.788713455200195\n",
      "Epoch 136, lr 0.000196875\n",
      "Epoch 136: Train 12.911914629459382, Test 12.880701065063477\n",
      "Epoch 137, lr 0.000196875\n",
      "Epoch 137: Train 12.915873654603958, Test 12.755799293518066\n",
      "Epoch 138, lr 0.000196875\n",
      "Epoch 138: Train 12.882932010173798, Test 12.765508651733398\n",
      "Epoch 139, lr 0.000196875\n",
      "Epoch 139: Train 12.884978336811066, Test 12.787842750549316\n",
      "Epoch 140, lr 0.000196875\n",
      "Epoch 140: Train 12.878581663370133, Test 12.80374526977539\n",
      "Epoch 141, lr 0.000196875\n",
      "Epoch 141: Train 12.87009653377533, Test 12.780921936035156\n",
      "Epoch 142, lr 0.000196875\n",
      "Epoch 142: Train 12.857262918949127, Test 12.719047546386719\n",
      "Epoch 143, lr 0.000196875\n",
      "Epoch 143: Train 12.851695167779923, Test 12.724985122680664\n",
      "Epoch 144, lr 0.000196875\n",
      "Epoch 144: Train 12.84000623703003, Test 12.777204513549805\n",
      "Epoch 145, lr 0.000196875\n",
      "Epoch 145: Train 12.837489838123322, Test 12.754159927368164\n",
      "Epoch 146, lr 0.000196875\n",
      "Epoch 146: Train 12.824219518661499, Test 12.804000854492188\n",
      "Epoch 147, lr 0.000196875\n",
      "Epoch 147: Train 12.815784492015839, Test 12.696723937988281\n",
      "Epoch 148, lr 0.000196875\n",
      "Epoch 148: Train 12.808811615228652, Test 12.737046241760254\n",
      "Epoch 149, lr 0.000196875\n",
      "Epoch 149: Train 12.808662194490433, Test 12.684123992919922\n",
      "Epoch 150, lr 0.000196875\n",
      "Epoch 150: Train 12.798401275396348, Test 12.711751937866211\n",
      "Epoch 151, lr 0.000196875\n",
      "Epoch 151: Train 12.795214427232743, Test 12.700980186462402\n",
      "Epoch 152, lr 0.000196875\n",
      "Epoch 152: Train 12.78872639131546, Test 12.719701766967773\n",
      "Epoch 153, lr 0.000196875\n",
      "Epoch 153: Train 12.778766721487045, Test 12.675774574279785\n",
      "Epoch 154, lr 0.000196875\n",
      "Epoch 154: Train 12.776669135808945, Test 12.635322570800781\n",
      "Epoch 155, lr 0.000196875\n",
      "Epoch 155: Train 12.770846295118332, Test 12.6800537109375\n",
      "Epoch 156, lr 0.000196875\n",
      "Epoch 156: Train 12.767664032936096, Test 12.673809051513672\n",
      "Epoch 157, lr 0.000196875\n",
      "Epoch 157: Train 12.756292652845383, Test 12.67912769317627\n",
      "Epoch 158, lr 0.000196875\n",
      "Epoch 158: Train 12.754450718164444, Test 12.647500991821289\n",
      "Epoch 159, lr 0.000196875\n",
      "Epoch 159: Train 12.75223433804512, Test 12.644551277160645\n",
      "Epoch 160, lr 0.000196875\n",
      "Epoch 160: Train 12.748039760351181, Test 12.678956031799316\n",
      "Epoch 161, lr 0.000196875\n",
      "Epoch 161: Train 12.746950978279115, Test 12.694302558898926\n",
      "Epoch 162, lr 0.000196875\n",
      "Epoch 162: Train 12.734405227661133, Test 12.700460433959961\n",
      "Epoch 163, lr 0.000196875\n",
      "Epoch 163: Train 12.739371073961259, Test 12.654302597045898\n",
      "Epoch 164, lr 0.000196875\n",
      "Epoch 164: Train 12.728296524524689, Test 12.614459991455078\n",
      "Epoch 165, lr 0.000196875\n",
      "Epoch 165: Train 12.72754471707344, Test 12.648094177246094\n",
      "Epoch 166, lr 0.000196875\n",
      "Epoch 166: Train 12.725637091398239, Test 12.693760871887207\n",
      "Epoch 167, lr 0.000196875\n",
      "Epoch 167: Train 12.718137139081955, Test 12.61375617980957\n",
      "Epoch 168, lr 0.000196875\n",
      "Epoch 168: Train 12.718383942604065, Test 12.591897964477539\n",
      "Epoch 169, lr 0.000196875\n",
      "Epoch 169: Train 12.723546542167664, Test 12.58155345916748\n",
      "Epoch 170, lr 0.000196875\n",
      "Epoch 170: Train 12.706103911399842, Test 12.55801773071289\n",
      "Epoch 171, lr 0.000196875\n",
      "Epoch 171: Train 12.71106512093544, Test 12.587652206420898\n",
      "Epoch 172, lr 0.000196875\n",
      "Epoch 172: Train 12.704701336622238, Test 12.600404739379883\n",
      "Epoch 173, lr 0.000196875\n",
      "Epoch 173: Train 12.710586047172546, Test 12.575702667236328\n",
      "Epoch 174, lr 0.000196875\n",
      "Epoch 174: Train 12.69648286986351, Test 12.578354835510254\n",
      "Epoch 175, lr 0.000196875\n",
      "Epoch 175: Train 12.69211627650261, Test 12.648046493530273\n",
      "Epoch 176, lr 0.000196875\n",
      "Epoch 176: Train 12.69625390625, Test 12.610715866088867\n",
      "Epoch 177, lr 0.000196875\n",
      "Epoch 177: Train 12.689802037239074, Test 12.58978271484375\n",
      "Epoch 178, lr 0.000196875\n",
      "Epoch 178: Train 12.680916856527329, Test 12.559372901916504\n",
      "Epoch 179, lr 0.000196875\n",
      "Epoch 179: Train 12.68506802725792, Test 12.564701080322266\n",
      "Epoch 180, lr 0.000196875\n",
      "Epoch 180: Train 12.67587140083313, Test 12.613346099853516\n",
      "Epoch 181, lr 0.000196875\n",
      "Epoch 181: Train 12.67453382730484, Test 12.530851364135742\n",
      "Epoch 182, lr 0.000196875\n",
      "Epoch 182: Train 12.673278933763504, Test 12.576318740844727\n",
      "Epoch 183, lr 0.000196875\n",
      "Epoch 183: Train 12.672181304931641, Test 12.521684646606445\n",
      "Epoch 184, lr 0.000196875\n",
      "Epoch 184: Train 12.666482188701629, Test 12.527510643005371\n",
      "Epoch 185, lr 0.000196875\n",
      "Epoch 185: Train 12.657657670021058, Test 12.552925109863281\n",
      "Epoch 186, lr 0.000196875\n",
      "Epoch 186: Train 12.664398326396942, Test 12.543573379516602\n",
      "Epoch 187, lr 0.00014765625\n",
      "Epoch 187: Train 12.655260280132294, Test 12.51343059539795\n",
      "Epoch 188, lr 0.00014765625\n",
      "Epoch 188: Train 12.452081194639206, Test 12.370203018188477\n",
      "Epoch 189, lr 0.00014765625\n",
      "Epoch 189: Train 12.453107716560364, Test 12.351377487182617\n",
      "Epoch 190, lr 0.00014765625\n",
      "Epoch 190: Train 12.445231137990952, Test 12.35032844543457\n",
      "Epoch 191, lr 0.00014765625\n",
      "Epoch 191: Train 12.442625972270966, Test 12.342761993408203\n",
      "Epoch 192, lr 0.00014765625\n",
      "Epoch 192: Train 12.436950463294982, Test 12.344635009765625\n",
      "Epoch 193, lr 0.00014765625\n",
      "Epoch 193: Train 12.430977773666381, Test 12.343488693237305\n",
      "Epoch 194, lr 0.00014765625\n",
      "Epoch 194: Train 12.431970051765441, Test 12.333747863769531\n",
      "Epoch 195, lr 0.00014765625\n",
      "Epoch 195: Train 12.425706780433655, Test 12.33647346496582\n",
      "Epoch 196, lr 0.00014765625\n",
      "Epoch 196: Train 12.419412887573243, Test 12.322547912597656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, lr 0.00014765625\n",
      "Epoch 197: Train 12.422227694034577, Test 12.323952674865723\n",
      "Epoch 198, lr 0.00014765625\n",
      "Epoch 198: Train 12.418656772851945, Test 12.338647842407227\n",
      "Epoch 199, lr 0.00014765625\n",
      "Epoch 199: Train 12.411706898212433, Test 12.321718215942383\n",
      "MSE 2.498997211456299\n",
      "Median 1.5364420413970947\n",
      "Standard Devitaion 2.8488047122955322\n",
      "Average std error 6.393063068389893\n",
      "Average % Error of Energy Reconstruction nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/nswood/Notebooks/AE_Dev/AE_Stats.py:156: RuntimeWarning: divide by zero encountered in log10\n",
      "  density_scatter(np.log10(pref[:,2].detach().numpy()),np.log10(pref[:,-2].detach().numpy()), bins = [50,50],s = 5, alpha = 0.75)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "autodetected range of [-inf, 1.4707489013671875] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m model_1\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     52\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model_1,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/MIP_dt_1_450_250_greater_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mAE_Stats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_all_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msize_1_test\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdt_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msize_1_test\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdt_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43msize_1_test\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/AE_Stats.py:91\u001b[0m, in \u001b[0;36mgen_all_stats\u001b[0;34m(decoded, truth, test_loc)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_all_stats\u001b[39m(decoded, truth,test_loc):\n\u001b[0;32m---> 91\u001b[0m     pref \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     gen_avg_perf(decoded, truth,test_loc)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pref\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/AE_Stats.py:156\u001b[0m, in \u001b[0;36mgenerate_stats\u001b[0;34m(decoded, truth, test_loc)\u001b[0m\n\u001b[1;32m    153\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(torch\u001b[38;5;241m.\u001b[39msum(decoded\u001b[38;5;241m-\u001b[39mtruth,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39msum_calcq)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\u001b[38;5;28mrange\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m], bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    154\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m--> 156\u001b[0m \u001b[43mdensity_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpref\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpref\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLog10 MSE Loss vs Log10 Sum CalcQ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    158\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLog10 Sum CalcQ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/AE_Stats.py:106\u001b[0m, in \u001b[0;36mdensity_scatter\u001b[0;34m(x, y, ax, sort, bins, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m :\n\u001b[1;32m    105\u001b[0m     fig , ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m--> 106\u001b[0m data , x_e, y_e \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m z \u001b[38;5;241m=\u001b[39m interpn( ( \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(x_e[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m x_e[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) , \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(y_e[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m+\u001b[39my_e[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) ) , data , np\u001b[38;5;241m.\u001b[39mvstack([x,y])\u001b[38;5;241m.\u001b[39mT , method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplinef2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, bounds_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m#To be sure to plot all data\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram2d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/numpy/lib/twodim_base.py:825\u001b[0m, in \u001b[0;36mhistogram2d\u001b[0;34m(x, y, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    823\u001b[0m     xedges \u001b[38;5;241m=\u001b[39m yedges \u001b[38;5;241m=\u001b[39m asarray(bins)\n\u001b[1;32m    824\u001b[0m     bins \u001b[38;5;241m=\u001b[39m [xedges, yedges]\n\u001b[0;32m--> 825\u001b[0m hist, edges \u001b[38;5;241m=\u001b[39m \u001b[43mhistogramdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist, edges[\u001b[38;5;241m0\u001b[39m], edges[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogramdd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/numpy/lib/histograms.py:1050\u001b[0m, in \u001b[0;36mhistogramdd\u001b[0;34m(sample, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bins[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[0;32m-> 1050\u001b[0m smin, smax \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1052\u001b[0m     n \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(bins[i])\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/numpy/lib/histograms.py:323\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmin(), a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# expand empty range to avoid divide by zero\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_edge \u001b[38;5;241m==\u001b[39m last_edge:\n",
      "\u001b[0;31mValueError\u001b[0m: autodetected range of [-inf, 1.4707489013671875] is not finite"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fine-tune autoencoder\n",
    "#batch 500\n",
    "\n",
    "model_1 = Naive_DAE([48,450,250,16]).to(device)\n",
    "# model_1 = Naive_DAE([48,150,50,16]).to(device)\n",
    "\n",
    "# model_1 = torch.load('models/dt_1_greater_0_450_250_100_dif_2').to(device)\n",
    "test = dt_1[-size_1_test:,0:48].to(device)\n",
    "lr = 0.00035\n",
    "num_epochs = 200\n",
    "\n",
    "optimizer = optim.Adam(model_1.parameters(), lr,weight_decay=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.75)\n",
    "test = dt_1[-size_1_test:,0:48]\n",
    "\n",
    "all_1_test_losses = []\n",
    "all_1_train_losses = []\n",
    "# train\n",
    "running_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i, data_list in enumerate(train_1_d1_flat):\n",
    "        model_1.train()\n",
    "        data = data_list[0]\n",
    "        v_pred = model_1(data)\n",
    "        \n",
    "#         batch_loss = loss(data[:,0:48], v_pred,epoch,std_dt_1,mean_dt_1,alpha = a) # difference between actual and reconstructed   \n",
    "        batch_loss = loss(data[:,0:48], v_pred,epoch,mean,std)\n",
    "        \n",
    "        all_1_train_losses.append(batch_loss.item())\n",
    "        losses.append(batch_loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step(batch_loss)\n",
    "    data_1_test = test\n",
    "    model_1.eval()\n",
    "    test_pred = model_1(data_1_test)\n",
    "    batch_1_test = loss(data_1_test[:,0:48], test_pred,epoch,mean,std)\n",
    "    all_1_test_losses.append(batch_1_test.item())\n",
    "    running_loss = np.mean(losses)\n",
    "    running_1est_loss = batch_1_test.item()\n",
    "    \n",
    "    print('Epoch {}, lr {}'.format(\n",
    "        epoch, optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train {running_loss}, Test {running_1est_loss}\")\n",
    "model_1.eval()\n",
    "# torch.save(model_1,'models/MIP_dt_1_450_250_greater_1')\n",
    "# AE_Stats.gen_all_stats(model_1(dt_1[-size_1_test:,0:48]).cpu(),dt_1[-size_1_test:,0:48].cpu(),dt_1[-size_1_test:].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd75b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = data_1_test*std_dt_1\n",
    "truth = truth.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64c76690",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model_1(data_1_test).cpu()*std_dt_1\n",
    "decoded = decoded.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe9cf713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr 0.00035\n",
      "Epoch 0: Train 74.85347858130932, Test 41.53546905517578\n",
      "Epoch 1, lr 0.00035\n",
      "Epoch 1: Train 44.30423974823952, Test 43.10154724121094\n",
      "Epoch 2, lr 0.00035\n",
      "Epoch 2: Train 35.11447572648525, Test 23.479976654052734\n",
      "Epoch 3, lr 0.00035\n",
      "Epoch 3: Train 23.80440046751499, Test 19.844974517822266\n",
      "Epoch 4, lr 0.00035\n",
      "Epoch 4: Train 20.68266538989544, Test 17.386760711669922\n",
      "Epoch 5, lr 0.00035\n",
      "Epoch 5: Train 17.956005610585212, Test 14.530996322631836\n",
      "Epoch 6, lr 0.00035\n",
      "Epoch 6: Train 14.63313472223282, Test 12.081674575805664\n",
      "Epoch 7, lr 0.00035\n",
      "Epoch 7: Train 14.29517177605629, Test 12.141375541687012\n",
      "Epoch 8, lr 0.00035\n",
      "Epoch 8: Train 13.584834607303142, Test 12.025175094604492\n",
      "Epoch 9, lr 0.00035\n",
      "Epoch 9: Train 13.231389334023, Test 12.581310272216797\n",
      "Epoch 10, lr 0.00035\n",
      "Epoch 10: Train 13.182960869133472, Test 12.512933731079102\n",
      "Epoch 11, lr 0.00035\n",
      "Epoch 11: Train 13.022049016416073, Test 11.366107940673828\n",
      "Epoch 12, lr 0.00035\n",
      "Epoch 12: Train 12.597792863070964, Test 12.305007934570312\n",
      "Epoch 13, lr 0.00035\n",
      "Epoch 13: Train 12.495097842216492, Test 12.006394386291504\n",
      "Epoch 14, lr 0.00035\n",
      "Epoch 14: Train 12.347748749792576, Test 11.092190742492676\n",
      "Epoch 15, lr 0.00035\n",
      "Epoch 15: Train 12.254395868360996, Test 12.011741638183594\n",
      "Epoch 16, lr 0.00035\n",
      "Epoch 16: Train 12.1572657584548, Test 11.295616149902344\n",
      "Epoch 17, lr 0.00035\n",
      "Epoch 17: Train 11.61056299817562, Test 11.158792495727539\n",
      "Epoch 18, lr 0.00035\n",
      "Epoch 18: Train 11.553620557248593, Test 11.337785720825195\n",
      "Epoch 19, lr 0.00035\n",
      "Epoch 19: Train 11.53792486834526, Test 10.55112075805664\n",
      "Epoch 20, lr 0.00035\n",
      "Epoch 20: Train 11.469874611973763, Test 10.51028060913086\n",
      "Epoch 21, lr 0.00035\n",
      "Epoch 21: Train 11.171628926753998, Test 10.680139541625977\n",
      "Epoch 22, lr 0.00035\n",
      "Epoch 22: Train 11.13415231436491, Test 11.016645431518555\n",
      "Epoch 23, lr 0.00035\n",
      "Epoch 23: Train 11.084862312853335, Test 10.503562927246094\n",
      "Epoch 24, lr 0.00035\n",
      "Epoch 24: Train 11.008669183433057, Test 10.559423446655273\n",
      "Epoch 25, lr 0.00035\n",
      "Epoch 25: Train 10.869493971824646, Test 10.767683982849121\n",
      "Epoch 26, lr 0.00035\n",
      "Epoch 26: Train 10.797461130201816, Test 10.118461608886719\n",
      "Epoch 27, lr 0.00035\n",
      "Epoch 27: Train 10.970313874542713, Test 10.341085433959961\n",
      "Epoch 28, lr 0.00035\n",
      "Epoch 28: Train 10.897119901835918, Test 10.255970001220703\n",
      "Epoch 29, lr 0.00035\n",
      "Epoch 29: Train 10.751144922792912, Test 11.227622985839844\n",
      "Epoch 30, lr 0.00035\n",
      "Epoch 30: Train 10.682712762713432, Test 9.731120109558105\n",
      "Epoch 31, lr 0.00035\n",
      "Epoch 31: Train 10.682429824590683, Test 10.925529479980469\n",
      "Epoch 32, lr 0.00035\n",
      "Epoch 32: Train 10.443955607950688, Test 10.128083229064941\n",
      "Epoch 33, lr 0.00035\n",
      "Epoch 33: Train 10.478528580069542, Test 10.469427108764648\n",
      "Epoch 34, lr 0.00035\n",
      "Epoch 34: Train 10.650038299381732, Test 9.708809852600098\n",
      "Epoch 35, lr 0.00035\n",
      "Epoch 35: Train 10.340210739016532, Test 9.622838973999023\n",
      "Epoch 36, lr 0.00035\n",
      "Epoch 36: Train 10.30698279184103, Test 9.424341201782227\n",
      "Epoch 37, lr 0.00035\n",
      "Epoch 37: Train 10.386032970964909, Test 9.938565254211426\n",
      "Epoch 38, lr 0.00035\n",
      "Epoch 38: Train 10.273559010982513, Test 10.618585586547852\n",
      "Epoch 39, lr 0.00035\n",
      "Epoch 39: Train 10.260849513947964, Test 10.009195327758789\n",
      "Epoch 40, lr 0.00035\n",
      "Epoch 40: Train 10.246915779352188, Test 9.538285255432129\n",
      "Epoch 41, lr 0.00035\n",
      "Epoch 41: Train 10.203075856387615, Test 10.059135437011719\n",
      "Epoch 42, lr 0.00035\n",
      "Epoch 42: Train 10.276483304560184, Test 10.092001914978027\n",
      "Epoch 43, lr 0.00035\n",
      "Epoch 43: Train 10.218305392682552, Test 10.407251358032227\n",
      "Epoch 44, lr 0.00035\n",
      "Epoch 44: Train 10.152405436575412, Test 9.717670440673828\n",
      "Epoch 45, lr 0.00035\n",
      "Epoch 45: Train 10.079539430141448, Test 9.403029441833496\n",
      "Epoch 46, lr 0.00035\n",
      "Epoch 46: Train 10.129791515350341, Test 10.307450294494629\n",
      "Epoch 47, lr 0.0002625\n",
      "Epoch 47: Train 10.082893773555755, Test 10.002738952636719\n",
      "Epoch 48, lr 0.0002625\n",
      "Epoch 48: Train 9.460384011983871, Test 9.273914337158203\n",
      "Epoch 49, lr 0.0002625\n",
      "Epoch 49: Train 9.57824337643385, Test 9.61085319519043\n",
      "Epoch 50, lr 0.0002625\n",
      "Epoch 50: Train 9.568174790680409, Test 9.187009811401367\n",
      "Epoch 51, lr 0.0002625\n",
      "Epoch 51: Train 9.528071931838989, Test 9.14001178741455\n",
      "Epoch 52, lr 0.0002625\n",
      "Epoch 52: Train 9.523020737767219, Test 9.414239883422852\n",
      "Epoch 53, lr 0.0002625\n",
      "Epoch 53: Train 9.543368390977383, Test 9.514755249023438\n",
      "Epoch 54, lr 0.0002625\n",
      "Epoch 54: Train 9.565114798724652, Test 9.641359329223633\n",
      "Epoch 55, lr 0.0002625\n",
      "Epoch 55: Train 9.491401652753353, Test 9.723031997680664\n",
      "Epoch 56, lr 0.0002625\n",
      "Epoch 56: Train 9.49037747335434, Test 9.07403564453125\n",
      "Epoch 57, lr 0.0002625\n",
      "Epoch 57: Train 9.485843776524067, Test 9.270549774169922\n",
      "Epoch 58, lr 0.0002625\n",
      "Epoch 58: Train 9.5003130479455, Test 9.417792320251465\n",
      "Epoch 59, lr 0.0002625\n",
      "Epoch 59: Train 9.47047973537445, Test 9.21078872680664\n",
      "Epoch 60, lr 0.0002625\n",
      "Epoch 60: Train 9.516096814632416, Test 9.181015968322754\n",
      "Epoch 61, lr 0.0002625\n",
      "Epoch 61: Train 9.429318547189236, Test 9.097365379333496\n",
      "Epoch 62, lr 0.0002625\n",
      "Epoch 62: Train 9.4341127256155, Test 9.436463356018066\n",
      "Epoch 63, lr 0.0002625\n",
      "Epoch 63: Train 9.447642713963985, Test 9.421122550964355\n",
      "Epoch 64, lr 0.0002625\n",
      "Epoch 64: Train 9.465306712985038, Test 9.013276100158691\n",
      "Epoch 65, lr 0.0002625\n",
      "Epoch 65: Train 9.420999377369881, Test 9.410307884216309\n",
      "Epoch 66, lr 0.0002625\n",
      "Epoch 66: Train 9.454206189870835, Test 9.594476699829102\n",
      "Epoch 67, lr 0.000196875\n",
      "Epoch 67: Train 9.397072332680226, Test 9.23603630065918\n",
      "Epoch 68, lr 0.000196875\n",
      "Epoch 68: Train 9.094787820518016, Test 9.015451431274414\n",
      "Epoch 69, lr 0.000196875\n",
      "Epoch 69: Train 9.095361861824989, Test 8.822917938232422\n",
      "Epoch 70, lr 0.000196875\n",
      "Epoch 70: Train 9.109867452204227, Test 9.065835952758789\n",
      "Epoch 71, lr 0.000196875\n",
      "Epoch 71: Train 9.088084671735764, Test 9.014878273010254\n",
      "Epoch 72, lr 0.000196875\n",
      "Epoch 72: Train 9.093259258329867, Test 8.860084533691406\n",
      "Epoch 73, lr 0.000196875\n",
      "Epoch 73: Train 9.068322012484074, Test 8.953644752502441\n",
      "Epoch 74, lr 0.000196875\n",
      "Epoch 74: Train 9.058443260729312, Test 9.134482383728027\n",
      "Epoch 75, lr 0.000196875\n",
      "Epoch 75: Train 9.085804819345475, Test 8.915933609008789\n",
      "Epoch 76, lr 0.000196875\n",
      "Epoch 76: Train 9.061243628382682, Test 8.976141929626465\n",
      "Epoch 77, lr 0.000196875\n",
      "Epoch 77: Train 9.054632178068161, Test 8.988422393798828\n",
      "Epoch 78, lr 0.000196875\n",
      "Epoch 78: Train 9.056104186177254, Test 8.921387672424316\n",
      "Epoch 79, lr 0.000196875\n",
      "Epoch 79: Train 9.043974769353866, Test 8.859777450561523\n",
      "Epoch 80, lr 0.000196875\n",
      "Epoch 80: Train 9.040770077228546, Test 8.920027732849121\n",
      "Epoch 81, lr 0.000196875\n",
      "Epoch 81: Train 9.050287071466446, Test 9.058168411254883\n",
      "Epoch 82, lr 0.000196875\n",
      "Epoch 82: Train 9.045966700911523, Test 9.119294166564941\n",
      "Epoch 83, lr 0.00014765625\n",
      "Epoch 83: Train 9.051380739271641, Test 9.029197692871094\n",
      "Epoch 84, lr 0.00014765625\n",
      "Epoch 84: Train 8.870503197431564, Test 8.77815055847168\n",
      "Epoch 85, lr 0.00014765625\n",
      "Epoch 85: Train 8.853801470458508, Test 8.819147109985352\n",
      "Epoch 86, lr 0.00014765625\n",
      "Epoch 86: Train 8.851137447535992, Test 8.857837677001953\n",
      "Epoch 87, lr 0.00014765625\n",
      "Epoch 87: Train 8.853024731099605, Test 8.768725395202637\n",
      "Epoch 88, lr 0.00014765625\n",
      "Epoch 88: Train 8.858619557738304, Test 8.740031242370605\n",
      "Epoch 89, lr 0.00014765625\n",
      "Epoch 89: Train 8.858797962367534, Test 8.77136516571045\n",
      "Epoch 90, lr 0.00014765625\n",
      "Epoch 90: Train 8.838515444159508, Test 8.765419006347656\n",
      "Epoch 91, lr 0.00014765625\n",
      "Epoch 91: Train 8.847325312435627, Test 8.790607452392578\n",
      "Epoch 92, lr 0.00014765625\n",
      "Epoch 92: Train 8.830129494547844, Test 8.725780487060547\n",
      "Epoch 93, lr 0.00014765625\n",
      "Epoch 93: Train 8.856007023155689, Test 8.886176109313965\n",
      "Epoch 94, lr 0.00014765625\n",
      "Epoch 94: Train 8.83712832570076, Test 8.782438278198242\n",
      "Epoch 96, lr 0.00014765625\n",
      "Epoch 96: Train 8.85060712426901, Test 8.724754333496094\n",
      "Epoch 97, lr 0.00014765625\n",
      "Epoch 97: Train 8.827113425791264, Test 8.693754196166992\n",
      "Epoch 98, lr 0.00014765625\n",
      "Epoch 98: Train 8.830846109569073, Test 8.786123275756836\n",
      "Epoch 99, lr 0.00014765625\n",
      "Epoch 99: Train 8.825591066777706, Test 8.758249282836914\n",
      "Epoch 100, lr 0.00014765625\n",
      "Epoch 100: Train 8.831978320777417, Test 8.787630081176758\n",
      "Epoch 101, lr 0.00014765625\n",
      "Epoch 101: Train 8.823227598965168, Test 8.80765151977539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102, lr 0.00014765625\n",
      "Epoch 102: Train 8.830057638704776, Test 8.754312515258789\n",
      "Epoch 103, lr 0.00014765625\n",
      "Epoch 103: Train 8.822053727805615, Test 8.740983963012695\n",
      "Epoch 104, lr 0.00014765625\n",
      "Epoch 104: Train 8.825962114274502, Test 8.770273208618164\n",
      "Epoch 105, lr 0.00014765625\n",
      "Epoch 105: Train 8.824758795678616, Test 8.710002899169922\n",
      "Epoch 106, lr 0.00014765625\n",
      "Epoch 106: Train 8.827021063804626, Test 8.717578887939453\n",
      "Epoch 107, lr 0.00014765625\n",
      "Epoch 107: Train 8.833883733272552, Test 8.752370834350586\n",
      "Epoch 108, lr 0.0001107421875\n",
      "Epoch 108: Train 8.82276924610138, Test 8.763489723205566\n",
      "Epoch 109, lr 0.0001107421875\n",
      "Epoch 109: Train 8.70056157541275, Test 8.634857177734375\n",
      "Epoch 110, lr 0.0001107421875\n",
      "Epoch 110: Train 8.701695530354977, Test 8.721878051757812\n",
      "Epoch 111, lr 0.0001107421875\n",
      "Epoch 111: Train 8.700857799828052, Test 8.64506721496582\n",
      "Epoch 112, lr 0.0001107421875\n",
      "Epoch 112: Train 8.708808480918407, Test 8.604930877685547\n",
      "Epoch 113, lr 0.0001107421875\n",
      "Epoch 113: Train 8.697212837517261, Test 8.636691093444824\n",
      "Epoch 114, lr 0.0001107421875\n",
      "Epoch 114: Train 8.69961030948162, Test 8.630390167236328\n",
      "Epoch 115, lr 0.0001107421875\n",
      "Epoch 115: Train 8.703212774336338, Test 8.625092506408691\n",
      "Epoch 116, lr 0.0001107421875\n",
      "Epoch 116: Train 8.701082060337066, Test 8.612871170043945\n",
      "Epoch 117, lr 0.0001107421875\n",
      "Epoch 117: Train 8.6969072214365, Test 8.6400146484375\n",
      "Epoch 118, lr 0.0001107421875\n",
      "Epoch 118: Train 8.696244265437127, Test 8.608929634094238\n",
      "Epoch 119, lr 0.0001107421875\n",
      "Epoch 119: Train 8.695518227159978, Test 8.637939453125\n",
      "Epoch 120, lr 0.0001107421875\n",
      "Epoch 120: Train 8.692714382708072, Test 8.575615882873535\n",
      "Epoch 121, lr 0.0001107421875\n",
      "Epoch 121: Train 8.695684027791023, Test 8.5906343460083\n",
      "Epoch 122, lr 0.0001107421875\n",
      "Epoch 122: Train 8.689266199827195, Test 8.620121002197266\n",
      "Epoch 123, lr 0.0001107421875\n",
      "Epoch 123: Train 8.685188497841358, Test 8.58115005493164\n",
      "Epoch 124, lr 0.0001107421875\n",
      "Epoch 124: Train 8.679782760679721, Test 8.638076782226562\n",
      "Epoch 125, lr 0.0001107421875\n",
      "Epoch 125: Train 8.687904769599438, Test 8.685201644897461\n",
      "Epoch 126, lr 0.0001107421875\n",
      "Epoch 126: Train 8.692333148360252, Test 8.572410583496094\n",
      "Epoch 127, lr 0.0001107421875\n",
      "Epoch 127: Train 8.678876314043999, Test 8.605283737182617\n",
      "Epoch 128, lr 0.0001107421875\n",
      "Epoch 128: Train 8.682962776899338, Test 8.611696243286133\n",
      "Epoch 129, lr 0.0001107421875\n",
      "Epoch 129: Train 8.682290852189064, Test 8.695479393005371\n",
      "Epoch 130, lr 0.0001107421875\n",
      "Epoch 130: Train 8.670530409514903, Test 8.602680206298828\n",
      "Epoch 131, lr 8.3056640625e-05\n",
      "Epoch 131: Train 8.681779483497143, Test 8.641838073730469\n",
      "Epoch 132, lr 8.3056640625e-05\n",
      "Epoch 132: Train 8.599252154409886, Test 8.530820846557617\n",
      "Epoch 133, lr 8.3056640625e-05\n",
      "Epoch 133: Train 8.603945035874844, Test 8.546065330505371\n",
      "Epoch 134, lr 8.3056640625e-05\n",
      "Epoch 134: Train 8.602729551911354, Test 8.533722877502441\n",
      "Epoch 135, lr 8.3056640625e-05\n",
      "Epoch 135: Train 8.595330566883087, Test 8.508868217468262\n",
      "Epoch 136, lr 8.3056640625e-05\n",
      "Epoch 136: Train 8.594410460293293, Test 8.53774642944336\n",
      "Epoch 137, lr 8.3056640625e-05\n",
      "Epoch 137: Train 8.594957721352577, Test 8.548931121826172\n",
      "Epoch 138, lr 8.3056640625e-05\n",
      "Epoch 138: Train 8.593765219926834, Test 8.517389297485352\n",
      "Epoch 139, lr 8.3056640625e-05\n",
      "Epoch 139: Train 8.590396405160426, Test 8.538396835327148\n",
      "Epoch 140, lr 8.3056640625e-05\n",
      "Epoch 140: Train 8.591179346680642, Test 8.501144409179688\n",
      "Epoch 141, lr 8.3056640625e-05\n",
      "Epoch 141: Train 8.593691118478775, Test 8.563091278076172\n",
      "Epoch 142, lr 8.3056640625e-05\n",
      "Epoch 142: Train 8.582517718970776, Test 8.534299850463867\n",
      "Epoch 143, lr 8.3056640625e-05\n",
      "Epoch 143: Train 8.589851589322091, Test 8.566991806030273\n",
      "Epoch 144, lr 8.3056640625e-05\n",
      "Epoch 144: Train 8.589698778629304, Test 8.498741149902344\n",
      "Epoch 145, lr 8.3056640625e-05\n",
      "Epoch 145: Train 8.587002203702927, Test 8.495720863342285\n",
      "Epoch 146, lr 8.3056640625e-05\n",
      "Epoch 146: Train 8.581303830206394, Test 8.548259735107422\n",
      "Epoch 147, lr 8.3056640625e-05\n",
      "Epoch 147: Train 8.58542463940382, Test 8.518974304199219\n",
      "Epoch 148, lr 8.3056640625e-05\n",
      "Epoch 148: Train 8.581532633483409, Test 8.522652626037598\n",
      "Epoch 149, lr 8.3056640625e-05\n",
      "Epoch 149: Train 8.585396827220917, Test 8.529958724975586\n",
      "Epoch 150, lr 8.3056640625e-05\n",
      "Epoch 150: Train 8.578181943833828, Test 8.517839431762695\n",
      "Epoch 151, lr 8.3056640625e-05\n",
      "Epoch 151: Train 8.583836049318313, Test 8.559208869934082\n",
      "Epoch 152, lr 8.3056640625e-05\n",
      "Epoch 152: Train 8.582436021387577, Test 8.496589660644531\n",
      "Epoch 153, lr 6.229248046875e-05\n",
      "Epoch 153: Train 8.581878284990788, Test 8.506617546081543\n",
      "Epoch 154, lr 6.229248046875e-05\n",
      "Epoch 154: Train 8.525340841531754, Test 8.480487823486328\n",
      "Epoch 155, lr 6.229248046875e-05\n",
      "Epoch 155: Train 8.531985033333301, Test 8.467914581298828\n",
      "Epoch 156, lr 6.229248046875e-05\n",
      "Epoch 156: Train 8.527276241838932, Test 8.496657371520996\n",
      "Epoch 157, lr 6.229248046875e-05\n",
      "Epoch 157: Train 8.527702510714532, Test 8.477282524108887\n",
      "Epoch 158, lr 6.229248046875e-05\n",
      "Epoch 158: Train 8.522750934660435, Test 8.457559585571289\n",
      "Epoch 159, lr 6.229248046875e-05\n",
      "Epoch 159: Train 8.520499120771884, Test 8.473502159118652\n",
      "Epoch 160, lr 6.229248046875e-05\n",
      "Epoch 160: Train 8.523505604207516, Test 8.484204292297363\n",
      "Epoch 161, lr 6.229248046875e-05\n",
      "Epoch 161: Train 8.51873107177019, Test 8.45683479309082\n",
      "Epoch 162, lr 6.229248046875e-05\n",
      "Epoch 162: Train 8.519982561945914, Test 8.477442741394043\n",
      "Epoch 163, lr 6.229248046875e-05\n",
      "Epoch 163: Train 8.517703739404679, Test 8.456756591796875\n",
      "Epoch 164, lr 6.229248046875e-05\n",
      "Epoch 164: Train 8.518552363812923, Test 8.475018501281738\n",
      "Epoch 165, lr 6.229248046875e-05\n",
      "Epoch 165: Train 8.51744226193428, Test 8.459063529968262\n",
      "Epoch 166, lr 6.229248046875e-05\n",
      "Epoch 166: Train 8.514988040626049, Test 8.465388298034668\n",
      "Epoch 167, lr 6.229248046875e-05\n",
      "Epoch 167: Train 8.518157691061496, Test 8.466401100158691\n",
      "Epoch 168, lr 6.229248046875e-05\n",
      "Epoch 168: Train 8.512609280288219, Test 8.457736015319824\n",
      "Epoch 169, lr 6.229248046875e-05\n",
      "Epoch 169: Train 8.515499549865723, Test 8.448129653930664\n",
      "Epoch 170, lr 6.229248046875e-05\n",
      "Epoch 170: Train 8.51334460568428, Test 8.480828285217285\n",
      "Epoch 171, lr 6.229248046875e-05\n",
      "Epoch 171: Train 8.509296080350875, Test 8.473137855529785\n",
      "Epoch 172, lr 6.229248046875e-05\n",
      "Epoch 172: Train 8.508608064055442, Test 8.47305679321289\n",
      "Epoch 173, lr 6.229248046875e-05\n",
      "Epoch 173: Train 8.513984161376953, Test 8.477148056030273\n",
      "Epoch 174, lr 6.229248046875e-05\n",
      "Epoch 174: Train 8.50699118578434, Test 8.450458526611328\n",
      "Epoch 175, lr 6.229248046875e-05\n",
      "Epoch 175: Train 8.505753787636756, Test 8.464860916137695\n",
      "Epoch 176, lr 4.67193603515625e-05\n",
      "Epoch 176: Train 8.50976561665535, Test 8.457536697387695\n",
      "Epoch 177, lr 4.67193603515625e-05\n",
      "Epoch 177: Train 8.473675878703594, Test 8.441246032714844\n",
      "Epoch 178, lr 4.67193603515625e-05\n",
      "Epoch 178: Train 8.475793730199337, Test 8.432979583740234\n",
      "Epoch 179, lr 4.67193603515625e-05\n",
      "Epoch 179: Train 8.473275242328643, Test 8.42458724975586\n",
      "Epoch 180, lr 4.67193603515625e-05\n",
      "Epoch 180: Train 8.471191646516322, Test 8.417736053466797\n",
      "Epoch 181, lr 4.67193603515625e-05\n",
      "Epoch 181: Train 8.472165036141872, Test 8.417129516601562\n",
      "Epoch 182, lr 4.67193603515625e-05\n",
      "Epoch 182: Train 8.472848440647125, Test 8.416230201721191\n",
      "Epoch 183, lr 4.67193603515625e-05\n",
      "Epoch 183: Train 8.467363728880882, Test 8.443587303161621\n",
      "Epoch 184, lr 4.67193603515625e-05\n",
      "Epoch 184: Train 8.46931032127142, Test 8.423006057739258\n",
      "Epoch 185, lr 4.67193603515625e-05\n",
      "Epoch 185: Train 8.465040692150593, Test 8.428958892822266\n",
      "Epoch 186, lr 4.67193603515625e-05\n",
      "Epoch 186: Train 8.466215478599072, Test 8.422894477844238\n",
      "Epoch 187, lr 4.67193603515625e-05\n",
      "Epoch 187: Train 8.466692887306213, Test 8.413251876831055\n",
      "Epoch 188, lr 4.67193603515625e-05\n",
      "Epoch 188: Train 8.465611117661, Test 8.432378768920898\n",
      "Epoch 189, lr 4.67193603515625e-05\n",
      "Epoch 189: Train 8.463941559910774, Test 8.420035362243652\n",
      "Epoch 190, lr 4.67193603515625e-05\n",
      "Epoch 190: Train 8.46362630212307, Test 8.416383743286133\n",
      "Epoch 191, lr 4.67193603515625e-05\n",
      "Epoch 191: Train 8.464143935143948, Test 8.427277565002441\n",
      "Epoch 192, lr 3.503952026367187e-05\n",
      "Epoch 192: Train 8.461028284192086, Test 8.409387588500977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193, lr 3.503952026367187e-05\n",
      "Epoch 193: Train 8.441026995420456, Test 8.391069412231445\n",
      "Epoch 194, lr 3.503952026367187e-05\n",
      "Epoch 194: Train 8.44016957616806, Test 8.396137237548828\n",
      "Epoch 195, lr 3.503952026367187e-05\n",
      "Epoch 195: Train 8.439215718626976, Test 8.393369674682617\n",
      "Epoch 196, lr 3.503952026367187e-05\n",
      "Epoch 196: Train 8.438882930278778, Test 8.397035598754883\n",
      "Epoch 197, lr 3.503952026367187e-05\n",
      "Epoch 197: Train 8.438459747970104, Test 8.389370918273926\n",
      "Epoch 198, lr 3.503952026367187e-05\n",
      "Epoch 198: Train 8.437153589189053, Test 8.403812408447266\n",
      "Epoch 199, lr 3.503952026367187e-05\n",
      "Epoch 199: Train 8.436656851649284, Test 8.394289016723633\n"
     ]
    }
   ],
   "source": [
    "#fine-tune autoencoder\n",
    "#batch 500\n",
    "\n",
    "model_2 = Naive_DAE([48,450,250,16]).to(device)\n",
    "# model_2 = Naive_DAE([48,150,50,16]).to(device)\n",
    "\n",
    "# model_2 = torch.load('models/dt_2_greater_0_450_250_200_dif_2').to(device)\n",
    "test = dt_2[-size_2_test:,0:48].to(device)\n",
    "lr = 0.00035\n",
    "num_epochs = 200\n",
    "\n",
    "optimizer = optim.Adam(model_2.parameters(), lr,weight_decay=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.75)\n",
    "test = dt_2[-size_2_test:,0:48]\n",
    "\n",
    "all_2_test_losses = []\n",
    "all_2_train_losses = []\n",
    "\n",
    "\n",
    "# train\n",
    "running_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i, data_list in enumerate(train_2_d1_flat):\n",
    "        model_2.train()\n",
    "        data = data_list[0]\n",
    "        v_pred = model_2(data)\n",
    "        \n",
    "        batch_loss = loss(data[:,0:48], v_pred,epoch,mean,std)\n",
    "        \n",
    "        all_2_train_losses.append(batch_loss.item())\n",
    "        losses.append(batch_loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step(batch_loss)\n",
    "    data_2_test = test\n",
    "    model_2.eval()\n",
    "    test_pred = model_2(data_2_test)\n",
    "    batch_2_test = loss(data_2_test[:,0:48], test_pred,epoch,mean,std)\n",
    "    all_2_test_losses.append(batch_2_test.item())\n",
    "    running_loss = np.mean(losses)\n",
    "    running_2est_loss = batch_2_test.item()\n",
    "    \n",
    "    print('Epoch {}, lr {}'.format(\n",
    "        epoch, optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train {running_loss}, Test {running_2est_loss}\")\n",
    "model_2.eval()\n",
    "# torch.save(model_2,'models/MIP_dt_2_450_250_greater_2')\n",
    "# AE_Stats.gen_all_stats(model_2(dt_2[-size_2_test:,0:48]).cpu(),dt_2[-size_2_test:,0:48].cpu(),dt_2[-size_2_test:].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95aafa94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr 0.00035\n",
      "Epoch 0: Train 450.34175523018837, Test 347.72491455078125\n",
      "Epoch 1, lr 0.00035\n",
      "Epoch 1: Train 256.78701057076455, Test 139.05516052246094\n",
      "Epoch 2, lr 0.00035\n",
      "Epoch 2: Train 132.99246336817743, Test 121.65837097167969\n",
      "Epoch 3, lr 0.00035\n",
      "Epoch 3: Train 129.97856419110298, Test 114.52790832519531\n",
      "Epoch 4, lr 0.00035\n",
      "Epoch 4: Train 124.0664761172533, Test 123.86380004882812\n",
      "Epoch 5, lr 0.00035\n",
      "Epoch 5: Train 116.87638272571564, Test 114.9498062133789\n",
      "Epoch 6, lr 0.00035\n",
      "Epoch 6: Train 104.47428644859791, Test 117.03913116455078\n",
      "Epoch 7, lr 0.00035\n",
      "Epoch 7: Train 104.88721168780327, Test 92.22373962402344\n",
      "Epoch 8, lr 0.00035\n",
      "Epoch 8: Train 78.17097823882104, Test 56.43605422973633\n",
      "Epoch 9, lr 0.00035\n",
      "Epoch 9: Train 54.769245875000955, Test 44.99381637573242\n",
      "Epoch 10, lr 0.00035\n",
      "Epoch 10: Train 46.77711431515217, Test 44.589290618896484\n",
      "Epoch 11, lr 0.00035\n",
      "Epoch 11: Train 40.38074205112457, Test 34.126216888427734\n",
      "Epoch 12, lr 0.00035\n",
      "Epoch 12: Train 35.52568795776367, Test 22.74151611328125\n",
      "Epoch 13, lr 0.00035\n",
      "Epoch 13: Train 25.901511909365652, Test 25.1033878326416\n",
      "Epoch 14, lr 0.00035\n",
      "Epoch 14: Train 23.26749801290035, Test 20.721134185791016\n",
      "Epoch 15, lr 0.00035\n",
      "Epoch 15: Train 24.224132310390473, Test 26.228124618530273\n",
      "Epoch 16, lr 0.00035\n",
      "Epoch 16: Train 21.59095242857933, Test 26.716501235961914\n",
      "Epoch 17, lr 0.00035\n",
      "Epoch 17: Train 23.316400360822676, Test 24.51231575012207\n",
      "Epoch 18, lr 0.00035\n",
      "Epoch 18: Train 21.339159787774086, Test 24.123083114624023\n",
      "Epoch 19, lr 0.00035\n",
      "Epoch 19: Train 20.527569573998452, Test 18.35562515258789\n",
      "Epoch 20, lr 0.00035\n",
      "Epoch 20: Train 21.715940891385078, Test 23.493927001953125\n",
      "Epoch 21, lr 0.00035\n",
      "Epoch 21: Train 19.977998893380164, Test 20.894533157348633\n",
      "Epoch 22, lr 0.00035\n",
      "Epoch 22: Train 19.551317233800887, Test 18.842241287231445\n",
      "Epoch 23, lr 0.00035\n",
      "Epoch 23: Train 18.8879325517416, Test 15.933347702026367\n",
      "Epoch 24, lr 0.00035\n",
      "Epoch 24: Train 18.84923465311527, Test 20.995460510253906\n",
      "Epoch 25, lr 0.00035\n",
      "Epoch 25: Train 17.615180362820624, Test 19.75516128540039\n",
      "Epoch 26, lr 0.00035\n",
      "Epoch 26: Train 16.567391252040863, Test 14.196139335632324\n",
      "Epoch 27, lr 0.00035\n",
      "Epoch 27: Train 15.70471532213688, Test 15.79916000366211\n",
      "Epoch 28, lr 0.00035\n",
      "Epoch 28: Train 15.97777669352293, Test 16.59935760498047\n",
      "Epoch 29, lr 0.00035\n",
      "Epoch 29: Train 15.831176679074764, Test 13.8885498046875\n",
      "Epoch 30, lr 0.00035\n",
      "Epoch 30: Train 15.471168502926826, Test 16.861770629882812\n",
      "Epoch 31, lr 0.00035\n",
      "Epoch 31: Train 15.97894093978405, Test 18.10822868347168\n",
      "Epoch 32, lr 0.00035\n",
      "Epoch 32: Train 15.05051996088028, Test 19.5162410736084\n",
      "Epoch 33, lr 0.00035\n",
      "Epoch 33: Train 14.950859287679195, Test 18.108301162719727\n",
      "Epoch 34, lr 0.00035\n",
      "Epoch 34: Train 14.717796425819397, Test 13.742925643920898\n",
      "Epoch 35, lr 0.00035\n",
      "Epoch 35: Train 14.975488620996476, Test 13.70913314819336\n",
      "Epoch 36, lr 0.00035\n",
      "Epoch 36: Train 14.10800009804964, Test 14.222225189208984\n",
      "Epoch 37, lr 0.00035\n",
      "Epoch 37: Train 14.35557399725914, Test 12.104640007019043\n",
      "Epoch 38, lr 0.00035\n",
      "Epoch 38: Train 14.769340061545371, Test 15.850703239440918\n",
      "Epoch 39, lr 0.00035\n",
      "Epoch 39: Train 13.821315499782562, Test 13.143078804016113\n",
      "Epoch 40, lr 0.00035\n",
      "Epoch 40: Train 13.750470916450023, Test 16.300800323486328\n",
      "Epoch 41, lr 0.00035\n",
      "Epoch 41: Train 13.811328073382377, Test 17.36168670654297\n",
      "Epoch 42, lr 0.00035\n",
      "Epoch 42: Train 14.414820072233677, Test 14.50863265991211\n",
      "Epoch 43, lr 0.00035\n",
      "Epoch 43: Train 13.40029887855053, Test 14.888323783874512\n",
      "Epoch 44, lr 0.00035\n",
      "Epoch 44: Train 13.788937209844589, Test 13.754131317138672\n",
      "Epoch 45, lr 0.00035\n",
      "Epoch 45: Train 13.296788802385331, Test 13.834107398986816\n",
      "Epoch 46, lr 0.00035\n",
      "Epoch 46: Train 13.143872731864452, Test 12.491662979125977\n",
      "Epoch 47, lr 0.00035\n",
      "Epoch 47: Train 13.618840378761291, Test 18.72772979736328\n",
      "Epoch 48, lr 0.0002625\n",
      "Epoch 48: Train 13.383185249626637, Test 17.665555953979492\n",
      "Epoch 49, lr 0.0002625\n",
      "Epoch 49: Train 10.941857804179191, Test 10.709310531616211\n",
      "Epoch 50, lr 0.0002625\n",
      "Epoch 50: Train 11.216458646893502, Test 11.649942398071289\n",
      "Epoch 51, lr 0.0002625\n",
      "Epoch 51: Train 11.338055337667464, Test 11.90641975402832\n",
      "Epoch 52, lr 0.0002625\n",
      "Epoch 52: Train 11.276010824084281, Test 12.00643253326416\n",
      "Epoch 53, lr 0.0002625\n",
      "Epoch 53: Train 11.300235651254654, Test 11.940417289733887\n",
      "Epoch 54, lr 0.0002625\n",
      "Epoch 54: Train 11.125176403522492, Test 11.443334579467773\n",
      "Epoch 55, lr 0.0002625\n",
      "Epoch 55: Train 11.155893303930759, Test 13.058843612670898\n",
      "Epoch 56, lr 0.0002625\n",
      "Epoch 56: Train 11.151805297315121, Test 12.354314804077148\n",
      "Epoch 57, lr 0.0002625\n",
      "Epoch 57: Train 11.06645502203703, Test 11.467499732971191\n",
      "Epoch 58, lr 0.0002625\n",
      "Epoch 58: Train 11.247183054924012, Test 12.165470123291016\n",
      "Epoch 59, lr 0.0002625\n",
      "Epoch 59: Train 11.093050543129443, Test 11.77623462677002\n",
      "Epoch 60, lr 0.0002625\n",
      "Epoch 60: Train 10.877149064302445, Test 12.075602531433105\n",
      "Epoch 61, lr 0.0002625\n",
      "Epoch 61: Train 10.908026125609874, Test 11.230283737182617\n",
      "Epoch 62, lr 0.0002625\n",
      "Epoch 62: Train 11.059479239404201, Test 10.3023042678833\n",
      "Epoch 63, lr 0.0002625\n",
      "Epoch 63: Train 10.93015813368559, Test 11.824960708618164\n",
      "Epoch 64, lr 0.0002625\n",
      "Epoch 64: Train 11.252225596606731, Test 12.145726203918457\n",
      "Epoch 65, lr 0.0002625\n",
      "Epoch 65: Train 10.81907702988386, Test 11.836019515991211\n",
      "Epoch 66, lr 0.0002625\n",
      "Epoch 66: Train 10.906492467045783, Test 12.430719375610352\n",
      "Epoch 67, lr 0.0002625\n",
      "Epoch 67: Train 10.967708440303802, Test 10.482133865356445\n",
      "Epoch 68, lr 0.0002625\n",
      "Epoch 68: Train 10.855955545961857, Test 12.717384338378906\n",
      "Epoch 69, lr 0.0002625\n",
      "Epoch 69: Train 10.78330101186037, Test 11.039016723632812\n",
      "Epoch 70, lr 0.0002625\n",
      "Epoch 70: Train 10.774897657990456, Test 10.287843704223633\n",
      "Epoch 71, lr 0.0002625\n",
      "Epoch 71: Train 10.73091264051199, Test 11.063535690307617\n",
      "Epoch 72, lr 0.0002625\n",
      "Epoch 72: Train 10.776725827038288, Test 11.114672660827637\n",
      "Epoch 73, lr 0.0002625\n",
      "Epoch 73: Train 10.723326510965824, Test 12.446496963500977\n",
      "Epoch 74, lr 0.0002625\n",
      "Epoch 74: Train 10.827158431887627, Test 10.59975528717041\n",
      "Epoch 75, lr 0.0002625\n",
      "Epoch 75: Train 10.687923926889896, Test 11.387292861938477\n",
      "Epoch 76, lr 0.0002625\n",
      "Epoch 76: Train 10.613920176744461, Test 10.872518539428711\n",
      "Epoch 77, lr 0.0002625\n",
      "Epoch 77: Train 10.719394634008408, Test 11.337254524230957\n",
      "Epoch 78, lr 0.000196875\n",
      "Epoch 78: Train 10.724717687129974, Test 10.492473602294922\n",
      "Epoch 79, lr 0.000196875\n",
      "Epoch 79: Train 9.585010438024998, Test 9.485258102416992\n",
      "Epoch 80, lr 0.000196875\n",
      "Epoch 80: Train 9.65449699729681, Test 9.724172592163086\n",
      "Epoch 81, lr 0.000196875\n",
      "Epoch 81: Train 9.712913994789124, Test 10.698955535888672\n",
      "Epoch 82, lr 0.000196875\n",
      "Epoch 82: Train 9.80948543202877, Test 9.653044700622559\n",
      "Epoch 83, lr 0.000196875\n",
      "Epoch 83: Train 9.620955352544785, Test 9.785423278808594\n",
      "Epoch 84, lr 0.000196875\n",
      "Epoch 84: Train 9.709445900142192, Test 10.639305114746094\n",
      "Epoch 85, lr 0.000196875\n",
      "Epoch 85: Train 9.607261363625527, Test 9.756017684936523\n",
      "Epoch 86, lr 0.000196875\n",
      "Epoch 86: Train 9.679721477031707, Test 9.727612495422363\n",
      "Epoch 87, lr 0.000196875\n",
      "Epoch 87: Train 9.757407973229885, Test 10.027870178222656\n",
      "Epoch 88, lr 0.000196875\n",
      "Epoch 88: Train 9.618118180155754, Test 10.050252914428711\n",
      "Epoch 89, lr 0.000196875\n",
      "Epoch 89: Train 9.506866821169853, Test 10.01620101928711\n",
      "Epoch 90, lr 0.000196875\n",
      "Epoch 90: Train 9.650012392938137, Test 9.04479694366455\n",
      "Epoch 91, lr 0.000196875\n",
      "Epoch 91: Train 9.600317941784859, Test 9.992670059204102\n",
      "Epoch 92, lr 0.000196875\n",
      "Epoch 92: Train 9.719432077527046, Test 9.694280624389648\n",
      "Epoch 93, lr 0.000196875\n",
      "Epoch 93: Train 9.619737827241421, Test 10.316736221313477\n",
      "Epoch 94, lr 0.000196875\n",
      "Epoch 94: Train 9.791893087923526, Test 9.94949722290039\n",
      "Epoch 95, lr 0.000196875\n",
      "Epoch 95: Train 9.499024293065071, Test 9.42538833618164\n",
      "Epoch 96, lr 0.000196875\n",
      "Epoch 96: Train 9.67609833061695, Test 10.460329055786133\n",
      "Epoch 97, lr 0.000196875\n",
      "Epoch 97: Train 9.52265990102291, Test 9.818479537963867\n",
      "Epoch 98, lr 0.000196875\n",
      "Epoch 98: Train 9.58620331299305, Test 9.680184364318848\n",
      "Epoch 99, lr 0.000196875\n",
      "Epoch 99: Train 9.572470434904098, Test 9.82193660736084\n",
      "Epoch 100, lr 0.000196875\n",
      "Epoch 100: Train 9.649133786380292, Test 9.402345657348633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, lr 0.00014765625\n",
      "Epoch 101: Train 9.509970287621021, Test 9.554611206054688\n",
      "Epoch 102, lr 0.00014765625\n",
      "Epoch 102: Train 8.936538495540619, Test 8.808542251586914\n",
      "Epoch 103, lr 0.00014765625\n",
      "Epoch 103: Train 8.936182595908642, Test 8.719893455505371\n",
      "Epoch 104, lr 0.00014765625\n",
      "Epoch 104: Train 8.951981376826764, Test 8.888189315795898\n",
      "Epoch 105, lr 0.00014765625\n",
      "Epoch 105: Train 8.946807778894902, Test 9.106748580932617\n",
      "Epoch 106, lr 0.00014765625\n",
      "Epoch 106: Train 8.998911280393601, Test 9.058899879455566\n",
      "Epoch 107, lr 0.00014765625\n",
      "Epoch 107: Train 8.883422044098378, Test 9.042335510253906\n",
      "Epoch 108, lr 0.00014765625\n",
      "Epoch 108: Train 8.925093101859092, Test 8.818687438964844\n",
      "Epoch 109, lr 0.00014765625\n",
      "Epoch 109: Train 8.917879958033561, Test 8.852718353271484\n",
      "Epoch 110, lr 0.00014765625\n",
      "Epoch 110: Train 8.958629574000835, Test 9.11441707611084\n",
      "Epoch 111, lr 0.00014765625\n",
      "Epoch 111: Train 8.944168126285076, Test 8.883227348327637\n",
      "Epoch 112, lr 0.00014765625\n",
      "Epoch 112: Train 8.923234905302525, Test 9.071181297302246\n",
      "Epoch 113, lr 0.00014765625\n",
      "Epoch 113: Train 8.891170547485352, Test 9.717330932617188\n",
      "Epoch 114, lr 0.00014765625\n",
      "Epoch 114: Train 8.939312981128692, Test 9.429475784301758\n",
      "Epoch 115, lr 0.00014765625\n",
      "Epoch 115: Train 8.891703634142875, Test 8.872156143188477\n",
      "Epoch 116, lr 0.00014765625\n",
      "Epoch 116: Train 8.947084324717522, Test 8.727188110351562\n",
      "Epoch 117, lr 0.00014765625\n",
      "Epoch 117: Train 8.887408336102963, Test 8.672956466674805\n",
      "Epoch 118, lr 0.00014765625\n",
      "Epoch 118: Train 8.885182944118977, Test 9.390691757202148\n",
      "Epoch 119, lr 0.00014765625\n",
      "Epoch 119: Train 8.90213474404812, Test 8.650139808654785\n",
      "Epoch 120, lr 0.00014765625\n",
      "Epoch 120: Train 8.969267060816287, Test 9.236597061157227\n",
      "Epoch 121, lr 0.00014765625\n",
      "Epoch 121: Train 8.87610288143158, Test 9.036352157592773\n",
      "Epoch 122, lr 0.0001107421875\n",
      "Epoch 122: Train 8.902256084024906, Test 8.895831108093262\n",
      "Epoch 123, lr 0.0001107421875\n",
      "Epoch 123: Train 8.541753162622452, Test 8.565420150756836\n",
      "Epoch 124, lr 0.0001107421875\n",
      "Epoch 124: Train 8.602311479568481, Test 8.508940696716309\n",
      "Epoch 125, lr 0.0001107421875\n",
      "Epoch 125: Train 8.534867175817489, Test 8.476448059082031\n",
      "Epoch 126, lr 0.0001107421875\n",
      "Epoch 126: Train 8.346606678843498, Test 8.392486572265625\n",
      "Epoch 127, lr 0.0001107421875\n",
      "Epoch 127: Train 8.369451936960221, Test 8.335559844970703\n",
      "Epoch 128, lr 0.0001107421875\n",
      "Epoch 128: Train 8.348814014673232, Test 8.292627334594727\n",
      "Epoch 129, lr 0.0001107421875\n",
      "Epoch 129: Train 8.374037289857865, Test 8.276603698730469\n",
      "Epoch 134, lr 0.0001107421875\n",
      "Epoch 134: Train 8.304039021968842, Test 8.406036376953125\n",
      "Epoch 135, lr 0.0001107421875\n",
      "Epoch 135: Train 8.343468030214309, Test 8.307923316955566\n",
      "Epoch 136, lr 0.0001107421875\n",
      "Epoch 136: Train 8.322147003591061, Test 8.597806930541992\n",
      "Epoch 137, lr 0.0001107421875\n",
      "Epoch 137: Train 8.317578000366687, Test 8.138118743896484\n",
      "Epoch 138, lr 0.0001107421875\n",
      "Epoch 138: Train 8.294527263760568, Test 8.2340087890625\n",
      "Epoch 139, lr 0.0001107421875\n",
      "Epoch 139: Train 8.333844713151455, Test 8.25476360321045\n",
      "Epoch 140, lr 0.0001107421875\n",
      "Epoch 140: Train 8.29875844848156, Test 8.307463645935059\n",
      "Epoch 141, lr 0.0001107421875\n",
      "Epoch 141: Train 8.337303662896156, Test 8.158428192138672\n",
      "Epoch 142, lr 0.0001107421875\n",
      "Epoch 142: Train 8.299157198607922, Test 8.15329360961914\n",
      "Epoch 143, lr 0.0001107421875\n",
      "Epoch 143: Train 8.309670909881591, Test 8.376399040222168\n",
      "Epoch 144, lr 0.0001107421875\n",
      "Epoch 144: Train 8.312292011320592, Test 8.16908073425293\n",
      "Epoch 145, lr 0.0001107421875\n",
      "Epoch 145: Train 8.332321977198124, Test 8.33504867553711\n",
      "Epoch 146, lr 0.0001107421875\n",
      "Epoch 146: Train 8.289756420850754, Test 8.1328125\n",
      "Epoch 147, lr 0.0001107421875\n",
      "Epoch 147: Train 8.298974221348763, Test 8.217355728149414\n",
      "Epoch 148, lr 0.0001107421875\n",
      "Epoch 148: Train 8.300554892599582, Test 8.236083984375\n",
      "Epoch 149, lr 0.0001107421875\n",
      "Epoch 149: Train 8.305646777272225, Test 8.166563034057617\n",
      "Epoch 150, lr 0.0001107421875\n",
      "Epoch 150: Train 8.306016659617423, Test 8.115144729614258\n",
      "Epoch 151, lr 8.3056640625e-05\n",
      "Epoch 151: Train 8.279880709826946, Test 8.341666221618652\n",
      "Epoch 152, lr 8.3056640625e-05\n",
      "Epoch 152: Train 8.08645012485981, Test 8.126266479492188\n",
      "Epoch 153, lr 8.3056640625e-05\n",
      "Epoch 153: Train 8.097568075180053, Test 8.027301788330078\n",
      "Epoch 154, lr 8.3056640625e-05\n",
      "Epoch 154: Train 8.117893542289734, Test 8.128005981445312\n",
      "Epoch 155, lr 8.3056640625e-05\n",
      "Epoch 155: Train 8.095928837180137, Test 8.098870277404785\n",
      "Epoch 156, lr 8.3056640625e-05\n",
      "Epoch 156: Train 8.1115380204916, Test 8.128978729248047\n",
      "Epoch 157, lr 8.3056640625e-05\n",
      "Epoch 157: Train 8.07798315012455, Test 7.983341217041016\n",
      "Epoch 158, lr 8.3056640625e-05\n",
      "Epoch 158: Train 8.1009198898077, Test 8.104133605957031\n",
      "Epoch 159, lr 8.3056640625e-05\n",
      "Epoch 159: Train 8.080092354297637, Test 8.172502517700195\n",
      "Epoch 160, lr 8.3056640625e-05\n",
      "Epoch 160: Train 8.095649596512319, Test 8.081877708435059\n",
      "Epoch 161, lr 8.3056640625e-05\n",
      "Epoch 161: Train 8.095588119447232, Test 8.074688911437988\n",
      "Epoch 162, lr 8.3056640625e-05\n",
      "Epoch 162: Train 8.088474372804164, Test 7.941985130310059\n",
      "Epoch 163, lr 6.229248046875e-05\n",
      "Epoch 163: Train 8.090124459385873, Test 8.0963773727417\n",
      "Epoch 164, lr 6.229248046875e-05\n",
      "Epoch 164: Train 7.950836542367935, Test 7.904213905334473\n",
      "Epoch 165, lr 6.229248046875e-05\n",
      "Epoch 165: Train 7.96821006065607, Test 7.956639289855957\n",
      "Epoch 166, lr 6.229248046875e-05\n",
      "Epoch 166: Train 7.967885202050209, Test 8.00903034210205\n",
      "Epoch 167, lr 6.229248046875e-05\n",
      "Epoch 167: Train 7.960588805198669, Test 8.016416549682617\n",
      "Epoch 168, lr 6.229248046875e-05\n",
      "Epoch 168: Train 7.962335461020469, Test 7.884579658508301\n",
      "Epoch 169, lr 6.229248046875e-05\n",
      "Epoch 169: Train 7.966437481045723, Test 7.891742706298828\n",
      "Epoch 170, lr 6.229248046875e-05\n",
      "Epoch 170: Train 7.956305088162422, Test 8.027227401733398\n",
      "Epoch 171, lr 6.229248046875e-05\n",
      "Epoch 171: Train 7.961415703475475, Test 7.906756401062012\n",
      "Epoch 172, lr 6.229248046875e-05\n",
      "Epoch 172: Train 7.941556429505348, Test 7.811295032501221\n",
      "Epoch 173, lr 6.229248046875e-05\n",
      "Epoch 173: Train 7.956155332565308, Test 7.868361949920654\n",
      "Epoch 174, lr 6.229248046875e-05\n",
      "Epoch 174: Train 7.945130679011345, Test 7.935965538024902\n",
      "Epoch 175, lr 6.229248046875e-05\n",
      "Epoch 175: Train 7.942980424404144, Test 7.866141319274902\n",
      "Epoch 176, lr 6.229248046875e-05\n",
      "Epoch 176: Train 7.941354474842548, Test 7.923070907592773\n",
      "Epoch 177, lr 6.229248046875e-05\n",
      "Epoch 177: Train 7.945004338026047, Test 7.930473327636719\n",
      "Epoch 178, lr 6.229248046875e-05\n",
      "Epoch 178: Train 7.94109845918417, Test 7.8845319747924805\n",
      "Epoch 179, lr 4.67193603515625e-05\n",
      "Epoch 179: Train 7.937235837996006, Test 7.909390449523926\n",
      "Epoch 180, lr 4.67193603515625e-05\n",
      "Epoch 180: Train 7.859095627486706, Test 7.773394584655762\n",
      "Epoch 181, lr 4.67193603515625e-05\n",
      "Epoch 181: Train 7.867976368367672, Test 7.800229072570801\n",
      "Epoch 182, lr 4.67193603515625e-05\n",
      "Epoch 182: Train 7.8637237045764925, Test 7.783188819885254\n",
      "Epoch 183, lr 4.67193603515625e-05\n",
      "Epoch 183: Train 7.859253432512284, Test 7.810432434082031\n",
      "Epoch 184, lr 4.67193603515625e-05\n",
      "Epoch 184: Train 7.858748278141022, Test 7.782445907592773\n",
      "Epoch 185, lr 4.67193603515625e-05\n",
      "Epoch 185: Train 7.860500070512295, Test 7.748378276824951\n",
      "Epoch 186, lr 4.67193603515625e-05\n",
      "Epoch 186: Train 7.855354879617691, Test 7.812725067138672\n",
      "Epoch 187, lr 4.67193603515625e-05\n",
      "Epoch 187: Train 7.8565964880585675, Test 7.8474602699279785\n",
      "Epoch 188, lr 4.67193603515625e-05\n",
      "Epoch 188: Train 7.852338985919952, Test 7.838601589202881\n",
      "Epoch 189, lr 4.67193603515625e-05\n",
      "Epoch 189: Train 7.853241902530193, Test 7.758037090301514\n",
      "Epoch 190, lr 4.67193603515625e-05\n",
      "Epoch 190: Train 7.851754233121872, Test 7.794651031494141\n",
      "Epoch 191, lr 4.67193603515625e-05\n",
      "Epoch 191: Train 7.847206603467464, Test 7.749824523925781\n",
      "Epoch 192, lr 4.67193603515625e-05\n",
      "Epoch 192: Train 7.8452172846198085, Test 7.739233016967773\n",
      "Epoch 193, lr 4.67193603515625e-05\n",
      "Epoch 193: Train 7.846786524176598, Test 7.778540134429932\n",
      "Epoch 194, lr 4.67193603515625e-05\n",
      "Epoch 194: Train 7.845175751388073, Test 7.731805801391602\n",
      "Epoch 195, lr 4.67193603515625e-05\n",
      "Epoch 195: Train 7.850499624073505, Test 7.72524356842041\n",
      "Epoch 196, lr 4.67193603515625e-05\n",
      "Epoch 196: Train 7.838799957156182, Test 7.755990982055664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, lr 4.67193603515625e-05\n",
      "Epoch 197: Train 7.847531202077866, Test 7.744824409484863\n",
      "Epoch 198, lr 4.67193603515625e-05\n",
      "Epoch 198: Train 7.8361651569604875, Test 7.78928279876709\n",
      "Epoch 199, lr 4.67193603515625e-05\n",
      "Epoch 199: Train 7.843333874106407, Test 7.880531311035156\n"
     ]
    }
   ],
   "source": [
    "#fine-tune autoencoder\n",
    "#batch 500\n",
    "\n",
    "model_3 = Naive_DAE([48,450,250,16]).to(device)\n",
    "# model_3 = Naive_DAE([48,150,50,16]).to(device)\n",
    "\n",
    "# model_3 = torch.load('models/dt_3_greater_0_450_350_300_dif_3').to(device)\n",
    "test = dt_3[-size_3_test:,0:48].to(device)\n",
    "lr = 0.00035\n",
    "num_epochs = 200\n",
    "\n",
    "optimizer = optim.Adam(model_3.parameters(), lr,weight_decay=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.75)\n",
    "test = dt_3[-size_3_test:,0:48]\n",
    "\n",
    "all_3_test_losses = []\n",
    "all_3_train_losses = []\n",
    "\n",
    "\n",
    "# train\n",
    "running_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i, data_list in enumerate(train_3_d1_flat):\n",
    "        model_3.train()\n",
    "        data = data_list[0]\n",
    "        v_pred = model_3(data)\n",
    "        \n",
    "        batch_loss = loss(data[:,0:48], v_pred,epoch,mean,std)\n",
    "        \n",
    "        all_3_train_losses.append(batch_loss.item())\n",
    "        losses.append(batch_loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step(batch_loss)\n",
    "    data_3_test = test\n",
    "    model_3.eval()\n",
    "    test_pred = model_3(data_3_test)\n",
    "    batch_3_test = loss(data_3_test[:,0:48], test_pred,epoch,mean,std)\n",
    "    all_3_test_losses.append(batch_3_test.item())\n",
    "    running_loss = np.mean(losses)\n",
    "    running_3est_loss = batch_3_test.item()\n",
    "    \n",
    "    print('Epoch {}, lr {}'.format(\n",
    "        epoch, optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train {running_loss}, Test {running_3est_loss}\")\n",
    "model_3.eval()\n",
    "# torch.save(model_3,'models/MIP_dt_3_450_250_greater_1')\n",
    "# AE_Stats.gen_all_stats(model_3(dt_3[-size_3_test:,0:48]).cpu(),dt_3[-size_3_test:,0:48].cpu(),dt_3[-size_3_test:].cpu())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
