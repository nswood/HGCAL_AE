{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999ff939",
   "metadata": {},
   "source": [
    "# AE for Application\n",
    "\n",
    "Idea is to train AE for final result, rather than simply for representation. This seems most idea when future steps are based on ML methods as we can condense entire sequence of ML models. Effectively should be based on the question of if a model performs better when it has more information to consider, but a more complex problem or the combination of limited info models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5173b7d9",
   "metadata": {},
   "source": [
    "Think I need each wafer to go through NN then combine into one batch. not that hard. redo in torch wiith new structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72b1c9",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75669356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:00:44.388990: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 11:01:05.788706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-31 11:01:05.789164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-31 11:01:05.789198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import h5py as h5\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba1209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                            train=True, \n",
    "                                       transform=transforms.Compose([torchvision.transforms.ToTensor(),\n",
    " transforms.Lambda(lambda x: torch.flatten(x))]),  \n",
    "                                           download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=False, \n",
    "                                           transform=transforms.Compose([torchvision.transforms.ToTensor(), transforms.Lambda(lambda x: torch.flatten(x))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ae0c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                            batch_size=batch_size, \n",
    "                                            shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=batch_size*100, \n",
    "                                           shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3464099",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epochs_ae = 100\n",
    "epochs_reg = 100\n",
    "lr = 0.005\n",
    "ae_lossfn = MSE_AE_Loss\n",
    "reg_lossfn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2384def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(100, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 10),\n",
    "          nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "c5acc813",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train 46.43074549015482, Test 681.864501953125\n",
      "Epoch 1: Train 27.548490771013707, Test 679.9625244140625\n",
      "Epoch 2: Train 24.0491834706928, Test 678.2036743164062\n",
      "Epoch 3: Train 22.09078292467108, Test 627.1720581054688\n",
      "Epoch 4: Train 20.948135793505617, Test 640.614990234375\n",
      "Epoch 5: Train 20.055490806921206, Test 660.2119750976562\n",
      "Epoch 6: Train 18.764941443258255, Test 646.1060791015625\n",
      "Epoch 7: Train 18.36169476295585, Test 653.8004150390625\n",
      "Epoch 8: Train 17.964126833635778, Test 702.2236938476562\n",
      "Epoch 9: Train 17.711179348959853, Test 691.6189575195312\n",
      "Epoch 10: Train 17.55449487676668, Test 687.1033325195312\n",
      "Epoch 11: Train 17.255501870492203, Test 646.8136596679688\n",
      "Epoch 12: Train 17.070397111313852, Test 640.191162109375\n",
      "Epoch 13: Train 16.94103272281476, Test 646.94775390625\n",
      "Epoch 14: Train 16.77463698980227, Test 619.9539184570312\n",
      "Epoch 15: Train 16.606524676232787, Test 645.3894653320312\n",
      "Epoch 16: Train 16.631611743376624, Test 621.5107421875\n",
      "Epoch 17: Train 16.62180358260425, Test 638.7487182617188\n",
      "Epoch 18: Train 16.489380575531158, Test 649.576171875\n",
      "Epoch 19: Train 16.40631472649266, Test 633.7303466796875\n",
      "Epoch 20: Train 16.429136252521875, Test 640.66259765625\n",
      "Epoch 21: Train 16.26728649518976, Test 660.3839721679688\n",
      "Epoch 22: Train 15.983112567692848, Test 664.2647705078125\n",
      "Epoch 23: Train 15.97080064175734, Test 620.7078247070312\n",
      "Epoch 24: Train 15.950692803112428, Test 621.2969970703125\n",
      "Epoch 25: Train 15.863063598746685, Test 659.6215209960938\n",
      "Epoch 26: Train 15.878773822120174, Test 670.427734375\n",
      "Epoch 27: Train 15.841679027424522, Test 666.280029296875\n",
      "Epoch 28: Train 15.713876876071911, Test 598.48583984375\n",
      "Epoch 29: Train 15.574627980664001, Test 626.5961303710938\n",
      "Epoch 30: Train 15.681703292315278, Test 667.3035278320312\n",
      "Epoch 31: Train 15.658291413416316, Test 685.9619140625\n",
      "Epoch 32: Train 15.436221293549039, Test 683.1494750976562\n",
      "Epoch 33: Train 15.508646827432054, Test 637.9817504882812\n",
      "Epoch 34: Train 15.616611732179253, Test 617.05712890625\n",
      "Epoch 35: Train 15.729143223359218, Test 671.276123046875\n",
      "Epoch 36: Train 15.532744445610994, Test 631.7930908203125\n",
      "Epoch 37: Train 15.541843224520708, Test 641.1650390625\n",
      "Epoch 38: Train 15.506548108153082, Test 681.2416381835938\n",
      "Epoch 39: Train 15.529428676586246, Test 652.754150390625\n",
      "Epoch 40: Train 15.543939320009146, Test 634.25\n",
      "Epoch 41: Train 15.50346199434195, Test 673.4826049804688\n",
      "Epoch 42: Train 15.486631317518244, Test 685.6627197265625\n",
      "Epoch 43: Train 15.329197323737453, Test 664.3760375976562\n",
      "Epoch 44: Train 15.429825868179549, Test 645.2556762695312\n",
      "Epoch 45: Train 15.448326960131897, Test 611.4057006835938\n",
      "Epoch 46: Train 15.423317472733075, Test 633.97412109375\n",
      "Epoch 47: Train 15.166200841837261, Test 635.6318359375\n",
      "Epoch 48: Train 15.227646571486744, Test 669.4712524414062\n",
      "Epoch 49: Train 15.025047610648237, Test 599.8344116210938\n",
      "Epoch 50: Train 14.980571210680909, Test 637.226318359375\n",
      "Epoch 51: Train 14.982338758250375, Test 605.6204223632812\n",
      "Epoch 52: Train 14.875356783321248, Test 667.8081665039062\n",
      "Epoch 53: Train 14.919310569763184, Test 677.0264282226562\n",
      "Epoch 54: Train 14.823131561279297, Test 634.68994140625\n",
      "Epoch 55: Train 14.892412873642956, Test 634.1035766601562\n",
      "Epoch 56: Train 14.960338630486483, Test 640.7058715820312\n",
      "Epoch 57: Train 14.902339987493866, Test 632.1563720703125\n",
      "Epoch 58: Train 14.796233523544387, Test 645.1014404296875\n",
      "Epoch 59: Train 14.96064401503226, Test 666.701171875\n",
      "Epoch 60: Train 14.876751762124437, Test 654.6002197265625\n",
      "Epoch 61: Train 14.869341271433663, Test 649.0003662109375\n",
      "Epoch 62: Train 14.822193624961436, Test 663.964599609375\n",
      "Epoch 63: Train 14.835284721792041, Test 653.6777954101562\n",
      "Epoch 64: Train 14.962993303934732, Test 616.1002807617188\n",
      "Epoch 65: Train 15.016429948569531, Test 594.882080078125\n",
      "Epoch 66: Train 14.73216796039942, Test 658.148193359375\n",
      "Epoch 67: Train 14.830349694437055, Test 617.3497924804688\n",
      "Epoch 68: Train 14.785208830192907, Test 612.7674560546875\n",
      "Epoch 69: Train 14.829009440407825, Test 698.3931274414062\n",
      "Epoch 70: Train 14.814353392491887, Test 652.1771850585938\n",
      "Epoch 71: Train 14.729064860747227, Test 618.2172241210938\n",
      "Epoch 72: Train 14.746790767309085, Test 653.04931640625\n",
      "Epoch 73: Train 14.834720293680826, Test 598.1253662109375\n",
      "Epoch 74: Train 14.835596948120724, Test 640.1017456054688\n",
      "Epoch 75: Train 14.764362017313639, Test 656.5288696289062\n",
      "Epoch 76: Train 14.940077520721587, Test 656.9639282226562\n",
      "Epoch 77: Train 14.762294100291694, Test 657.195556640625\n",
      "Epoch 78: Train 14.917346052862518, Test 611.6571655273438\n",
      "Epoch 79: Train 14.795513746157214, Test 686.7935180664062\n",
      "Epoch 80: Train 14.767099148005395, Test 605.868896484375\n",
      "Epoch 81: Train 14.756365747594122, Test 618.3289794921875\n",
      "Epoch 82: Train 14.868871797969685, Test 684.775634765625\n",
      "Epoch 83: Train 14.607767318611714, Test 609.3123779296875\n",
      "Epoch 84: Train 14.555363024052102, Test 679.1939086914062\n",
      "Epoch 85: Train 14.61905566258217, Test 630.2993774414062\n",
      "Epoch 86: Train 14.366740383319001, Test 635.6887817382812\n",
      "Epoch 87: Train 14.461177503291648, Test 666.7119750976562\n",
      "Epoch 88: Train 14.488195870053115, Test 641.4707641601562\n",
      "Epoch 89: Train 14.330233132661279, Test 656.1353149414062\n",
      "Epoch 90: Train 14.372649548658684, Test 646.9491577148438\n",
      "Epoch 91: Train 14.428545923375372, Test 665.019775390625\n",
      "Epoch 92: Train 14.318967102771968, Test 641.7991333007812\n",
      "Epoch 93: Train 14.376779134000712, Test 656.5855712890625\n",
      "Epoch 94: Train 14.378104053326506, Test 610.2650146484375\n",
      "Epoch 95: Train 14.3905310891754, Test 628.4133911132812\n",
      "Epoch 96: Train 14.376838703060624, Test 639.2999877929688\n",
      "Epoch 97: Train 14.43150897998715, Test 644.219482421875\n",
      "Epoch 98: Train 14.2608219450386, Test 651.0130615234375\n",
      "Epoch 99: Train 14.342866579691568, Test 617.3699951171875\n",
      "Epoch 0: AE 15.422696291208267, Reg 3.0450217080116273\n",
      "Epoch 1: AE 13.990081712007523, Reg 2.9729416942596436\n",
      "Epoch 2: AE 15.134472031593322, Reg 3.0305500197410584\n",
      "Epoch 3: AE 14.948389422893523, Reg 3.0211375832557676\n",
      "Epoch 4: AE 16.543247294425964, Reg 3.10132383108139\n",
      "Epoch 5: AE 15.477243299484252, Reg 3.047680060863495\n",
      "Epoch 6: AE 15.622155474424362, Reg 3.05474645614624\n",
      "Epoch 7: AE 15.639887881278991, Reg 3.055839407444\n",
      "Epoch 8: AE 14.449665054082871, Reg 2.9962789392471314\n",
      "Epoch 9: AE 15.611002025604249, Reg 3.053912868499756\n",
      "Epoch 10: AE 14.219409427642823, Reg 2.9847152495384215\n",
      "Epoch 11: AE 13.309573439359665, Reg 2.939049472808838\n",
      "Epoch 12: AE 13.450454908609391, Reg 2.946210038661957\n",
      "Epoch 13: AE 13.594498715400697, Reg 2.953279938697815\n",
      "Epoch 14: AE 14.192734451293946, Reg 2.9833447980880736\n",
      "Epoch 15: AE 15.080347174406052, Reg 3.027474558353424\n",
      "Epoch 16: AE 14.966135005950928, Reg 3.022204144001007\n",
      "Epoch 17: AE 14.526691596508027, Reg 3.0000179839134216\n",
      "Epoch 18: AE 15.393108425140381, Reg 3.044076731204987\n",
      "Epoch 19: AE 15.626338399648667, Reg 3.05527242898941\n",
      "Epoch 20: AE 14.879814740419388, Reg 3.0172991013526915\n",
      "Epoch 21: AE 14.0936327624321, Reg 2.978353340625763\n",
      "Epoch 22: AE 13.290909479856492, Reg 2.938193185329437\n",
      "Epoch 23: AE 16.65933365702629, Reg 3.1067942810058593\n",
      "Epoch 24: AE 15.60104787349701, Reg 3.0541961193084717\n",
      "Epoch 25: AE 14.375958948135375, Reg 2.992655987739563\n",
      "Epoch 26: AE 14.314090541601182, Reg 2.9892358446121214\n",
      "Epoch 27: AE 13.08950997710228, Reg 2.9280716705322267\n",
      "Epoch 28: AE 15.799789437055587, Reg 3.0638907289505006\n",
      "Epoch 29: AE 14.307295340299607, Reg 2.989249122142792\n",
      "Epoch 30: AE 14.751107811927795, Reg 3.0113572478294373\n",
      "Epoch 31: AE 16.32332400918007, Reg 3.0903717851638794\n",
      "Epoch 32: AE 14.371220077276229, Reg 2.992324631214142\n",
      "Epoch 33: AE 15.325678004026413, Reg 3.0399181771278383\n",
      "Epoch 34: AE 15.801460137367249, Reg 3.064196012020111\n",
      "Epoch 35: AE 15.463847843408585, Reg 3.047077467441559\n",
      "Epoch 36: AE 15.940125349760056, Reg 3.0706358098983766\n",
      "Epoch 37: AE 15.434246233701707, Reg 3.0457912945747374\n",
      "Epoch 38: AE 14.462592579126358, Reg 2.9965693044662474\n",
      "Epoch 39: AE 14.60943044066429, Reg 3.0042752957344057\n",
      "Epoch 40: AE 16.13704021334648, Reg 3.0802676105499267\n",
      "Epoch 41: AE 15.034021743535995, Reg 3.025264713764191\n",
      "Epoch 42: AE 14.71778657913208, Reg 3.0094845843315126\n",
      "Epoch 43: AE 15.178146764039994, Reg 3.0327272987365723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: AE 15.426026923656464, Reg 3.0452237057685854\n",
      "Epoch 45: AE 13.781222631931305, Reg 2.9629597282409668\n",
      "Epoch 46: AE 14.613362456560134, Reg 3.0042610573768616\n",
      "Epoch 47: AE 13.081885662078857, Reg 2.9280683851242064\n",
      "Epoch 48: AE 15.01287767291069, Reg 3.0243199467658997\n",
      "Epoch 49: AE 14.659754722118377, Reg 3.0067117762565614\n",
      "Epoch 50: AE 14.50908218383789, Reg 2.9990327382087707\n",
      "Epoch 51: AE 14.713851318359374, Reg 3.009497227668762\n",
      "Epoch 52: AE 15.192287292480469, Reg 3.0330780005455016\n",
      "Epoch 53: AE 14.806684445142746, Reg 3.014303071498871\n",
      "Epoch 54: AE 14.18686670422554, Reg 2.982999517917633\n",
      "Epoch 55: AE 13.169655064344406, Reg 2.932373561859131\n",
      "Epoch 56: AE 13.548017823696137, Reg 2.9508023619651795\n",
      "Epoch 57: AE 13.778018443584441, Reg 2.9620881628990174\n",
      "Epoch 58: AE 14.689264419078826, Reg 3.0084628558158872\n",
      "Epoch 59: AE 14.964932055473328, Reg 3.021667902469635\n",
      "Epoch 60: AE 15.308880571126938, Reg 3.0390429639816285\n",
      "Epoch 61: AE 14.913283553123474, Reg 3.0196189737319945\n",
      "Epoch 62: AE 14.509659150838852, Reg 2.9989598774909973\n",
      "Epoch 63: AE 13.718499845266342, Reg 2.959610366821289\n",
      "Epoch 64: AE 14.86967451453209, Reg 3.0169209671020507\n",
      "Epoch 65: AE 13.113288160562515, Reg 2.929449782371521\n",
      "Epoch 66: AE 15.300917949676514, Reg 3.0383645153045653\n",
      "Epoch 67: AE 15.31817850112915, Reg 3.039649472236633\n",
      "Epoch 68: AE 15.949508292675018, Reg 3.071401529312134\n",
      "Epoch 69: AE 16.697205679416655, Reg 3.108355231285095\n",
      "Epoch 70: AE 14.036193606853486, Reg 2.975954821109772\n",
      "Epoch 71: AE 15.821236907243728, Reg 3.0651789855957032\n",
      "Epoch 72: AE 14.640668562650681, Reg 3.0058580899238585\n",
      "Epoch 73: AE 14.333479306697846, Reg 2.9905202054977416\n",
      "Epoch 74: AE 17.508996057510377, Reg 3.149253261089325\n",
      "Epoch 75: AE 13.129220160245895, Reg 2.9297224378585813\n",
      "Epoch 76: AE 13.019682066440582, Reg 2.9243961644172667\n",
      "Epoch 77: AE 14.728750023841858, Reg 3.010379388332367\n",
      "Epoch 78: AE 15.386771125793457, Reg 3.0435521531105043\n",
      "Epoch 79: AE 13.686265692710876, Reg 2.958089234828949\n",
      "Epoch 80: AE 15.331216440200805, Reg 3.040328743457794\n",
      "Epoch 81: AE 14.407891066074372, Reg 2.993865554332733\n",
      "Epoch 82: AE 14.158593637943268, Reg 2.9813618588447572\n",
      "Epoch 83: AE 14.282843705415726, Reg 2.9880476450920104\n",
      "Epoch 84: AE 14.55310673236847, Reg 3.0015582728385923\n",
      "Epoch 85: AE 14.454386870861054, Reg 2.996365053653717\n",
      "Epoch 86: AE 16.06984998822212, Reg 3.077427706718445\n",
      "Epoch 87: AE 15.348038817644118, Reg 3.04122088432312\n",
      "Epoch 88: AE 15.188117626905441, Reg 3.0334903144836427\n",
      "Epoch 89: AE 15.544806762933732, Reg 3.050852653980255\n",
      "Epoch 90: AE 14.047185134887695, Reg 2.9761745810508726\n",
      "Epoch 91: AE 14.978888039588929, Reg 3.022546191215515\n",
      "Epoch 92: AE 14.756354876756667, Reg 3.0114461517333986\n",
      "Epoch 93: AE 16.18647425532341, Reg 3.0832227730751036\n",
      "Epoch 94: AE 16.465800522565843, Reg 3.0975764346122743\n",
      "Epoch 95: AE 16.02803107023239, Reg 3.074970202445984\n",
      "Epoch 96: AE 15.131736665964127, Reg 3.030269205570221\n",
      "Epoch 97: AE 16.175439442396165, Reg 3.082795958518982\n",
      "Epoch 98: AE 13.91348830461502, Reg 2.969439709186554\n",
      "Epoch 99: AE 15.230618875026703, Reg 3.035467667579651\n",
      "MSE 14.548860549926758\n",
      "Median 13.676042556762695\n",
      "Standard Devitaion 6.743080139160156\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine-tune autoencoder\n",
    "#batch 500\n",
    "\n",
    "ae = Naive_DAE([28*28,200,100])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer_ae = torch.optim.Adam(ae.parameters(), lr,weight_decay=5e-2)\n",
    "\n",
    "\n",
    "all_test_losses = []\n",
    "all_train_losses = []\n",
    "\n",
    "\n",
    "# initial training of AE\n",
    "running_loss = float(\"inf\")\n",
    "for epoch in range(epochs_ae):\n",
    "    losses = []\n",
    "    \n",
    "    for i, data_list in enumerate(train_loader):\n",
    "        data = data_list[0]\n",
    "        v_pred = ae(data)\n",
    "        batch_loss = ae_lossfn(data, v_pred) # difference between actual and reconstructed   \n",
    "        \n",
    "        all_train_losses.append(batch_loss.item())\n",
    "        losses.append(batch_loss.item())\n",
    "        optimizer_ae.zero_grad()\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        optimizer_ae.step()\n",
    "        if i == 200:\n",
    "            break\n",
    "    data_test = next(iter(test_loader))[0]\n",
    "    test_pred = ae(data_test)\n",
    "    batch_test = loss(data_test, test_pred)\n",
    "    running_loss = np.mean(losses)\n",
    "    running_test_loss = batch_test.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train {running_loss}, Test {running_test_loss}\")\n",
    "# generate_stats(ae(data_test),data_test)\n",
    "\n",
    "\n",
    "#Combine trainng ae and regression\n",
    "reg =MLP()\n",
    "optimizer_reg = torch.optim.Adam(reg.parameters(), lr,weight_decay=5e-2)\n",
    "\n",
    "for epoch in range(epochs_reg):\n",
    "    \n",
    "    for i, data_list in enumerate(train_loader):\n",
    "        data = data_list[0]\n",
    "        pred_recon = ae(data)\n",
    "        batch_ae_loss = ae_lossfn(data, pred_recon)\n",
    "\n",
    "        data_latent = ae.encode(data)\n",
    "\n",
    "        pred_reg = reg(data_latent.detach())\n",
    "        one_hot =F.one_hot(data_list[1], num_classes=10).type(torch.DoubleTensor)\n",
    "\n",
    "        batch_reg_loss = reg_lossfn(pred_reg,one_hot)\n",
    "        if i == 200:\n",
    "            break\n",
    "        \n",
    "        ae_loss = batch_ae_loss + 0.25 *batch_reg_loss.detach()\n",
    "        \n",
    "        reg_loss = 0.05 *batch_ae_loss.detach() +  batch_reg_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "         # difference between actual and reconstructed   \n",
    "        \n",
    "        all_train_losses.append(batch_loss.item())\n",
    "        losses.append(batch_loss.item())\n",
    "        \n",
    "        optimizer_ae.zero_grad()\n",
    "        ae_loss.backward()\n",
    "        optimizer_ae.step()\n",
    "\n",
    "        \n",
    "        optimizer_reg.zero_grad()\n",
    "        reg_loss.backward()\n",
    "        optimizer_reg.step()\n",
    "        \n",
    "    \n",
    "    print(f\"Epoch {epoch}: AE {ae_loss.item()}, Reg {reg_loss.item()}\")\n",
    "generate_stats(ae(data_test),data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192454e",
   "metadata": {},
   "source": [
    "## Method #2\n",
    "\n",
    "Here I am training the AE and the regression in each batch concurrently. Possibly I could train them in cycles. To allow for more learning on each pattern. \n",
    "\n",
    "For example, train AE for 10 epochs, take this current model, train regression for 10 epochs.\n",
    "\n",
    "Seems that AE loss should be linked to the regression, but regression loss should not be linked to AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be191a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train 42.93924533787058, Test 28.674684524536133\n",
      "Epoch 1: Train 25.739784307147733, Test 23.051410675048828\n",
      "Epoch 2: Train 22.689771063885285, Test 21.70984649658203\n",
      "Epoch 3: Train 21.104974310196454, Test 20.236186981201172\n",
      "Epoch 4: Train 19.979070962364993, Test 19.231212615966797\n",
      "Epoch 5: Train 19.139781524885947, Test 18.77940559387207\n",
      "Epoch 6: Train 18.66438991394802, Test 18.472795486450195\n",
      "Epoch 7: Train 18.43161811638827, Test 18.055160522460938\n",
      "Epoch 8: Train 18.122678543204692, Test 17.868349075317383\n",
      "Epoch 9: Train 17.712390652936488, Test 17.47765350341797\n",
      "Epoch 10: Train 17.59605604143285, Test 17.132827758789062\n",
      "Epoch 11: Train 17.20395430284946, Test 17.221054077148438\n",
      "Epoch 12: Train 17.204863211408778, Test 17.434337615966797\n",
      "Epoch 13: Train 17.164789669549286, Test 16.877532958984375\n",
      "Epoch 14: Train 16.890301310600925, Test 16.82844352722168\n",
      "Epoch 15: Train 16.892429593783707, Test 16.513872146606445\n",
      "Epoch 16: Train 16.585128788924337, Test 16.35450553894043\n",
      "Epoch 17: Train 16.52037853981132, Test 16.669830322265625\n",
      "Epoch 18: Train 16.602983014500555, Test 16.399450302124023\n",
      "Epoch 19: Train 16.308423208360054, Test 16.441389083862305\n",
      "Epoch 20: Train 16.451978726173515, Test 16.199464797973633\n",
      "Epoch 21: Train 16.24028094372346, Test 16.171300888061523\n",
      "Epoch 22: Train 16.219565661985484, Test 16.173667907714844\n",
      "Epoch 23: Train 16.390845308256388, Test 16.47078514099121\n",
      "Epoch 24: Train 16.223846577886324, Test 16.5623836517334\n",
      "Epoch 25: Train 16.24571218538047, Test 16.368520736694336\n",
      "Epoch 26: Train 16.321090275968483, Test 15.985604286193848\n",
      "Epoch 27: Train 16.153855774533096, Test 16.135780334472656\n",
      "Epoch 28: Train 16.177988768810064, Test 16.236061096191406\n",
      "Epoch 29: Train 16.168768104629137, Test 15.937049865722656\n",
      "Epoch 30: Train 15.926106263155962, Test 15.954182624816895\n",
      "Epoch 31: Train 15.805438326365913, Test 15.742118835449219\n",
      "Epoch 32: Train 15.79058300796433, Test 15.669833183288574\n",
      "Epoch 33: Train 15.683263925770621, Test 15.719452857971191\n",
      "Epoch 34: Train 15.669847782571518, Test 15.88369369506836\n",
      "Epoch 35: Train 15.64431386919164, Test 15.789809226989746\n",
      "Epoch 36: Train 15.523351489014887, Test 16.101808547973633\n",
      "Epoch 37: Train 15.547225591555163, Test 15.339353561401367\n",
      "Epoch 38: Train 15.401662238201691, Test 15.5851469039917\n",
      "Epoch 39: Train 15.427836565235953, Test 15.596162796020508\n",
      "Epoch 40: Train 15.52426860581583, Test 15.513762474060059\n",
      "Epoch 41: Train 15.503874475090065, Test 15.501458168029785\n",
      "Epoch 42: Train 15.439124292402125, Test 15.343757629394531\n",
      "Epoch 43: Train 15.348881009799332, Test 15.360626220703125\n",
      "Epoch 44: Train 15.45526162901921, Test 15.099834442138672\n",
      "Epoch 45: Train 15.299564328359727, Test 15.383155822753906\n",
      "Epoch 46: Train 15.502601822810387, Test 15.42896842956543\n",
      "Epoch 47: Train 15.401280474306931, Test 15.383901596069336\n",
      "Epoch 48: Train 15.431515987832748, Test 15.363245010375977\n",
      "Epoch 49: Train 15.32224827382102, Test 15.681571960449219\n",
      "Epoch 50: Train 15.304984918281214, Test 15.37552547454834\n",
      "Epoch 51: Train 15.347059183452853, Test 15.347420692443848\n",
      "Epoch 52: Train 15.372836435612161, Test 15.181201934814453\n",
      "Epoch 53: Train 15.22247920344718, Test 15.445716857910156\n",
      "Epoch 54: Train 15.32270865179413, Test 15.173174858093262\n",
      "Epoch 55: Train 15.087918997997075, Test 15.132853507995605\n",
      "Epoch 56: Train 15.177668524025684, Test 15.711517333984375\n",
      "Epoch 57: Train 14.880480946593023, Test 14.807374954223633\n",
      "Epoch 58: Train 14.883911545596906, Test 14.964859008789062\n",
      "Epoch 59: Train 14.800516512856555, Test 14.9575777053833\n",
      "Epoch 60: Train 14.914686131833205, Test 15.061383247375488\n",
      "Epoch 61: Train 14.811974852832396, Test 14.929971694946289\n",
      "Epoch 62: Train 14.826799402189492, Test 14.904106140136719\n",
      "Epoch 63: Train 14.875984604678937, Test 14.726706504821777\n",
      "Epoch 64: Train 14.728093071363459, Test 14.688704490661621\n",
      "Epoch 65: Train 14.831493344473008, Test 14.969584465026855\n",
      "Epoch 66: Train 14.99569391848436, Test 14.996068954467773\n",
      "Epoch 67: Train 14.72643514414925, Test 14.857659339904785\n",
      "Epoch 68: Train 14.828454947590235, Test 14.835379600524902\n",
      "Epoch 69: Train 14.854005044965602, Test 15.100390434265137\n",
      "Epoch 70: Train 14.85038836559846, Test 14.535970687866211\n",
      "Epoch 71: Train 14.781201077930962, Test 14.625926971435547\n",
      "Epoch 72: Train 14.78908674040837, Test 14.873779296875\n",
      "Epoch 73: Train 14.853405468499483, Test 14.623351097106934\n",
      "Epoch 74: Train 14.81961812546004, Test 14.928260803222656\n",
      "Epoch 75: Train 14.685440865322132, Test 14.75107192993164\n",
      "Epoch 76: Train 14.74719243974828, Test 14.812607765197754\n",
      "Epoch 77: Train 14.579808334806073, Test 14.627381324768066\n",
      "Epoch 78: Train 14.654880409810081, Test 14.665210723876953\n",
      "Epoch 79: Train 14.573119671190556, Test 14.896859169006348\n",
      "Epoch 80: Train 14.716552487653287, Test 14.834806442260742\n",
      "Epoch 81: Train 14.619762316272034, Test 14.531168937683105\n",
      "Epoch 82: Train 14.606121257763004, Test 14.70825481414795\n",
      "Epoch 83: Train 14.726193418550254, Test 14.659740447998047\n",
      "Epoch 84: Train 14.568878909248618, Test 14.738611221313477\n",
      "Epoch 85: Train 14.630525717094763, Test 15.043445587158203\n",
      "Epoch 86: Train 14.665557908774609, Test 14.82296371459961\n",
      "Epoch 87: Train 14.529522032286991, Test 14.69140625\n",
      "Epoch 88: Train 14.71134253165022, Test 14.820363998413086\n",
      "Epoch 89: Train 14.67891985860037, Test 14.701282501220703\n",
      "Epoch 90: Train 14.673226802503292, Test 14.769854545593262\n",
      "Epoch 91: Train 14.523489140752536, Test 14.643240928649902\n",
      "Epoch 92: Train 14.717690643386462, Test 14.514229774475098\n",
      "Epoch 93: Train 14.597827479613954, Test 14.783895492553711\n",
      "Epoch 94: Train 14.603582401180741, Test 14.652867317199707\n",
      "Epoch 95: Train 14.478218961117873, Test 14.638082504272461\n",
      "Epoch 96: Train 14.368373391640127, Test 14.367829322814941\n",
      "Epoch 97: Train 14.3086848045463, Test 14.293412208557129\n",
      "Epoch 98: Train 14.193429989601249, Test 14.34711742401123\n",
      "Epoch 99: Train 14.227221555377714, Test 14.244951248168945\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50x50 and 100x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m batch_ae_loss \u001b[38;5;241m=\u001b[39m ae_lossfn(data, pred_recon)\n\u001b[1;32m     61\u001b[0m data_latent \u001b[38;5;241m=\u001b[39m ae\u001b[38;5;241m.\u001b[39mencode(data)\n\u001b[0;32m---> 63\u001b[0m pred_reg \u001b[38;5;241m=\u001b[39m \u001b[43mreg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_latent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m one_hot \u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mone_hot(data_list[\u001b[38;5;241m1\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mDoubleTensor)\n\u001b[1;32m     66\u001b[0m batch_reg_loss \u001b[38;5;241m=\u001b[39m reg_lossfn(pred_reg,one_hot)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [14], line 19\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m      Forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (50x50 and 100x64)"
     ]
    }
   ],
   "source": [
    "# fine-tune autoencoder\n",
    "#batch 500\n",
    "\n",
    "ae = Naive_DAE([28*28,200,50])\n",
    "\n",
    "optimizer_ae = torch.optim.Adam(ae.parameters(), lr,weight_decay=5e-2)\n",
    "\n",
    "\n",
    "all_test_losses = []\n",
    "all_train_losses = []\n",
    "\n",
    "\n",
    "# initial training of AE\n",
    "running_loss = float(\"inf\")\n",
    "for epoch in range(epochs_ae):\n",
    "    losses = []\n",
    "    \n",
    "    for i, data_list in enumerate(train_loader):\n",
    "        data = data_list[0]\n",
    "        v_pred = ae(data)\n",
    "        batch_loss = ae_lossfn(data, v_pred) # difference between actual and reconstructed   \n",
    "        \n",
    "        all_train_losses.append(batch_loss.item())\n",
    "        losses.append(batch_loss.item())\n",
    "        optimizer_ae.zero_grad()\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        optimizer_ae.step()\n",
    "        if i == 200:\n",
    "            break\n",
    "    data_test = next(iter(test_loader))[0]\n",
    "    test_pred = ae(data_test)\n",
    "    batch_test = ae_lossfn(data_test, test_pred)\n",
    "    running_loss = np.mean(losses)\n",
    "    running_test_loss = batch_test.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train {running_loss}, Test {running_test_loss}\")\n",
    "    \n",
    "    \n",
    "'''\n",
    "\n",
    "Still want an initial AE training so nothing changes here\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#Declare our regression MLP\n",
    "reg =MLP()\n",
    "optimizer_reg = torch.optim.Adam(reg.parameters(), lr,weight_decay=5e-2)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs_reg):\n",
    "    \n",
    "    for i, data_list in enumerate(train_loader):\n",
    "        data = data_list[0]\n",
    "        \n",
    "        #train AE\n",
    "        if epoch % 2 == 0:\n",
    "            pred_recon = ae(data)\n",
    "            batch_ae_loss = ae_lossfn(data, pred_recon)\n",
    "            data_latent = ae.encode(data)\n",
    "\n",
    "            pred_reg = reg(data_latent.detach())\n",
    "            one_hot =F.one_hot(data_list[1], num_classes=10).type(torch.DoubleTensor)\n",
    "\n",
    "            batch_reg_loss = reg_lossfn(pred_reg,one_hot)\n",
    "            ae_loss = batch_ae_loss + 0.25 *batch_reg_loss.detach()\n",
    "            optimizer_ae.zero_grad()\n",
    "            ae_loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            \n",
    "        #Train regression    \n",
    "        else:\n",
    "            data_latent = ae.encode(data)\n",
    "            pred_reg = reg(data_latent.detach())\n",
    "            one_hot =F.one_hot(data_list[1], num_classes=10).type(torch.DoubleTensor)\n",
    "            batch_reg_loss = reg_lossfn(pred_reg,one_hot)\n",
    "            reg_loss = batch_reg_loss\n",
    "            \n",
    "            optimizer_reg.zero_grad()\n",
    "            reg_loss.backward()\n",
    "            optimizer_reg.step()\n",
    "        \n",
    "       \n",
    "    \n",
    "    print(f\"Epoch {epoch}: AE {ae_loss.item()}, Reg {reg_loss.item()}\")\n",
    "generate_stats(ae(data_test),data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "eaca1a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_list[0]\n",
    "pred_recon = ae(data)\n",
    "batch_ae_loss = loss(data, v_pred)\n",
    "\n",
    "data_latent = ae.encode(data)\n",
    "\n",
    "pred_reg = reg(data_latent)\n",
    "one_hot =F.one_hot(data_list[1], num_classes=10)\n",
    "\n",
    "batch_reg_loss = loss(pred_reg,one_hot )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4d127d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recon =ae(data)\n",
    "recon = torch.reshape(recon,shape = (50,28,28))\n",
    "fig = plt.figure()\n",
    "predicted_labels =torch.argmax(reg(ae.encode(data)),dim=1)\n",
    "for i in range(4):\n",
    "    plt.subplot(4,2,2*i+1)\n",
    "    plt.title(predicted_labels[i].item())\n",
    "    plt.imshow(recon[i].detach().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "   \n",
    "\n",
    "    plt.subplot(4,2,2*i+2)\n",
    "    plt.imshow(torch.reshape(data[i],shape = (28,28)).detach().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9847997f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b8a762f",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Loss function made to combine reconstruction loss and energy prediction loss.\n",
    "\n",
    "$L_{AE} = \\alpha_1 L_{R} + \\beta_1 L_{P} + \\lambda_1 R(\\theta_{AE})$\n",
    "\n",
    "$L_{E} = \\alpha_2 L_{R} + \\beta_2 L_{P} + \\lambda_2 R(\\theta_{R})$\n",
    "\n",
    "Where $\\alpha_i$, $\\beta_i$, $\\lambda_i \\in [0,1]$\n",
    "\n",
    "Idea here is to not only create a possible encoding which can be decoded with minimal loss, but to weight NN such that we find a useful encoding. Similar to how solving a system with a specific basis can drastically reduce the computation. Allow our $L_{AE}$ to be aware of $L_P$ can push it to develop towards good representations. $L_E$ including the $L_{R}$ ensure that we aren't overtraining and still have an accurate representation of our wafers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e941c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE_AE_Loss(truth, decoded):\n",
    "    dif = torch.sub(decoded, truth)\n",
    "    return torch.mean(torch.sum(dif**2, dim = 1))\n",
    "\n",
    "def MAE_AE_Loss(truth, decoded):\n",
    "    return torch.mean(torch.sum(torch.abs(decoded-truth), dim = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c3d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_recon_loss(truth, decoded):\n",
    "    dif = decoded - truth\n",
    "    return torch.mean(torch.sum(dif**2, dim = 1))\n",
    "def mse_regress_loss(truth, predicted):\n",
    "    dif = predicted - truth\n",
    "    return torch.mean(torch.sum(dif**2, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5adc3a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.5000)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_recon_loss(torch.tensor([[2,2,2],[3,3,3]],dtype = torch.float),torch.tensor([[5,5,5],[3,3,3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d3d7f",
   "metadata": {},
   "source": [
    "# Notes on Dev\n",
    "So almost done.\n",
    "\n",
    "Need to iterate through the train, test datasets.\n",
    "\n",
    "might be better to train AE alone for some determined number of iterations so the results are reasonable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f6de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implementation of a Deep Autoencoder\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class DAE(nn.Module):\n",
    "    \"\"\"A Deep Autoencoder that takes a list of RBMs as input\"\"\"\n",
    "\n",
    "    def __init__(self, models):\n",
    "        \"\"\"Create a deep autoencoder based on a list of RBM models\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models: list[RBM]\n",
    "            a list of RBM models to use for autoencoding\n",
    "        \"\"\"\n",
    "        super(DAE, self).__init__()\n",
    "\n",
    "        # extract weights from each model\n",
    "        encoders = []\n",
    "        encoder_biases = []\n",
    "        decoders = []\n",
    "        decoder_biases = []\n",
    "        for model in models:\n",
    "            encoders.append(nn.Parameter(model.W.clone()))\n",
    "            encoder_biases.append(nn.Parameter(model.h_bias.clone()))\n",
    "            decoders.append(nn.Parameter(model.W.clone()))\n",
    "            decoder_biases.append(nn.Parameter(model.v_bias.clone()))\n",
    "\n",
    "        # build encoders and decoders\n",
    "        self.encoders = nn.ParameterList(encoders)\n",
    "        self.encoder_biases = nn.ParameterList(encoder_biases)\n",
    "        self.decoders = nn.ParameterList(reversed(decoders))\n",
    "        self.decoder_biases = nn.ParameterList(reversed(decoder_biases))\n",
    "\n",
    "    def forward(self, v):\n",
    "        \"\"\"Forward step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v: Tensor\n",
    "            input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            a reconstruction of v from the autoencoder\n",
    "\n",
    "        \"\"\"\n",
    "        # encode\n",
    "        p_h = self.encode(v)\n",
    "\n",
    "        # decode\n",
    "        p_v = self.decode(p_h)\n",
    "\n",
    "        return p_v\n",
    "\n",
    "    def encode(self, v):  # for visualization, encode without sigmoid\n",
    "        \"\"\"Encode input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v: Tensor\n",
    "            visible input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            the activations of the last layer\n",
    "\n",
    "        \"\"\"\n",
    "        p_v = v\n",
    "        activation = v\n",
    "        for i in range(len(self.encoders)):\n",
    "            W = self.encoders[i]\n",
    "            h_bias = self.encoder_biases[i]\n",
    "            activation = torch.mm(p_v, W) + h_bias\n",
    "            p_v = activation\n",
    "\n",
    "        # for the last layer, we want to return the activation directly rather than the sigmoid\n",
    "        return activation\n",
    "\n",
    "    def decode(self, h):\n",
    "        \"\"\"Encode hidden layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h: Tensor\n",
    "            activations from last hidden layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            reconstruction of original input based on h\n",
    "\n",
    "        \"\"\"\n",
    "        p_h = h\n",
    "        for i in range(len(self.encoders)):\n",
    "            W = self.decoders[i]\n",
    "            v_bias = self.decoder_biases[i]\n",
    "            activation = torch.mm(p_h, W.t()) + v_bias\n",
    "            p_h = activation\n",
    "        return p_h\n",
    "\n",
    "\n",
    "class Naive_DAE(nn.Module):\n",
    "    \"\"\"A Naive implementation of the DAE to be trained without RBMs\"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"Initialize the DAE\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layers: list[int]\n",
    "            the number of dimensions in each layer of the DAE\n",
    "\n",
    "        \"\"\"\n",
    "        super(Naive_DAE, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        encoders = []\n",
    "        decoders = []\n",
    "        prev_layer = layers[0]\n",
    "        for layer in layers[1:]:\n",
    "            encoders.append(\n",
    "                nn.Linear(in_features=prev_layer, out_features=layer))\n",
    "            encoders.append(\n",
    "                nn.ReLU())\n",
    "            \n",
    "            decoders.append(\n",
    "                nn.Linear(in_features=layer, out_features=prev_layer))\n",
    "            decoders.append(\n",
    "                nn.ReLU())\n",
    "           \n",
    "            prev_layer = layer\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        self.decoders = nn.ModuleList(reversed(decoders))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward step\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Tensor\n",
    "            input tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            a reconstructed version of x\n",
    "\n",
    "        \"\"\"\n",
    "        x_encoded = self.encode(x)\n",
    "        x_reconstructed = torch.sigmoid(self.decode(x_encoded))\n",
    "        return x_reconstructed\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode the input x\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Tensor\n",
    "            input to encode\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            encoded input\n",
    "\n",
    "        \"\"\"\n",
    "        for i, enc in enumerate(self.encoders):\n",
    "            if i == len(self.encoders) - 1:\n",
    "                x = enc(x)\n",
    "            else:\n",
    "                x = enc(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "        \"\"\"Decode the representation x\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: Tensor\n",
    "            input to decode\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            decoded input\n",
    "\n",
    "        \"\"\"\n",
    "        for dec in self.decoders:\n",
    "            x = dec(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f567d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(decoded, truth):\n",
    "    eps = 10e-8\n",
    "    #Printing basic info\n",
    "    print(f'MSE {torch.mean(torch.sum((decoded-truth)**2,dim =1))}')\n",
    "    print(f'Median {torch.median(torch.sum((decoded-truth)**2,dim =1))}')\n",
    "    print(f'Standard Devitaion {torch.std(torch.sum((decoded-truth)**2,dim =1))}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #plotting residual distribution\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(np.log10(torch.sum((decoded-truth).cpu()**2 +eps,dim =1).detach().numpy()), bins = 50)\n",
    "    plt.xlabel('Log SE')\n",
    "    plt.ylabel('#')\n",
    "    plt.title('Residual Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist((torch.sum(torch.sqrt((decoded-truth).cpu()**2+eps),dim =1).detach().numpy()), bins = 50)\n",
    "    plt.xlabel('RSE')\n",
    "    plt.ylabel('#')\n",
    "    plt.title('Residual Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738c1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(AE,Reg, AE_optimizer,Reg_optimizer, x_train,x_test,y_train, y_test, epochs):\n",
    "    AE.train()\n",
    "    Reg.train()\n",
    "    losses_solo_ae = []\n",
    "    losses_ae = []\n",
    "    losses_reg = []\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    \n",
    "    loss_mse = torch.nn.MSELoss()\n",
    "    \n",
    "    #Initial Training of AE on x_train wafers\n",
    "    \n",
    "    AE,losses_solo_ae, test_losses_solo_ae, _, _ = train_pos(AE,AE_optimizer, x_train,x_test, epochs)\n",
    "    \n",
    "            \n",
    "            \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"AE Pre-training losses\")\n",
    "    plt.plot(losses_solo_ae)\n",
    "    plt.plot(test_losses_solo_ae)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Log AE Pre-training losses\")\n",
    "    plt.plot(np.log(losses_solo_ae))\n",
    "    plt.plot(np.log(test_losses_solo_ae))\n",
    "    plt.show()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #Iterate through batches of calorimeter\n",
    "        #need to fix to iterate through \n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            events = x_train[i]\n",
    "            energy = y_train[i]\n",
    "            \n",
    "            #AE Reconstruction\n",
    "            recon = AE(events)\n",
    "            \n",
    "            \n",
    "            #Calculating Wafer Encoding Error\n",
    "            AE_Loss,__ = MSE_AE_Loss(recon, events[:,0:48])\n",
    "            \n",
    "            \n",
    "            #Encoded\n",
    "            wafers_encoded = AE.out_encode(events)\n",
    "            \n",
    "            #Passing wafers forward into Regression NN\n",
    "            events = torch.detach(torch.unsqueeze(torch.flatten(wafers_encoded),dim = 0))\n",
    "            pred_energy = Reg(events)            \n",
    "\n",
    "            \n",
    "            #Calculating Regression NN Error\n",
    "            Loss_reg = loss_mse(pred_energy, energy)\n",
    "           \n",
    "\n",
    "            #combining both\n",
    "            AE_Loss =AE_Loss + torch.detach(Loss_reg)*0.05\n",
    "            Loss_reg =0.5*torch.detach(AE_Loss) + Loss_reg\n",
    "            losses_ae.append(AE_Loss.item())\n",
    "            losses_reg.append(Loss_reg.item())\n",
    "\n",
    "            \n",
    "            #Step both AE with Error\n",
    "            AE_Loss.backward()\n",
    "            AE_optimizer.step()\n",
    "                        \n",
    "            Loss_reg.backward()\n",
    "            Reg_optimizer.step()\n",
    "        \n",
    "            #Seems like we should have two or four hyperparameters for the combintation of the losses\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Total Training')\n",
    "    plt.plot(losses_ae, label = 'AE Loss')\n",
    "    plt.plot(losses_reg, label = 'Reg Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Total Training')\n",
    "    plt.plot(np.log(losses_ae), label = 'AE Loss')\n",
    "    plt.plot(np.log(losses_reg), label = 'Reg Loss')\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    return AE,Reg,losses_ae, losses_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef127646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
