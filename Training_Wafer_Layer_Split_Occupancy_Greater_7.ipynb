{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f5a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from Naive_DAE import Naive_DAE,Dropout_DAE\n",
    "import AE_Stats\n",
    "from load_data_fn import load_data,load_data_no_filter\n",
    "from telescope_torch import telescopeMSE2\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import importlib\n",
    "import time\n",
    "import ae_train\n",
    "from losses import *\n",
    "\n",
    "path = 'MIT_TTbar'\n",
    "prefixed = [filename for filename in os.listdir(path) if filename.startswith(\"dt_norm\")]\n",
    "\n",
    "data = []\n",
    "for p in prefixed:\n",
    "    data.append([torch.load(f'{path}/{p}'),p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through data and take out all wiht less than 7 occupied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88dab913",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_smol = []\n",
    "for d in data:\n",
    "    non = torch.count_nonzero(d[0][:,0:48],dim =1)\n",
    "    \n",
    "    data_smol.append([d[0][non > 7],d[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8455ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_smol_high = []\n",
    "for d in data:\n",
    "    if int(d[1][10:]) > 26:\n",
    "        data_smol_high.append([d[0],d[1]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f719b3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_3_36\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.801915490945566, Test 0.7100458145141602\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.5547378765823, Test 0.5431606769561768\n",
      "Epoch 50, lr 5.625e-06\n",
      "Epoch 50: Train 0.5264441469340648, Test 0.5122934579849243\n",
      "Epoch 75, lr 1.40625e-06\n",
      "Epoch 75: Train 0.5250068409157417, Test 0.5104655027389526\n",
      "Epoch 100, lr 3.515625e-07\n",
      "Epoch 100: Train 0.5247798056664205, Test 0.5101989507675171\n",
      "Epoch 125, lr 8.7890625e-08\n",
      "Epoch 125: Train 0.52472857817477, Test 0.5101195573806763\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.52470656882212, Test 0.5100902915000916\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.5246970495359797, Test 0.5100803375244141\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.5246875059450329, Test 0.5100709795951843\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.5246779358309835, Test 0.5100606679916382\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.524668366584963, Test 0.5100512504577637\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.5246587915520837, Test 0.5100411176681519\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.5246492697583047, Test 0.5100314617156982\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.5246397035021613, Test 0.5100217461585999\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.5246302813870235, Test 0.5100125074386597\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.5246208243096144, Test 0.5100023746490479\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.5246113448081279, Test 0.5099929571151733\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.5246018306819367, Test 0.5099837183952332\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.5245924604074083, Test 0.5099735856056213\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.5245830100331106, Test 0.5099644660949707\n",
      "TRAINING MODEL dt_norm_3_30\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.278743945993483, Test 1.1621865034103394\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.9541307600215078, Test 0.9313613176345825\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.9263348311185837, Test 0.8991832137107849\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.9094962688162923, Test 0.8810989856719971\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.9018432227894664, Test 0.8736462593078613\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 0.899245309829712, Test 0.8714871406555176\n",
      "Epoch 150, lr 2.8125e-06\n",
      "Epoch 150: Train 0.8958817943930626, Test 0.867822527885437\n",
      "Epoch 175, lr 1.40625e-06\n",
      "Epoch 175: Train 0.8951455273665487, Test 0.8670867085456848\n",
      "Epoch 200, lr 3.515625e-07\n",
      "Epoch 200: Train 0.8946954241953791, Test 0.8666728734970093\n",
      "Epoch 225, lr 8.7890625e-08\n",
      "Epoch 225: Train 0.8945828915573657, Test 0.8665571808815002\n",
      "Epoch 250, lr 2.197265625e-08\n",
      "Epoch 250: Train 0.8945511964149773, Test 0.8665294647216797\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.8945420192554593, Test 0.8665214776992798\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.894536835886538, Test 0.8665175437927246\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.8945317271165549, Test 0.8665132522583008\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.8945266606286169, Test 0.8665088415145874\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.894521594978869, Test 0.8665053844451904\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.8945165166631341, Test 0.8665006160736084\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.8945114884525538, Test 0.8664962649345398\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.8945064196363092, Test 0.8664921522140503\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.8945015370845795, Test 0.8664880990982056\n",
      "TRAINING MODEL dt_norm_3_31\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.4252004778012632, Test 1.307650089263916\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.067437558993697, Test 1.0450540781021118\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 1.020172133669257, Test 0.9935067892074585\n",
      "Epoch 75, lr 1.125e-05\n",
      "Epoch 75: Train 1.0101320419460535, Test 0.9844160079956055\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 1.0052692594006658, Test 0.9793410301208496\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 1.0046588160097598, Test 0.9790182113647461\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 1.0023226356133819, Test 0.9760039448738098\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 1.0018429229035974, Test 0.9748663902282715\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 1.0014751952141523, Test 0.9747810959815979\n",
      "Epoch 225, lr 2.8125e-06\n",
      "Epoch 225: Train 0.99938380792737, Test 0.972962498664856\n",
      "Epoch 250, lr 1.40625e-06\n",
      "Epoch 250: Train 0.9976926935836673, Test 0.9715389013290405\n",
      "Epoch 275, lr 3.515625e-07\n",
      "Epoch 275: Train 0.9972878243774176, Test 0.9711635112762451\n",
      "Epoch 300, lr 8.7890625e-08\n",
      "Epoch 300: Train 0.9971756622195244, Test 0.9710833430290222\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.9971366802230477, Test 0.9710522294044495\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.9971284825354815, Test 0.9710490107536316\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.9971209900453687, Test 0.9710463285446167\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.9971133571118116, Test 0.9710431098937988\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.9971060711890459, Test 0.9710409045219421\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.9970984777435661, Test 0.9710369110107422\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.997091137804091, Test 0.9710337519645691\n",
      "TRAINING MODEL dt_norm_3_33\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.085457169637084, Test 0.9980403184890747\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.8757948317565024, Test 0.8464338779449463\n",
      "Epoch 50, lr 1.125e-05\n",
      "Epoch 50: Train 0.8494733116589487, Test 0.8185259699821472\n",
      "Epoch 75, lr 1.125e-05\n",
      "Epoch 75: Train 0.8443555345758795, Test 0.8125330209732056\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.8412621924653649, Test 0.8085016012191772\n",
      "Epoch 125, lr 5.625e-06\n",
      "Epoch 125: Train 0.8274830002337694, Test 0.7948465347290039\n",
      "Epoch 150, lr 1.40625e-06\n",
      "Epoch 150: Train 0.8239809681661427, Test 0.7913857102394104\n",
      "Epoch 175, lr 3.515625e-07\n",
      "Epoch 175: Train 0.8233107462525368, Test 0.7907431721687317\n",
      "Epoch 200, lr 8.7890625e-08\n",
      "Epoch 200: Train 0.8231493258848787, Test 0.7905912399291992\n",
      "Epoch 225, lr 2.197265625e-08\n",
      "Epoch 225: Train 0.8231013156473637, Test 0.7905486822128296\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.8230838155373931, Test 0.7905343770980835\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.8230702831409872, Test 0.7905238270759583\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.8230569556355476, Test 0.7905128002166748\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.8230435347184539, Test 0.7905020713806152\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.8230302213691175, Test 0.7904911041259766\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.8230169391259551, Test 0.7904805541038513\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.8230035967193544, Test 0.7904700040817261\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.8229904118925333, Test 0.7904595732688904\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.8229773066937923, Test 0.790448784828186\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.8229641856625676, Test 0.7904385328292847\n",
      "TRAINING MODEL dt_norm_3_35\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.8084661384113133, Test 0.7241879105567932\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.6293029394000769, Test 0.6167135834693909\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.616702142637223, Test 0.6049344539642334\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 0.6112434178590774, Test 0.5967963337898254\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 0.6061978949233889, Test 0.5923784971237183\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 0.6034567561931908, Test 0.589560866355896\n",
      "Epoch 150, lr 2.8125e-06\n",
      "Epoch 150: Train 0.6017048769630492, Test 0.5876298546791077\n",
      "Epoch 175, lr 7.03125e-07\n",
      "Epoch 175: Train 0.6012859142385423, Test 0.5871787071228027\n",
      "Epoch 200, lr 1.7578125e-07\n",
      "Epoch 200: Train 0.6011808621697128, Test 0.5870591402053833\n",
      "Epoch 225, lr 4.39453125e-08\n",
      "Epoch 225: Train 0.6011527811177075, Test 0.5870316624641418\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.6011437349021435, Test 0.5870230197906494\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.6011402697302402, Test 0.5870198011398315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.60113704232499, Test 0.5870168805122375\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.6011338939890265, Test 0.5870136618614197\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.601130713429302, Test 0.587010383605957\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.6011275270953774, Test 0.5870072245597839\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.6011243684217333, Test 0.587004542350769\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.6011211320757865, Test 0.5870012044906616\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.6011180127039552, Test 0.5869983434677124\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.6011148940771818, Test 0.5869953036308289\n",
      "TRAINING MODEL dt_norm_3_37\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.5259784735598654, Test 0.44669631123542786\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.3884624759168745, Test 0.37923920154571533\n",
      "Epoch 50, lr 5.625e-06\n",
      "Epoch 50: Train 0.3906969049441739, Test 0.3812101185321808\n",
      "Epoch 75, lr 1.40625e-06\n",
      "Epoch 75: Train 0.3911689018022339, Test 0.3816207945346832\n",
      "Epoch 100, lr 3.515625e-07\n",
      "Epoch 100: Train 0.3912590706029778, Test 0.38166189193725586\n",
      "Epoch 125, lr 4.39453125e-08\n",
      "Epoch 125: Train 0.39126163140033027, Test 0.3816552758216858\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.39125898058684366, Test 0.3816518485546112\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.39125464959714396, Test 0.3816472291946411\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.39125040534344857, Test 0.3816428780555725\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.3912462327371603, Test 0.3816387355327606\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.39124199766782847, Test 0.3816342353820801\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.39123773523284205, Test 0.3816298842430115\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.39123364636358227, Test 0.38162583112716675\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.3912294276011815, Test 0.3816215991973877\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.3912253313750591, Test 0.38161715865135193\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.3912213181362212, Test 0.3816130459308624\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.3912171564949384, Test 0.38160908222198486\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.3912130854789566, Test 0.3816048502922058\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.3912089762728919, Test 0.3816004991531372\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.3912050226386988, Test 0.3815968334674835\n",
      "TRAINING MODEL dt_norm_3_27\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 2.4136635501022092, Test 2.1267447471618652\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.764671252769174, Test 1.7292555570602417\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 1.7490839078588394, Test 1.7119247913360596\n",
      "Epoch 75, lr 5.625e-06\n",
      "Epoch 75: Train 1.7433022719756686, Test 1.704487681388855\n",
      "Epoch 100, lr 2.8125e-06\n",
      "Epoch 100: Train 1.7425206432836342, Test 1.703373908996582\n",
      "Epoch 125, lr 7.03125e-07\n",
      "Epoch 125: Train 1.741274932441588, Test 1.701764464378357\n",
      "Epoch 150, lr 1.7578125e-07\n",
      "Epoch 150: Train 1.7409804110388154, Test 1.7014293670654297\n",
      "Epoch 175, lr 4.39453125e-08\n",
      "Epoch 175: Train 1.7409091937117591, Test 1.70133376121521\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 1.74088843316322, Test 1.7013158798217773\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 1.7408822049600792, Test 1.7013118267059326\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 1.7408760539150547, Test 1.7013083696365356\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 1.7408696108265602, Test 1.7013046741485596\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 1.740863371049702, Test 1.7012994289398193\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 1.7408571837403628, Test 1.7012965679168701\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 1.7408509219734414, Test 1.701291561126709\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 1.740844694156091, Test 1.7012882232666016\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 1.7408384435770967, Test 1.7012834548950195\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 1.7408321289568658, Test 1.701279640197754\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 1.7408174372799574, Test 1.7012667655944824\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 1.7407892707096335, Test 1.7012405395507812\n",
      "TRAINING MODEL dt_norm_3_29\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.998630291223526, Test 0.9338677525520325\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.8652948146685958, Test 0.8439111113548279\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.85894048307091, Test 0.8347626328468323\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.848178249411285, Test 0.8253345489501953\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.8313789902254939, Test 0.8084970712661743\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 0.8287513384595513, Test 0.805813729763031\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 0.8262074567377568, Test 0.8032774329185486\n",
      "Epoch 175, lr 1.40625e-06\n",
      "Epoch 175: Train 0.8246605219319463, Test 0.8019518852233887\n",
      "Epoch 200, lr 3.515625e-07\n",
      "Epoch 200: Train 0.8242296045646071, Test 0.8015540838241577\n",
      "Epoch 225, lr 8.7890625e-08\n",
      "Epoch 225: Train 0.8241050569340587, Test 0.8014562129974365\n",
      "Epoch 250, lr 2.197265625e-08\n",
      "Epoch 250: Train 0.8240743031725287, Test 0.8014280200004578\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.8240638552233577, Test 0.8014184832572937\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.8240579426288605, Test 0.8014129400253296\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.8240518929436803, Test 0.8014076352119446\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.8240460585802794, Test 0.8014022707939148\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.8240400811657309, Test 0.801396906375885\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.8240342607721687, Test 0.8013911843299866\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.8240284519270062, Test 0.801385760307312\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.8240227973088622, Test 0.8013810515403748\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.824017227254808, Test 0.8013757467269897\n",
      "TRAINING MODEL dt_norm_3_32\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.2452758302912117, Test 1.121190071105957\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.9685973059386015, Test 0.9527957439422607\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 0.8763690318912267, Test 0.8573133945465088\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.841545345261693, Test 0.8205442428588867\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.8327245199121535, Test 0.8125516176223755\n",
      "Epoch 125, lr 5.625e-06\n",
      "Epoch 125: Train 0.8306338643655181, Test 0.8102294206619263\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 0.8291729710064828, Test 0.8086124658584595\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 0.8278530952520669, Test 0.8071521520614624\n",
      "Epoch 200, lr 5.625e-06\n",
      "Epoch 200: Train 0.8235363366082311, Test 0.8028061389923096\n",
      "Epoch 225, lr 5.625e-06\n",
      "Epoch 225: Train 0.8227225176990032, Test 0.80202317237854\n",
      "Epoch 250, lr 5.625e-06\n",
      "Epoch 250: Train 0.8208459313958884, Test 0.7999403476715088\n",
      "Epoch 275, lr 5.625e-06\n",
      "Epoch 275: Train 0.8200073927640915, Test 0.7989908456802368\n",
      "Epoch 300, lr 5.625e-06\n",
      "Epoch 300: Train 0.8192100486718118, Test 0.7983121871948242\n",
      "Epoch 325, lr 5.625e-06\n",
      "Epoch 325: Train 0.8187493656761944, Test 0.7975868582725525\n",
      "Epoch 350, lr 5.625e-06\n",
      "Epoch 350: Train 0.8182846399024128, Test 0.7972291707992554\n",
      "Epoch 375, lr 2.8125e-06\n",
      "Epoch 375: Train 0.8173122595064342, Test 0.7961728572845459\n",
      "Epoch 400, lr 1.40625e-06\n",
      "Epoch 400: Train 0.8165698670782149, Test 0.7954974174499512\n",
      "Epoch 425, lr 7.03125e-07\n",
      "Epoch 425: Train 0.8163003203459084, Test 0.7951968312263489\n",
      "Epoch 450, lr 8.7890625e-08\n",
      "Epoch 450: Train 0.8161053318530321, Test 0.7950165271759033\n",
      "Epoch 475, lr 2.197265625e-08\n",
      "Epoch 475: Train 0.8160753317177296, Test 0.7949904203414917\n",
      "TRAINING MODEL dt_norm_3_34\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.8219093257561326, Test 0.7507227659225464\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.6672848852351307, Test 0.6518932580947876\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 0.6596216374076903, Test 0.6431731581687927\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.658619427215308, Test 0.6408537030220032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.6539088591001928, Test 0.6359338760375977\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 0.6529144143685699, Test 0.6352810263633728\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 0.6517538252286613, Test 0.6339253187179565\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 0.6510060274973511, Test 0.6330850124359131\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 0.6495715047232806, Test 0.6315804123878479\n",
      "Epoch 225, lr 2.8125e-06\n",
      "Epoch 225: Train 0.6485496062785387, Test 0.6306349635124207\n",
      "Epoch 250, lr 2.8125e-06\n",
      "Epoch 250: Train 0.6478154412470758, Test 0.6298058032989502\n",
      "Epoch 275, lr 1.40625e-06\n",
      "Epoch 275: Train 0.6470990282483399, Test 0.6289985775947571\n",
      "Epoch 300, lr 7.03125e-07\n",
      "Epoch 300: Train 0.6467882497236133, Test 0.628711998462677\n",
      "Epoch 325, lr 1.7578125e-07\n",
      "Epoch 325: Train 0.6466210337355733, Test 0.6285437345504761\n",
      "Epoch 350, lr 4.39453125e-08\n",
      "Epoch 350: Train 0.6465756203979254, Test 0.6285001039505005\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.6465619888156653, Test 0.6284878253936768\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.6465569587424398, Test 0.6284841299057007\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.6465521812438965, Test 0.6284801959991455\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.6465472855605185, Test 0.6284765601158142\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.646542408131063, Test 0.628473162651062\n",
      "TRAINING MODEL dt_norm_3_38\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.0929804543654125, Test 1.0179860591888428\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.9614590348375871, Test 0.9549630880355835\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 0.5810121967551414, Test 0.5713818073272705\n",
      "Epoch 75, lr 1.125e-05\n",
      "Epoch 75: Train 0.5575767672910226, Test 0.5480337142944336\n",
      "Epoch 100, lr 2.8125e-06\n",
      "Epoch 100: Train 0.550870748606514, Test 0.541273295879364\n",
      "Epoch 125, lr 3.515625e-07\n",
      "Epoch 125: Train 0.5502845132618808, Test 0.5407878160476685\n",
      "Epoch 150, lr 1.7578125e-07\n",
      "Epoch 150: Train 0.5502347053213512, Test 0.5407424569129944\n",
      "Epoch 175, lr 8.7890625e-08\n",
      "Epoch 175: Train 0.5502103463317571, Test 0.5407227277755737\n",
      "Epoch 200, lr 2.197265625e-08\n",
      "Epoch 200: Train 0.5501899710308746, Test 0.5407049655914307\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.5501813519090303, Test 0.5406986474990845\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.5501754657829299, Test 0.5406942367553711\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.5501696084545793, Test 0.5406900644302368\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.5501638230089838, Test 0.540685772895813\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.5501582339015346, Test 0.5406815409660339\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.5501526551746697, Test 0.5406776070594788\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.5501469880454103, Test 0.5406741499900818\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.5501414518454548, Test 0.5406699776649475\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.5501357890693436, Test 0.5406656861305237\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.5501301373435317, Test 0.5406615734100342\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.5501246421078171, Test 0.540657639503479\n",
      "TRAINING MODEL dt_norm_1_1\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 11.354819965362548, Test 9.471136093139648\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.3758383251726627, Test 1.2205336093902588\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.7854647929780185, Test 0.6478776335716248\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 0.6244086184538901, Test 0.49826544523239136\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 0.5591320237144828, Test 0.43935883045196533\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 0.5345736572518944, Test 0.415025532245636\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 0.5134227038826793, Test 0.386700838804245\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 0.5057200234383344, Test 0.3770601749420166\n",
      "Epoch 200, lr 2.25e-05\n",
      "Epoch 200: Train 0.4713671302422881, Test 0.3505295515060425\n",
      "Epoch 225, lr 2.25e-05\n",
      "Epoch 225: Train 0.46539077353663744, Test 0.3414137363433838\n",
      "Epoch 250, lr 2.25e-05\n",
      "Epoch 250: Train 0.46002111020497977, Test 0.33166414499282837\n",
      "Epoch 275, lr 1.125e-05\n",
      "Epoch 275: Train 0.44364087395370005, Test 0.3176075220108032\n",
      "Epoch 300, lr 5.625e-06\n",
      "Epoch 300: Train 0.43631526581011715, Test 0.31029391288757324\n",
      "Epoch 325, lr 2.8125e-06\n",
      "Epoch 325: Train 0.43236123411916194, Test 0.3058261275291443\n",
      "Epoch 350, lr 7.03125e-07\n",
      "Epoch 350: Train 0.4293719637207687, Test 0.30295008420944214\n",
      "Epoch 375, lr 3.515625e-07\n",
      "Epoch 375: Train 0.4287832625210285, Test 0.3023983836174011\n",
      "Epoch 400, lr 8.7890625e-08\n",
      "Epoch 400: Train 0.4283991022966802, Test 0.3020212650299072\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.4282876802608371, Test 0.3019123077392578\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.42828445159830153, Test 0.3019104599952698\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.42828125404193995, Test 0.3019081950187683\n",
      "TRAINING MODEL dt_norm_1_3\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 20.832210886478425, Test 20.22285270690918\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 17.386097607016563, Test 16.950611114501953\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 16.14617600142956, Test 15.662725448608398\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 15.341435319185257, Test 14.823625564575195\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 14.747247785329819, Test 14.201539039611816\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 14.270287546515465, Test 13.702804565429688\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 13.872103813290597, Test 13.286447525024414\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 13.529252541065215, Test 12.926998138427734\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 13.22999464571476, Test 12.615814208984375\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 12.966116046905517, Test 12.334320068359375\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 12.731258311867714, Test 12.08255672454834\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 12.519580110907555, Test 11.859777450561523\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 12.331214770674706, Test 11.65818977355957\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 12.156469565629958, Test 11.470236778259277\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 11.989434316754341, Test 11.29092788696289\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 11.836651241779327, Test 11.133241653442383\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 11.69825459420681, Test 10.982585906982422\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 11.569701993465424, Test 10.845184326171875\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 11.449950277805328, Test 10.723777770996094\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 11.341618737578392, Test 10.602067947387695\n",
      "TRAINING MODEL dt_norm_1_5\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 22.35095059275627, Test 21.596694946289062\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 19.00398713350296, Test 18.407833099365234\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 17.748117592930793, Test 17.10542869567871\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 16.92830619812012, Test 16.242517471313477\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 16.305784061551094, Test 15.581130981445312\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 15.82132009267807, Test 15.062049865722656\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 15.423573186993599, Test 14.642483711242676\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 15.090983381867408, Test 14.292184829711914\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 14.807606145739555, Test 13.996908187866211\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 14.555449411273003, Test 13.73369026184082\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 14.331968459486962, Test 13.487165451049805\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 14.128616419434547, Test 13.275735855102539\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 13.947944065928459, Test 13.07313346862793\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 13.775569289922714, Test 12.898738861083984\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 13.629653108119964, Test 12.736987113952637\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 13.501214239001275, Test 12.593893051147461\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 13.373835000395776, Test 12.473371505737305\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 13.255564039945602, Test 12.338913917541504\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 13.143462631106377, Test 12.218006134033203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 13.037329003214836, Test 12.102075576782227\n",
      "TRAINING MODEL dt_norm_1_7\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 18.066801822185518, Test 17.316150665283203\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 15.148745366930962, Test 14.648605346679688\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 14.113554382324219, Test 13.599193572998047\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 13.45674255490303, Test 12.913263320922852\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 12.954447835683823, Test 12.409078598022461\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 12.59336048066616, Test 12.013504028320312\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 12.255530506372452, Test 11.680097579956055\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 12.003792786598206, Test 11.398031234741211\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 11.778998005390168, Test 11.162459373474121\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 11.564760929346084, Test 10.946176528930664\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 11.3779456615448, Test 10.758258819580078\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 11.220312592387199, Test 10.588624954223633\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 11.065998330712318, Test 10.436628341674805\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 10.938293609023095, Test 10.29860782623291\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 10.8081375092268, Test 10.160257339477539\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 10.706300622224807, Test 10.035959243774414\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 10.572862622141837, Test 9.922722816467285\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 10.490555593371392, Test 9.818094253540039\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 10.389709037542342, Test 9.715150833129883\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 10.309770405292511, Test 9.626792907714844\n",
      "TRAINING MODEL dt_norm_1_9\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 13.83919142484665, Test 13.22280502319336\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 11.08532648384571, Test 10.74853515625\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 10.266462954878808, Test 9.937294960021973\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 9.745095759630203, Test 9.413312911987305\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 9.380558323860168, Test 9.028390884399414\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 9.07937907129526, Test 8.717263221740723\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 8.836338652670383, Test 8.46583366394043\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 8.634243023395538, Test 8.251057624816895\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 8.465725472569465, Test 8.078388214111328\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 8.2983338534832, Test 7.914156913757324\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 8.171982350945473, Test 7.770771026611328\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 8.043224668502807, Test 7.629717826843262\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 7.936481867730618, Test 7.524857044219971\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 7.838265718519688, Test 7.430341720581055\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 7.751349800825119, Test 7.33596134185791\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 7.658803799748421, Test 7.23382568359375\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 7.57308704406023, Test 7.153765678405762\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 7.4876851186156275, Test 7.06962776184082\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 7.420117531716824, Test 7.003440856933594\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 7.365232355892658, Test 6.929621696472168\n",
      "TRAINING MODEL dt_norm_1_11\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 10.797154188156128, Test 10.271167755126953\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 8.13153183311224, Test 7.935601234436035\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 7.4232494801282884, Test 7.2093963623046875\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 6.995019641518593, Test 6.769294738769531\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 6.685803027451039, Test 6.461155414581299\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 6.452131326496601, Test 6.219760417938232\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 6.257787664234638, Test 6.023432731628418\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 6.09121481180191, Test 5.848894119262695\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 5.95293939858675, Test 5.706177234649658\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 5.829404656589031, Test 5.5939741134643555\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 5.73030177205801, Test 5.486656665802002\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 5.631969827413559, Test 5.386898517608643\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 5.548743753135204, Test 5.300761699676514\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 5.474298079311848, Test 5.217368125915527\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 5.407749092578888, Test 5.155736446380615\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 5.3387645646929744, Test 5.0930986404418945\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 5.283588242530823, Test 5.032890796661377\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 5.22587762773037, Test 4.966620445251465\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 5.172354730963707, Test 4.919539928436279\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 5.126072683930397, Test 4.867895126342773\n",
      "TRAINING MODEL dt_norm_1_13\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 8.782886727154255, Test 8.308187484741211\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 6.058594211935997, Test 5.931199073791504\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 5.457961325347424, Test 5.3234710693359375\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 5.106428894400596, Test 4.988070487976074\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 4.851039443910122, Test 4.7032470703125\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 4.650760991871357, Test 4.495006561279297\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 4.48887023255229, Test 4.331527233123779\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 4.356478315591812, Test 4.20479679107666\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 4.243499092012644, Test 4.079103946685791\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 4.1452130384743215, Test 3.9849162101745605\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 4.057252110540867, Test 3.889864206314087\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 3.9828258253633977, Test 3.827521324157715\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 3.9122275695204736, Test 3.7509713172912598\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 3.846808545291424, Test 3.6725380420684814\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 3.7901939906179907, Test 3.622523546218872\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 3.7360966086387633, Test 3.565962791442871\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 3.6834641106426718, Test 3.520634651184082\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 3.6371033161878588, Test 3.462155818939209\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 3.592987595498562, Test 3.432375907897949\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 3.549136622995138, Test 3.3725051879882812\n",
      "TRAINING MODEL dt_norm_1_15\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 7.753750923275947, Test 7.299567222595215\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 5.059127728641033, Test 4.947182655334473\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 4.480471803247928, Test 4.361220359802246\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 4.139325599372387, Test 4.017883777618408\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 3.90609045997262, Test 3.7826895713806152\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 3.7307867027819155, Test 3.604922294616699\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 3.5928584054112433, Test 3.467531681060791\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 3.476008144766092, Test 3.3480544090270996\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 3.3792478159070014, Test 3.2481870651245117\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 3.2922664783895015, Test 3.1580936908721924\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 3.219139788299799, Test 3.0866217613220215\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 3.1527862802147864, Test 3.01603364944458\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 3.09211408495903, Test 2.9510300159454346\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 3.035744567215443, Test 2.89782452583313\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 2.9875507801771164, Test 2.847684860229492\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 2.94176282659173, Test 2.8078606128692627\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 2.9010506577789785, Test 2.7595479488372803\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 2.8608628436923027, Test 2.7184090614318848\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 2.8248642116785048, Test 2.6790294647216797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 2.7921634025871755, Test 2.6453118324279785\n",
      "TRAINING MODEL dt_norm_1_17\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 6.571960325539112, Test 6.098953723907471\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 4.022074697911739, Test 3.910362958908081\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 3.5229443214833736, Test 3.405642509460449\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 3.245587442815304, Test 3.1318039894104004\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 3.0574514873325827, Test 2.9392545223236084\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 2.911170207709074, Test 2.7860891819000244\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 2.796944850683212, Test 2.672654628753662\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 2.7066728845238686, Test 2.5798583030700684\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 2.6310658007860184, Test 2.5063233375549316\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 2.567702765017748, Test 2.436840057373047\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 2.5123047858476637, Test 2.381157159805298\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 2.4628773666918278, Test 2.3302736282348633\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 2.4199286133050917, Test 2.2857184410095215\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 2.3812927570194007, Test 2.2475461959838867\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 2.347000077366829, Test 2.2101075649261475\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 2.3150428462773562, Test 2.183042049407959\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 2.286910226941109, Test 2.149646282196045\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 2.25804835408926, Test 2.12776517868042\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 2.2333296708762647, Test 2.0990262031555176\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 2.208406164124608, Test 2.069772481918335\n",
      "TRAINING MODEL dt_norm_1_19\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 5.486843690276146, Test 5.10607385635376\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 3.357803598791361, Test 3.2737393379211426\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 2.963899081200361, Test 2.8812930583953857\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 2.7441483877599238, Test 2.655369997024536\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 2.598828426748514, Test 2.502739429473877\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 2.4904294475913047, Test 2.400907516479492\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 2.401940099149942, Test 2.3030436038970947\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 2.331001861393452, Test 2.2331044673919678\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 2.2747693680226804, Test 2.1750080585479736\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 2.225256146490574, Test 2.1316192150115967\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 2.1786234505474567, Test 2.0916714668273926\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 2.1415098644793034, Test 2.042891502380371\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 2.104599018767476, Test 2.010503053665161\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 2.074044283106923, Test 1.9758033752441406\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 2.0470457140356304, Test 1.9449478387832642\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 2.020218539983034, Test 1.9218181371688843\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 1.9937081217765809, Test 1.8956298828125\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 1.9708428572863341, Test 1.872680425643921\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 1.9498924419283867, Test 1.8476653099060059\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 1.9291698817163705, Test 1.8292369842529297\n",
      "TRAINING MODEL dt_norm_1_21\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 5.24360243678093, Test 4.7896223068237305\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.75281725153327, Test 2.672645092010498\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 2.363967314735055, Test 2.283557891845703\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 2.164502004161477, Test 2.080805778503418\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 2.0298882205039264, Test 1.9467967748641968\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 1.9345290280878544, Test 1.8479853868484497\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 1.8611617378890515, Test 1.7720012664794922\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 1.8046204928308724, Test 1.7183012962341309\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 1.7592986326664686, Test 1.6725460290908813\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 1.7183484110981226, Test 1.6289196014404297\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 1.6829254738986492, Test 1.5930651426315308\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 1.6524185221642256, Test 1.5631370544433594\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 1.6252315923571587, Test 1.5327517986297607\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 1.6004146683961153, Test 1.5072933435440063\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 1.5775060337036848, Test 1.487963080406189\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 1.5568402167409658, Test 1.4645233154296875\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 1.5390271838754415, Test 1.443880558013916\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 1.5224560104310512, Test 1.4269609451293945\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 1.5070011503994465, Test 1.4132676124572754\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 1.4936219166964293, Test 1.3991308212280273\n",
      "TRAINING MODEL dt_norm_1_23\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 4.346996491402388, Test 3.9530463218688965\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.3292716089636087, Test 2.2654290199279785\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 2.0197476029396055, Test 1.956682801246643\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 1.8519977752119303, Test 1.7875442504882812\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 1.7472738228738307, Test 1.6792727708816528\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 1.6685491152107716, Test 1.600405216217041\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 1.6051388692110777, Test 1.5353670120239258\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 1.556281155720353, Test 1.484663724899292\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 1.5184959817677737, Test 1.4470806121826172\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 1.4831719130277634, Test 1.4147528409957886\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 1.4539641175419091, Test 1.3805994987487793\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 1.4274555455893279, Test 1.3546971082687378\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 1.4027421958744526, Test 1.328370213508606\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 1.383544896170497, Test 1.3107703924179077\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 1.363860348239541, Test 1.2894024848937988\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 1.3452101070433855, Test 1.270581603050232\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 1.3301498409360648, Test 1.2557951211929321\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 1.3156133718788623, Test 1.2427184581756592\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 1.3019312907010316, Test 1.2290093898773193\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 1.2892015758901834, Test 1.2123597860336304\n",
      "TRAINING MODEL dt_norm_1_25\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 4.23963242918253, Test 3.820232391357422\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.1526743952184915, Test 2.096223831176758\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 1.862507900968194, Test 1.8094197511672974\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 1.7042095314711332, Test 1.6475828886032104\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 1.6060309577733278, Test 1.5522143840789795\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 1.5290775891393422, Test 1.472376823425293\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 1.4673407137393952, Test 1.4104924201965332\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 1.4193966265767812, Test 1.361450433731079\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 1.3829223796725274, Test 1.3252487182617188\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 1.344990112259984, Test 1.2833030223846436\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 1.3150993295013904, Test 1.2542130947113037\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 1.2917031669989227, Test 1.2337661981582642\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 1.2726302314549685, Test 1.2116222381591797\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 1.2561359465122224, Test 1.192209243774414\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 1.240406378172338, Test 1.180112361907959\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 1.2251954708248376, Test 1.161375880241394\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 1.2126280292868614, Test 1.1514644622802734\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 1.2018719494342804, Test 1.1394504308700562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 1.1900755373761058, Test 1.1277687549591064\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 1.1793794179335237, Test 1.1182239055633545\n",
      "TRAINING MODEL dt_norm_1_27\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 2.6733820874243976, Test 2.307187795639038\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.1997951872646808, Test 1.1585280895233154\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 1.0328894503414632, Test 0.9883502125740051\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 0.9486771238967776, Test 0.9052211046218872\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 0.8859469076618552, Test 0.8389362096786499\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 0.8473065532743931, Test 0.7982032299041748\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 0.8118439260870218, Test 0.7629433274269104\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 0.7878241343423724, Test 0.7398346662521362\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 0.7673718763515354, Test 0.7173184156417847\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 0.751275729201734, Test 0.7030823826789856\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 0.7399844955652952, Test 0.6893336176872253\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 0.7290109142661094, Test 0.6785642504692078\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 0.7183957451954484, Test 0.6679057478904724\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 0.7098517775535583, Test 0.6592706441879272\n",
      "Epoch 350, lr 4.5e-05\n",
      "Epoch 350: Train 0.7006478313356638, Test 0.6504104137420654\n",
      "Epoch 375, lr 4.5e-05\n",
      "Epoch 375: Train 0.69429741948843, Test 0.6464435458183289\n",
      "Epoch 400, lr 4.5e-05\n",
      "Epoch 400: Train 0.6883913885802031, Test 0.6374162435531616\n",
      "Epoch 425, lr 4.5e-05\n",
      "Epoch 425: Train 0.6827315897680819, Test 0.6345428824424744\n",
      "Epoch 450, lr 4.5e-05\n",
      "Epoch 450: Train 0.6741581524722278, Test 0.6215353012084961\n",
      "Epoch 475, lr 4.5e-05\n",
      "Epoch 475: Train 0.6692294653505086, Test 0.6203256845474243\n",
      "TRAINING MODEL dt_norm_1_29\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.8332518626004457, Test 1.7603312730789185\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.7279106698930264, Test 1.687009572982788\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 1.7244678728282452, Test 1.6813032627105713\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 1.718933839723468, Test 1.6750767230987549\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 1.7141040671616792, Test 1.668454885482788\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 1.7113834783434867, Test 1.6636879444122314\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 1.7083245508372784, Test 1.6590481996536255\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 1.6996760439127683, Test 1.6518163681030273\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 1.6939410977065563, Test 1.6466460227966309\n",
      "Epoch 225, lr 5.625e-06\n",
      "Epoch 225: Train 1.6910721477121116, Test 1.6441541910171509\n",
      "Epoch 250, lr 2.8125e-06\n",
      "Epoch 250: Train 1.6887731924653053, Test 1.6422516107559204\n",
      "Epoch 275, lr 1.40625e-06\n",
      "Epoch 275: Train 1.6876454181969165, Test 1.6412794589996338\n",
      "Epoch 300, lr 3.515625e-07\n",
      "Epoch 300: Train 1.6870392568409442, Test 1.6407872438430786\n",
      "Epoch 325, lr 8.7890625e-08\n",
      "Epoch 325: Train 1.6868692822754383, Test 1.6406522989273071\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 1.6868110585957765, Test 1.6406066417694092\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 1.6867848977446556, Test 1.6405861377716064\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 1.6867655765265226, Test 1.640570878982544\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 1.6867466922849417, Test 1.64055597782135\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 1.6867282167077065, Test 1.6405413150787354\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 1.6867100819945335, Test 1.6405270099639893\n",
      "TRAINING MODEL dt_norm_1_30\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.7811498682945968, Test 1.6657187938690186\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.4660237450152636, Test 1.4356328248977661\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 1.446521146222949, Test 1.4124436378479004\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 1.4332036461681128, Test 1.3980201482772827\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 1.4228513337671758, Test 1.3870187997817993\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 1.4127836275845767, Test 1.3751897811889648\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 1.4058074325323104, Test 1.3683290481567383\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 1.3991381216794252, Test 1.3605176210403442\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 1.3941678378731013, Test 1.3555176258087158\n",
      "Epoch 225, lr 2.25e-05\n",
      "Epoch 225: Train 1.3811077378690242, Test 1.3436294794082642\n",
      "Epoch 250, lr 1.125e-05\n",
      "Epoch 250: Train 1.3736941490322352, Test 1.3370497226715088\n",
      "Epoch 275, lr 1.125e-05\n",
      "Epoch 275: Train 1.3719685710966587, Test 1.3353304862976074\n",
      "Epoch 300, lr 5.625e-06\n",
      "Epoch 300: Train 1.3682991590350866, Test 1.3316311836242676\n",
      "Epoch 325, lr 5.625e-06\n",
      "Epoch 325: Train 1.3674267049878837, Test 1.3309406042099\n",
      "Epoch 350, lr 2.8125e-06\n",
      "Epoch 350: Train 1.3656076218932867, Test 1.3290915489196777\n",
      "Epoch 375, lr 1.40625e-06\n",
      "Epoch 375: Train 1.364673475921154, Test 1.3281800746917725\n",
      "Epoch 400, lr 3.515625e-07\n",
      "Epoch 400: Train 1.3640820987522602, Test 1.3276185989379883\n",
      "Epoch 425, lr 8.7890625e-08\n",
      "Epoch 425: Train 1.3639196936041116, Test 1.327469825744629\n",
      "Epoch 450, lr 2.197265625e-08\n",
      "Epoch 450: Train 1.3638730123639107, Test 1.3274279832839966\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 1.363859010115266, Test 1.3274166584014893\n",
      "TRAINING MODEL dt_norm_1_31\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.647841487452388, Test 1.5133394002914429\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.3165496066212654, Test 1.2954261302947998\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 1.3031861416995525, Test 1.2783464193344116\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 1.296483675763011, Test 1.2703158855438232\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 1.2843896659091114, Test 1.2562212944030762\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 1.2822124861180781, Test 1.2543413639068604\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 1.2748209906741976, Test 1.2462646961212158\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 1.2731371941044927, Test 1.2445132732391357\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 1.2722853258252145, Test 1.2434738874435425\n",
      "Epoch 225, lr 1.125e-05\n",
      "Epoch 225: Train 1.2716473160311579, Test 1.2426857948303223\n",
      "Epoch 250, lr 1.125e-05\n",
      "Epoch 250: Train 1.2710172343999147, Test 1.2414989471435547\n",
      "Epoch 275, lr 1.125e-05\n",
      "Epoch 275: Train 1.2699820147827268, Test 1.2406319379806519\n",
      "Epoch 300, lr 1.125e-05\n",
      "Epoch 300: Train 1.268697634898126, Test 1.2396628856658936\n",
      "Epoch 325, lr 1.125e-05\n",
      "Epoch 325: Train 1.2675832699984313, Test 1.2380638122558594\n",
      "Epoch 350, lr 1.125e-05\n",
      "Epoch 350: Train 1.2667271690443158, Test 1.2372071743011475\n",
      "Epoch 375, lr 1.125e-05\n",
      "Epoch 375: Train 1.2657765220850705, Test 1.2357395887374878\n",
      "Epoch 400, lr 1.125e-05\n",
      "Epoch 400: Train 1.2648461347445845, Test 1.2351046800613403\n",
      "Epoch 425, lr 1.125e-05\n",
      "Epoch 425: Train 1.2640917401760816, Test 1.234243631362915\n",
      "Epoch 450, lr 1.125e-05\n",
      "Epoch 450: Train 1.2633055087178946, Test 1.233180284500122\n",
      "Epoch 475, lr 1.125e-05\n",
      "Epoch 475: Train 1.2619720054790378, Test 1.2317700386047363\n",
      "TRAINING MODEL dt_norm_1_32\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.44504689052701, Test 1.323612928390503\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 1.1775792501866817, Test 1.1548455953598022\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 1.1685687301680445, Test 1.1443451642990112\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 1.1616175930947066, Test 1.1378082036972046\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 1.155072165466845, Test 1.1300976276397705\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 1.1481393720954656, Test 1.1238692998886108\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 1.139142018929124, Test 1.1129628419876099\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 1.1297392399981618, Test 1.1039706468582153\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 1.1263720368966461, Test 1.1007881164550781\n",
      "Epoch 225, lr 1.125e-05\n",
      "Epoch 225: Train 1.1235272461548447, Test 1.0979453325271606\n",
      "Epoch 250, lr 1.125e-05\n",
      "Epoch 250: Train 1.1207263842225075, Test 1.0952163934707642\n",
      "Epoch 275, lr 1.125e-05\n",
      "Epoch 275: Train 1.1178532443940639, Test 1.0918623208999634\n",
      "Epoch 300, lr 1.125e-05\n",
      "Epoch 300: Train 1.1155413648113608, Test 1.0895185470581055\n",
      "Epoch 325, lr 1.125e-05\n",
      "Epoch 325: Train 1.1133601425215602, Test 1.087178349494934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350, lr 5.625e-06\n",
      "Epoch 350: Train 1.1090116394683718, Test 1.0831583738327026\n",
      "Epoch 375, lr 5.625e-06\n",
      "Epoch 375: Train 1.1075076341629029, Test 1.0815556049346924\n",
      "Epoch 400, lr 5.625e-06\n",
      "Epoch 400: Train 1.106085877865553, Test 1.0802326202392578\n",
      "Epoch 425, lr 5.625e-06\n",
      "Epoch 425: Train 1.10459755808115, Test 1.0787941217422485\n",
      "Epoch 450, lr 5.625e-06\n",
      "Epoch 450: Train 1.103187308833003, Test 1.07698655128479\n",
      "Epoch 475, lr 5.625e-06\n",
      "Epoch 475: Train 1.1016879165545106, Test 1.0755586624145508\n",
      "TRAINING MODEL dt_norm_1_33\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.154013487137854, Test 1.0694735050201416\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.9943866284564138, Test 0.9791590571403503\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.9966010805219412, Test 0.9803684949874878\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.997205694951117, Test 0.9807583689689636\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.9973281556740403, Test 0.9808381199836731\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.9973071597516536, Test 0.9808162450790405\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.997263448126614, Test 0.9807788729667664\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.9972234103828669, Test 0.9807442426681519\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.9971835855394602, Test 0.9807105660438538\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.9971432434394956, Test 0.9806761145591736\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.9971024803817272, Test 0.98064124584198\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.9970619633793831, Test 0.9806066751480103\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.997021516226232, Test 0.9805721640586853\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.996981312148273, Test 0.9805376529693604\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.9969413170590997, Test 0.9805034399032593\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.9969015143811703, Test 0.9804694652557373\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.9968617785722017, Test 0.9804357290267944\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.996822297014296, Test 0.9804019927978516\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.9967830970883369, Test 0.9803686141967773\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.9967444449663162, Test 0.9803357124328613\n",
      "TRAINING MODEL dt_norm_2_1\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 11.91475469415838, Test 10.862446784973145\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 9.843798261700254, Test 9.849395751953125\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 9.806007414153129, Test 9.81103515625\n",
      "Epoch 75, lr 1.125e-05\n",
      "Epoch 75: Train 9.796083623712713, Test 9.795127868652344\n",
      "Epoch 100, lr 5.625e-06\n",
      "Epoch 100: Train 9.7916347619259, Test 9.788747787475586\n",
      "Epoch 125, lr 1.40625e-06\n",
      "Epoch 125: Train 9.787469950589267, Test 9.785493850708008\n",
      "Epoch 150, lr 7.03125e-07\n",
      "Epoch 150: Train 9.77214798782811, Test 9.770901679992676\n",
      "Epoch 175, lr 1.7578125e-07\n",
      "Epoch 175: Train 9.771620750427246, Test 9.77031135559082\n",
      "Epoch 200, lr 4.39453125e-08\n",
      "Epoch 200: Train 9.771479895620635, Test 9.770203590393066\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 9.771454348708644, Test 9.77017593383789\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 9.771453279437441, Test 9.770173072814941\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 9.771453250538219, Test 9.770176887512207\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 9.771452007871686, Test 9.77017593383789\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 9.771452845949115, Test 9.770174980163574\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 9.7714512275927, Test 9.77017593383789\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 9.771452065670129, Test 9.770174980163574\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 9.771451372088809, Test 9.770174026489258\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 9.771451863375576, Test 9.770177841186523\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 9.771449435840953, Test 9.770174026489258\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 9.771449782631613, Test 9.77017593383789\n",
      "TRAINING MODEL dt_norm_2_3\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 15.638725382089614, Test 15.16859245300293\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 13.207327377796172, Test 13.118098258972168\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 13.006362274289131, Test 12.911314010620117\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 12.918005129694938, Test 12.823219299316406\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 12.819291025400162, Test 12.720268249511719\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 12.764738434553147, Test 12.662527084350586\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 12.727827659249305, Test 12.636089324951172\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 12.714085066318512, Test 12.624279022216797\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 12.6792813539505, Test 12.584835052490234\n",
      "Epoch 225, lr 2.8125e-06\n",
      "Epoch 225: Train 12.673267900943756, Test 12.578941345214844\n",
      "Epoch 250, lr 7.03125e-07\n",
      "Epoch 250: Train 12.66884800195694, Test 12.57432746887207\n",
      "Epoch 275, lr 1.7578125e-07\n",
      "Epoch 275: Train 12.668276378512383, Test 12.573700904846191\n",
      "Epoch 300, lr 4.39453125e-08\n",
      "Epoch 300: Train 12.668142545223237, Test 12.573558807373047\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 12.668110060691834, Test 12.573528289794922\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 12.66810771226883, Test 12.573525428771973\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 12.66810493171215, Test 12.573524475097656\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 12.668102473020554, Test 12.573521614074707\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 12.66809984743595, Test 12.573519706726074\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 12.66809730231762, Test 12.573517799377441\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 12.668094757199288, Test 12.573514938354492\n",
      "TRAINING MODEL dt_norm_2_5\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 18.440840950608255, Test 18.08876609802246\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 16.389767956733703, Test 16.317249298095703\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 16.005779987573625, Test 15.940702438354492\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 15.786814084649086, Test 15.723955154418945\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 15.696909585595131, Test 15.640846252441406\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 15.638434931635857, Test 15.581673622131348\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 15.585798344016075, Test 15.526580810546875\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 15.509678572416306, Test 15.426965713500977\n",
      "Epoch 200, lr 2.25e-05\n",
      "Epoch 200: Train 15.464221876859664, Test 15.375259399414062\n",
      "Epoch 225, lr 2.25e-05\n",
      "Epoch 225: Train 15.436384263634682, Test 15.350669860839844\n",
      "Epoch 250, lr 2.25e-05\n",
      "Epoch 250: Train 15.420836317539216, Test 15.335384368896484\n",
      "Epoch 275, lr 1.125e-05\n",
      "Epoch 275: Train 15.396775141358376, Test 15.31027603149414\n",
      "Epoch 300, lr 1.40625e-06\n",
      "Epoch 300: Train 15.385051938891412, Test 15.29767894744873\n",
      "Epoch 325, lr 3.515625e-07\n",
      "Epoch 325: Train 15.38191723227501, Test 15.294663429260254\n",
      "Epoch 350, lr 8.7890625e-08\n",
      "Epoch 350: Train 15.381510102748871, Test 15.29423713684082\n",
      "Epoch 375, lr 2.197265625e-08\n",
      "Epoch 375: Train 15.381414899230004, Test 15.294130325317383\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 15.381394788622856, Test 15.294110298156738\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 15.381389385461807, Test 15.294107437133789\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 15.381384247541428, Test 15.29410171508789\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 15.381379041075707, Test 15.294095993041992\n",
      "TRAINING MODEL dt_norm_2_7\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 18.311282348632812, Test 18.220121383666992\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 16.93028109818697, Test 16.864749908447266\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 16.687085610628127, Test 16.605998992919922\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 16.5265475705266, Test 16.43960952758789\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 16.37144647538662, Test 16.398685455322266\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 16.18854840397835, Test 16.23906135559082\n",
      "Epoch 150, lr 2.8125e-06\n",
      "Epoch 150: Train 16.164163811504842, Test 16.215595245361328\n",
      "Epoch 175, lr 7.03125e-07\n",
      "Epoch 175: Train 16.158100230991842, Test 16.209896087646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, lr 1.7578125e-07\n",
      "Epoch 200: Train 16.1566929936409, Test 16.208600997924805\n",
      "Epoch 225, lr 4.39453125e-08\n",
      "Epoch 225: Train 16.15632920116186, Test 16.208267211914062\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 16.156248196959496, Test 16.208192825317383\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 16.156229707598687, Test 16.208175659179688\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 16.156211805343627, Test 16.208158493041992\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 16.15619299411774, Test 16.208141326904297\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 16.156175217032434, Test 16.20812225341797\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 16.15615695863962, Test 16.208105087280273\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 16.15613876581192, Test 16.208086013793945\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 16.15612068027258, Test 16.20806884765625\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 16.156102958321572, Test 16.208053588867188\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 16.156084625422956, Test 16.20803451538086\n",
      "TRAINING MODEL dt_norm_2_9\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 13.852858048677444, Test 13.623037338256836\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 12.782231429219246, Test 12.63049602508545\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 12.423103785514831, Test 12.354649543762207\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 12.242906801402569, Test 12.227067947387695\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 12.184486700594425, Test 12.183206558227539\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 12.160818056762219, Test 12.156291961669922\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 12.146705408394336, Test 12.142923355102539\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 12.129655994474888, Test 12.127917289733887\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 12.120687648653984, Test 12.118805885314941\n",
      "Epoch 225, lr 7.03125e-07\n",
      "Epoch 225: Train 12.116847738623619, Test 12.115482330322266\n",
      "Epoch 250, lr 1.7578125e-07\n",
      "Epoch 250: Train 12.115946470201015, Test 12.114614486694336\n",
      "Epoch 275, lr 2.197265625e-08\n",
      "Epoch 275: Train 12.115725322067737, Test 12.11441421508789\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 12.115673086047172, Test 12.11436939239502\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 12.115662357211113, Test 12.114357948303223\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 12.115651999413966, Test 12.114348411560059\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 12.115641452372074, Test 12.114340782165527\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 12.115631034970283, Test 12.11432933807373\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 12.115620404481888, Test 12.11431884765625\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 12.11560974419117, Test 12.114309310913086\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 12.115599516034127, Test 12.114299774169922\n",
      "TRAINING MODEL dt_norm_2_11\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 8.94235943555832, Test 8.669257164001465\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 7.614942546188831, Test 7.454289436340332\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 7.503785209357739, Test 7.295612812042236\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 7.342744471132756, Test 7.221063137054443\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 7.30482227653265, Test 7.144288063049316\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 7.201139876246453, Test 7.090436935424805\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 7.155185249447823, Test 7.061587810516357\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 7.175486472249031, Test 7.045368194580078\n",
      "Epoch 200, lr 2.25e-05\n",
      "Epoch 200: Train 7.043162307143211, Test 6.999783515930176\n",
      "Epoch 225, lr 1.125e-05\n",
      "Epoch 225: Train 7.051426886022091, Test 6.998597145080566\n",
      "Epoch 250, lr 5.625e-06\n",
      "Epoch 250: Train 7.019084829092026, Test 6.978845596313477\n",
      "Epoch 275, lr 2.8125e-06\n",
      "Epoch 275: Train 7.01560475230217, Test 6.974949359893799\n",
      "Epoch 300, lr 7.03125e-07\n",
      "Epoch 300: Train 7.013385488092899, Test 6.972623348236084\n",
      "Epoch 325, lr 8.7890625e-08\n",
      "Epoch 325: Train 7.012769892811775, Test 6.972026348114014\n",
      "Epoch 350, lr 2.197265625e-08\n",
      "Epoch 350: Train 7.012688258290291, Test 6.971944808959961\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 7.012674836814403, Test 6.97192907333374\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 7.012671183049679, Test 6.971926689147949\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 7.012667761743069, Test 6.971924781799316\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 7.012664899230003, Test 6.971920013427734\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 7.012661224603653, Test 6.971917152404785\n",
      "TRAINING MODEL dt_norm_2_13\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 6.504169617593289, Test 6.2060041427612305\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 5.305392128229141, Test 5.244833946228027\n",
      "Epoch 50, lr 1.125e-05\n",
      "Epoch 50: Train 5.237594050168991, Test 5.182391166687012\n",
      "Epoch 75, lr 1.125e-05\n",
      "Epoch 75: Train 5.213005241006613, Test 5.15757417678833\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 5.198129627853632, Test 5.143641948699951\n",
      "Epoch 125, lr 5.625e-06\n",
      "Epoch 125: Train 5.1880027882754804, Test 5.133974075317383\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 5.183204585313797, Test 5.128796100616455\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 5.178946355730295, Test 5.1251983642578125\n",
      "Epoch 200, lr 5.625e-06\n",
      "Epoch 200: Train 5.175014740228653, Test 5.121517181396484\n",
      "Epoch 225, lr 1.40625e-06\n",
      "Epoch 225: Train 5.1709367141127585, Test 5.117307662963867\n",
      "Epoch 250, lr 3.515625e-07\n",
      "Epoch 250: Train 5.1687705799937245, Test 5.1150617599487305\n",
      "Epoch 275, lr 8.7890625e-08\n",
      "Epoch 275: Train 5.1684317968785765, Test 5.114757537841797\n",
      "Epoch 300, lr 2.197265625e-08\n",
      "Epoch 300: Train 5.168345718830824, Test 5.114677906036377\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 5.168329114466905, Test 5.114662170410156\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 5.168321622163058, Test 5.114655494689941\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 5.168314510583878, Test 5.114649772644043\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 5.168307321518659, Test 5.114643096923828\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 5.168300069868565, Test 5.114636421203613\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 5.168292883038521, Test 5.114629745483398\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 5.168285619467497, Test 5.114622592926025\n",
      "TRAINING MODEL dt_norm_2_15\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 5.28263529241085, Test 4.9548139572143555\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 4.242213755100965, Test 4.191015243530273\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 4.1743376269936565, Test 4.116889953613281\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 4.049643028527498, Test 3.99652099609375\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 4.031434173136949, Test 3.9794669151306152\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 3.9933575287461283, Test 3.9346671104431152\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 3.9850186966359615, Test 3.9277544021606445\n",
      "Epoch 175, lr 2.8125e-06\n",
      "Epoch 175: Train 3.9804853975772856, Test 3.92238712310791\n",
      "Epoch 200, lr 3.515625e-07\n",
      "Epoch 200: Train 3.97926819473505, Test 3.921224355697632\n",
      "Epoch 225, lr 8.7890625e-08\n",
      "Epoch 225: Train 3.978932935744524, Test 3.9209275245666504\n",
      "Epoch 250, lr 2.197265625e-08\n",
      "Epoch 250: Train 3.9788933485746383, Test 3.9208884239196777\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 3.9788846991956235, Test 3.9208791255950928\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 3.9788819551467896, Test 3.920877456665039\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 3.9788790479302407, Test 3.9208760261535645\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 3.9788762383162974, Test 3.9208738803863525\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 3.9788733400404452, Test 3.9208717346191406\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 3.978870466351509, Test 3.920869827270508\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 3.978867746889591, Test 3.920868396759033\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 3.9788648784160614, Test 3.9208664894104004\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 3.9788620077073573, Test 3.9208638668060303\n",
      "TRAINING MODEL dt_norm_2_17\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 4.575780783593655, Test 4.233343124389648\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 3.596513621509075, Test 3.559216260910034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 3.5370469741523265, Test 3.501936912536621\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 3.515253622084856, Test 3.4748597145080566\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 3.4801401935517786, Test 3.440030097961426\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 3.468618004024029, Test 3.4308266639709473\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 3.4593882121145727, Test 3.4188246726989746\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 3.4430885650217533, Test 3.4061245918273926\n",
      "Epoch 200, lr 5.625e-06\n",
      "Epoch 200: Train 3.4331147730350495, Test 3.3947601318359375\n",
      "Epoch 225, lr 2.8125e-06\n",
      "Epoch 225: Train 3.430597872287035, Test 3.3921737670898438\n",
      "Epoch 250, lr 1.40625e-06\n",
      "Epoch 250: Train 3.4297018758952618, Test 3.390692710876465\n",
      "Epoch 275, lr 1.7578125e-07\n",
      "Epoch 275: Train 3.429108028113842, Test 3.3901634216308594\n",
      "Epoch 300, lr 8.7890625e-08\n",
      "Epoch 300: Train 3.4289631485939025, Test 3.3900034427642822\n",
      "Epoch 325, lr 2.197265625e-08\n",
      "Epoch 325: Train 3.428924824297428, Test 3.389960289001465\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 3.4289173752069475, Test 3.38995361328125\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 3.4289157070219516, Test 3.3899519443511963\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 3.42891364172101, Test 3.389951229095459\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 3.4289116889238356, Test 3.3899505138397217\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 3.428909696638584, Test 3.3899497985839844\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 3.4289080426096916, Test 3.3899483680725098\n",
      "TRAINING MODEL dt_norm_2_19\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 4.6145752690732476, Test 4.293909072875977\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 3.6606935299932957, Test 3.6332335472106934\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 3.626944263279438, Test 3.600050210952759\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 3.575676432996988, Test 3.542393684387207\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 3.5397600598633288, Test 3.507214069366455\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 3.53090847954154, Test 3.497969388961792\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 3.5239067181944845, Test 3.4913249015808105\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 3.5203661620616913, Test 3.4878673553466797\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 3.518740765750408, Test 3.4864747524261475\n",
      "Epoch 225, lr 7.03125e-07\n",
      "Epoch 225: Train 3.5176099352538586, Test 3.485175609588623\n",
      "Epoch 250, lr 1.7578125e-07\n",
      "Epoch 250: Train 3.517330702394247, Test 3.4848499298095703\n",
      "Epoch 275, lr 4.39453125e-08\n",
      "Epoch 275: Train 3.5172554180026054, Test 3.4847846031188965\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 3.5172359317541124, Test 3.4847657680511475\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 3.517235068976879, Test 3.4847652912139893\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 3.517234644293785, Test 3.4847660064697266\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 3.517233555018902, Test 3.4847657680511475\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 3.5172329947352408, Test 3.48476505279541\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 3.517232445627451, Test 3.4847640991210938\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 3.5172317810356617, Test 3.48476505279541\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 3.517230935394764, Test 3.484764337539673\n",
      "TRAINING MODEL dt_norm_2_21\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 3.2873213663697243, Test 3.058411121368408\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.64215344004333, Test 2.627440929412842\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 2.558854450285435, Test 2.5368924140930176\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 2.5370150111615657, Test 2.5175137519836426\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 2.5145442854613065, Test 2.4881858825683594\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 2.485245968028903, Test 2.460235118865967\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 2.4717326018959285, Test 2.4456377029418945\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 2.4576221611350775, Test 2.431039571762085\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 2.453269137814641, Test 2.4276084899902344\n",
      "Epoch 225, lr 5.625e-06\n",
      "Epoch 225: Train 2.451676782593131, Test 2.4250667095184326\n",
      "Epoch 250, lr 2.8125e-06\n",
      "Epoch 250: Train 2.446235469728708, Test 2.4199259281158447\n",
      "Epoch 275, lr 3.515625e-07\n",
      "Epoch 275: Train 2.4446365270763635, Test 2.4182047843933105\n",
      "Epoch 300, lr 1.7578125e-07\n",
      "Epoch 300: Train 2.4442442916333675, Test 2.4178147315979004\n",
      "Epoch 325, lr 2.197265625e-08\n",
      "Epoch 325: Train 2.4441390939056875, Test 2.4177122116088867\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 2.444126191362739, Test 2.41770076751709\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 2.4441207993775604, Test 2.417696952819824\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 2.4441152587532997, Test 2.417691230773926\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 2.444109722971916, Test 2.417686700820923\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 2.4441042099148036, Test 2.417682647705078\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 2.4440987180918454, Test 2.417677879333496\n",
      "TRAINING MODEL dt_norm_2_23\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 3.2269211357290093, Test 2.936285972595215\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.5685311309941166, Test 2.5497074127197266\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 2.5284709709507602, Test 2.5000460147857666\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 2.5172240546533278, Test 2.4923620223999023\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 2.4959678579043674, Test 2.4688310623168945\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 2.47508028420535, Test 2.446608066558838\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 2.4700918906218523, Test 2.4403724670410156\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 2.4694734161550347, Test 2.4393389225006104\n",
      "Epoch 200, lr 5.625e-06\n",
      "Epoch 200: Train 2.4626394135135037, Test 2.4331798553466797\n",
      "Epoch 225, lr 2.8125e-06\n",
      "Epoch 225: Train 2.461573068912213, Test 2.4318084716796875\n",
      "Epoch 250, lr 7.03125e-07\n",
      "Epoch 250: Train 2.4606542295509284, Test 2.4307193756103516\n",
      "Epoch 275, lr 8.7890625e-08\n",
      "Epoch 275: Train 2.460440800323353, Test 2.430508613586426\n",
      "Epoch 300, lr 2.197265625e-08\n",
      "Epoch 300: Train 2.460374826734716, Test 2.4304451942443848\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 2.460369322266612, Test 2.43044114112854\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 2.460368529066339, Test 2.430440902709961\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 2.4603681039143277, Test 2.430441379547119\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 2.4603674424278155, Test 2.430441379547119\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 2.4603666242186004, Test 2.430440902709961\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 2.4603659527285116, Test 2.430440902709961\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 2.4603653600165893, Test 2.430440902709961\n",
      "TRAINING MODEL dt_norm_2_25\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 2.6923746806753557, Test 2.4149599075317383\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.113447436505118, Test 2.080017566680908\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 2.031065621618497, Test 1.9834387302398682\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 2.0202362961688283, Test 1.9756087064743042\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 2.0120567325818337, Test 1.9609489440917969\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 1.9921149547490697, Test 1.9424424171447754\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 1.988192709152308, Test 1.9354069232940674\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 1.9777088683877289, Test 1.9228034019470215\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 1.9748866591749892, Test 1.9197814464569092\n",
      "Epoch 225, lr 7.03125e-07\n",
      "Epoch 225: Train 1.974234400495971, Test 1.9190278053283691\n",
      "Epoch 250, lr 1.7578125e-07\n",
      "Epoch 250: Train 1.9740643036567558, Test 1.9188053607940674\n",
      "Epoch 275, lr 4.39453125e-08\n",
      "Epoch 275: Train 1.974020235282553, Test 1.9187514781951904\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 1.9740097199456166, Test 1.91874098777771\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 1.9740089077060505, Test 1.918741226196289\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 1.97400823959523, Test 1.9187407493591309\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 1.9740075687904142, Test 1.918741226196289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 1.9740068791276317, Test 1.91874098777771\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 1.97400618340336, Test 1.9187405109405518\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 1.97400532805987, Test 1.9187402725219727\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 1.9740046215596172, Test 1.9187405109405518\n",
      "TRAINING MODEL dt_norm_2_27\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.5856814028380752, Test 1.372949242591858\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 1.1601677962711878, Test 1.1424410343170166\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 1.1205718559878213, Test 1.0953384637832642\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 1.1048895410903088, Test 1.0784547328948975\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 1.0855439142747358, Test 1.057971477508545\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 1.078352197811201, Test 1.0480488538742065\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 1.070343946868723, Test 1.0397300720214844\n",
      "Epoch 175, lr 2.8125e-06\n",
      "Epoch 175: Train 1.068539491721562, Test 1.037903070449829\n",
      "Epoch 200, lr 7.03125e-07\n",
      "Epoch 200: Train 1.067958969187427, Test 1.0373055934906006\n",
      "Epoch 225, lr 8.7890625e-08\n",
      "Epoch 225: Train 1.0677895151175463, Test 1.037154197692871\n",
      "Epoch 250, lr 2.197265625e-08\n",
      "Epoch 250: Train 1.0677692553439697, Test 1.0371397733688354\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 1.0677664051582287, Test 1.037136435508728\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 1.067766390063546, Test 1.0371360778808594\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 1.0677664260585586, Test 1.0371366739273071\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 1.067766293689802, Test 1.0371358394622803\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 1.0677663463276703, Test 1.0371358394622803\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 1.0677664500552337, Test 1.0371359586715698\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 1.0677663707113885, Test 1.0371360778808594\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 1.0677663265884696, Test 1.0371357202529907\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 1.0677664616665283, Test 1.0371358394622803\n",
      "TRAINING MODEL dt_norm_2_29\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.1534876923105415, Test 1.0254939794540405\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.8438254621248603, Test 0.8219019174575806\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 0.8509612956144704, Test 0.8281959295272827\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.8341899921055946, Test 0.8098798990249634\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 0.8321901484561048, Test 0.8074936866760254\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 0.8309226811135588, Test 0.8041414022445679\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 0.8279766015225303, Test 0.8015997409820557\n",
      "Epoch 175, lr 2.8125e-06\n",
      "Epoch 175: Train 0.8275459962086466, Test 0.8009154796600342\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 0.8276788641161479, Test 0.8008717894554138\n",
      "Epoch 225, lr 7.03125e-07\n",
      "Epoch 225: Train 0.8272840610543208, Test 0.8005704879760742\n",
      "Epoch 250, lr 1.7578125e-07\n",
      "Epoch 250: Train 0.8272040500575772, Test 0.8004840612411499\n",
      "Epoch 275, lr 2.197265625e-08\n",
      "Epoch 275: Train 0.827172059247925, Test 0.8004472255706787\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.8271634680419245, Test 0.8004424571990967\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.8271575747089581, Test 0.8004390597343445\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.8271518703613672, Test 0.8004368543624878\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.8271460952205463, Test 0.8004342317581177\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.8271403933141012, Test 0.8004312515258789\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.8271346981208072, Test 0.8004287481307983\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.8271289539011673, Test 0.8004252910614014\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.8271232682690278, Test 0.8004224300384521\n",
      "TRAINING MODEL dt_norm_2_30\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.316464947591914, Test 1.1483139991760254\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.9659400862632411, Test 0.9552435874938965\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.9491149246102513, Test 0.933840274810791\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.9364058266181757, Test 0.9207409620285034\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.9313189729605571, Test 0.9144814610481262\n",
      "Epoch 125, lr 5.625e-06\n",
      "Epoch 125: Train 0.9265456108173521, Test 0.9099632501602173\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 0.9226206691548375, Test 0.9054087400436401\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 0.9208557045105661, Test 0.9037171602249146\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 0.9194440118747182, Test 0.9021719694137573\n",
      "Epoch 225, lr 2.8125e-06\n",
      "Epoch 225: Train 0.9161266528143741, Test 0.89884352684021\n",
      "Epoch 250, lr 2.8125e-06\n",
      "Epoch 250: Train 0.914848392847741, Test 0.8975470066070557\n",
      "Epoch 275, lr 2.8125e-06\n",
      "Epoch 275: Train 0.9140723436185629, Test 0.8964561223983765\n",
      "Epoch 300, lr 2.8125e-06\n",
      "Epoch 300: Train 0.906994102614941, Test 0.8885549306869507\n",
      "Epoch 325, lr 2.8125e-06\n",
      "Epoch 325: Train 0.8894649561088864, Test 0.8717361688613892\n",
      "Epoch 350, lr 2.8125e-06\n",
      "Epoch 350: Train 0.8852578410417726, Test 0.8676609992980957\n",
      "Epoch 375, lr 2.8125e-06\n",
      "Epoch 375: Train 0.8839298011642871, Test 0.8662200570106506\n",
      "Epoch 400, lr 2.8125e-06\n",
      "Epoch 400: Train 0.8823936785211658, Test 0.8645929098129272\n",
      "Epoch 425, lr 2.8125e-06\n",
      "Epoch 425: Train 0.8811318136677884, Test 0.8633652925491333\n",
      "Epoch 450, lr 2.8125e-06\n",
      "Epoch 450: Train 0.8801762376091268, Test 0.8623038530349731\n",
      "Epoch 475, lr 2.8125e-06\n",
      "Epoch 475: Train 0.8796383818187336, Test 0.8616161942481995\n",
      "TRAINING MODEL dt_norm_2_31\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.002356160570074, Test 0.8857531547546387\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.7668690089090371, Test 0.7511768341064453\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.7234023739526301, Test 0.7071289420127869\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.6706747188244337, Test 0.6513172388076782\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 0.652056195485739, Test 0.6326232552528381\n",
      "Epoch 125, lr 2.8125e-06\n",
      "Epoch 125: Train 0.6490415577535276, Test 0.6294674873352051\n",
      "Epoch 150, lr 7.03125e-07\n",
      "Epoch 150: Train 0.6484119682400314, Test 0.6287823915481567\n",
      "Epoch 175, lr 1.7578125e-07\n",
      "Epoch 175: Train 0.6483347099504353, Test 0.6286916732788086\n",
      "Epoch 200, lr 4.39453125e-08\n",
      "Epoch 200: Train 0.6483264889246152, Test 0.628683865070343\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.6483237482147453, Test 0.6286789178848267\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.6483240885499083, Test 0.6286792159080505\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.6483243431574033, Test 0.6286790370941162\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.6483246183689729, Test 0.6286792755126953\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.6483249925536874, Test 0.6286795139312744\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.6483252026416637, Test 0.6286798119544983\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.6483254612963877, Test 0.6286797523498535\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.6483258652834245, Test 0.6286797523498535\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.648326141230854, Test 0.6286799311637878\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.6483264388861479, Test 0.6286799907684326\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.6483267328621428, Test 0.6286797523498535\n",
      "TRAINING MODEL dt_norm_2_32\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.05451781697133, Test 0.9440361261367798\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.7893939640592126, Test 0.7761309146881104\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.7644742990241331, Test 0.7471365928649902\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.7590587928014643, Test 0.7395599484443665\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 0.750788523870356, Test 0.7308335304260254\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 0.7259636635289473, Test 0.7049424648284912\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 0.7010076729690328, Test 0.679879367351532\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 0.6994328853838584, Test 0.6781318187713623\n",
      "Epoch 200, lr 1.40625e-06\n",
      "Epoch 200: Train 0.6990445278146687, Test 0.6778101921081543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225, lr 3.515625e-07\n",
      "Epoch 225: Train 0.6989856625304502, Test 0.677740216255188\n",
      "Epoch 250, lr 8.7890625e-08\n",
      "Epoch 250: Train 0.6989709436893463, Test 0.6777274012565613\n",
      "Epoch 275, lr 2.197265625e-08\n",
      "Epoch 275: Train 0.6989664466065519, Test 0.6777225732803345\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.6989655819009332, Test 0.6777213215827942\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.6989655087099356, Test 0.6777210235595703\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.6989654504200992, Test 0.6777210235595703\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.6989653119269539, Test 0.677720844745636\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.6989652023595923, Test 0.6777206659317017\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.6989651440697557, Test 0.6777201890945435\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.698964973582941, Test 0.677720308303833\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.698964873657507, Test 0.6777198314666748\n",
      "TRAINING MODEL dt_norm_2_34\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 1.0388372758403421, Test 0.9633995890617371\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.9066593512892723, Test 0.8962024450302124\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.9044483112171292, Test 0.8931522369384766\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.9046772250905633, Test 0.893413782119751\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.904729044996202, Test 0.8934659957885742\n",
      "Epoch 125, lr 4.39453125e-08\n",
      "Epoch 125: Train 0.904727554321289, Test 0.8934646844863892\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.9047124920412898, Test 0.8934496641159058\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.9047027129679919, Test 0.8934410810470581\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.904692929238081, Test 0.8934318423271179\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.904683274589479, Test 0.8934229016304016\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.9046734564006329, Test 0.8934139013290405\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.9046638188883662, Test 0.8934052586555481\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.9046541290357709, Test 0.8933964967727661\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.9046445621177555, Test 0.8933876752853394\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.9046348787844181, Test 0.8933787941932678\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.904625165835023, Test 0.8933702707290649\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.9046155884861946, Test 0.8933614492416382\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.9046059891581535, Test 0.8933525681495667\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.9045965392142534, Test 0.8933438658714294\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.904586824402213, Test 0.8933352828025818\n",
      "TRAINING MODEL dt_norm_2_35\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.8724449736997485, Test 0.8040839433670044\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.7780181715264917, Test 0.7668992280960083\n",
      "Epoch 50, lr 5.625e-06\n",
      "Epoch 50: Train 0.7869921751320362, Test 0.7750343680381775\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.7882451279088855, Test 0.7761699557304382\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.7885611522942781, Test 0.7764540910720825\n",
      "Epoch 125, lr 4.39453125e-08\n",
      "Epoch 125: Train 0.7886173967272043, Test 0.7765082716941833\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.788613386824727, Test 0.776504397392273\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.7886077729985118, Test 0.7764987945556641\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.7886020993813873, Test 0.7764932513237\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.7885963363572955, Test 0.7764880657196045\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.7885907709598541, Test 0.7764824032783508\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.7885853551328182, Test 0.7764775156974792\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.7885798728093505, Test 0.7764725685119629\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.7885746845975519, Test 0.7764673829078674\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.7885695254430175, Test 0.776462733745575\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.7885643208399415, Test 0.7764570713043213\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.7885591311380267, Test 0.7764521837234497\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.7885539749637246, Test 0.7764472961425781\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.7885489083826542, Test 0.7764423489570618\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.7885437892749906, Test 0.7764376401901245\n",
      "TRAINING MODEL dt_norm_2_36\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.7737888380885124, Test 0.7041906118392944\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.6692167520523071, Test 0.6604079604148865\n",
      "Epoch 50, lr 5.625e-06\n",
      "Epoch 50: Train 0.6697811149992049, Test 0.6614295840263367\n",
      "Epoch 75, lr 1.40625e-06\n",
      "Epoch 75: Train 0.6699006328359246, Test 0.661564290523529\n",
      "Epoch 100, lr 3.515625e-07\n",
      "Epoch 100: Train 0.6700913487002254, Test 0.6617587804794312\n",
      "Epoch 125, lr 8.7890625e-08\n",
      "Epoch 125: Train 0.6701205955818296, Test 0.6617950797080994\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.6701130896806717, Test 0.6617908477783203\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.6701083658263087, Test 0.6617872714996338\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.6701043339446187, Test 0.6617834568023682\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.6701002866029739, Test 0.6617804765701294\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.6700962947681546, Test 0.6617775559425354\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.6700923476368189, Test 0.6617741584777832\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.6700885552912951, Test 0.6617706418037415\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.6700846953317523, Test 0.6617680191993713\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.6700808450579643, Test 0.6617646217346191\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.6700769700109959, Test 0.661761462688446\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.6700731532648205, Test 0.6617584824562073\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.6700693394988775, Test 0.6617556214332581\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.6700655534863472, Test 0.6617527008056641\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.6700617413967848, Test 0.6617493033409119\n",
      "TRAINING MODEL dt_norm_2_37\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.735232799872756, Test 0.6818374395370483\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.7117811061441899, Test 0.7037806510925293\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.715323394164443, Test 0.7061737179756165\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.7145850580185652, Test 0.7053413987159729\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.7145381605252623, Test 0.7052644491195679\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.7145094266161323, Test 0.7052300572395325\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.7144881099462509, Test 0.7052081823348999\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.7144728776067495, Test 0.7051923274993896\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.7144576283171773, Test 0.7051769495010376\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.7144425816833972, Test 0.7051612138748169\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.7144275486469269, Test 0.7051455974578857\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.7144125185906887, Test 0.7051302790641785\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.7143976135179401, Test 0.7051151394844055\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.7143825648352504, Test 0.7050994634628296\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.7143677806481719, Test 0.705083966255188\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.7143527695909142, Test 0.7050684094429016\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.7143379200249911, Test 0.7050533294677734\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.7143231108784676, Test 0.7050378918647766\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.7143090328201651, Test 0.7050232887268066\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.7142962126061321, Test 0.7050105333328247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_2_38\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.544381927186196, Test 0.4817758798599243\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.3964937856984795, Test 0.39082959294319153\n",
      "Epoch 50, lr 5.625e-06\n",
      "Epoch 50: Train 0.4035464476007934, Test 0.397497296333313\n",
      "Epoch 75, lr 1.40625e-06\n",
      "Epoch 75: Train 0.40484266488923937, Test 0.398613303899765\n",
      "Epoch 100, lr 3.515625e-07\n",
      "Epoch 100: Train 0.40528411411364146, Test 0.3990102708339691\n",
      "Epoch 125, lr 4.39453125e-08\n",
      "Epoch 125: Train 0.40537898392852295, Test 0.39909785985946655\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.4053912228400554, Test 0.3991090655326843\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.40539097649241806, Test 0.3991081416606903\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.40539086657926576, Test 0.39910808205604553\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.4053908972018356, Test 0.39910775423049927\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.40539072166889084, Test 0.39910730719566345\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.4053907153803274, Test 0.3991071581840515\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.4053860011450741, Test 0.39910173416137695\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.4053755492792217, Test 0.39909103512763977\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.40536549031187635, Test 0.3990810811519623\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.4053553509602853, Test 0.3990703821182251\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.40534536800253285, Test 0.3990599513053894\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.4053357741154662, Test 0.3990499973297119\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.4053262015548321, Test 0.39904022216796875\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.4053168947543573, Test 0.39903080463409424\n",
      "TRAINING MODEL dt_norm_2_39\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.6016989280195797, Test 0.5130637884140015\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.31424690892591195, Test 0.3012298345565796\n",
      "Epoch 50, lr 1.125e-05\n",
      "Epoch 50: Train 0.31581396916333365, Test 0.29755499958992004\n",
      "Epoch 75, lr 2.8125e-06\n",
      "Epoch 75: Train 0.31750958149923997, Test 0.2981345057487488\n",
      "Epoch 100, lr 7.03125e-07\n",
      "Epoch 100: Train 0.3178483366089709, Test 0.29825496673583984\n",
      "Epoch 125, lr 8.7890625e-08\n",
      "Epoch 125: Train 0.3179152441375396, Test 0.2982867956161499\n",
      "Epoch 150, lr 2.197265625e-08\n",
      "Epoch 150: Train 0.3179215615724816, Test 0.29828259348869324\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.31792306527495384, Test 0.2982827126979828\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.3179241611677058, Test 0.29828330874443054\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.3179253835011931, Test 0.29828357696533203\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.31792644564719763, Test 0.2982841432094574\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.3179275257622494, Test 0.2982849180698395\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.31792886314146657, Test 0.2982850670814514\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.3179298957042834, Test 0.29828545451164246\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.31793107662130804, Test 0.29828596115112305\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.3179322411032284, Test 0.29828667640686035\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.31793339703889456, Test 0.2982868552207947\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.31793452185742993, Test 0.29828739166259766\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.31793561402489157, Test 0.29828792810440063\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.31793685498483043, Test 0.29828840494155884\n",
      "TRAINING MODEL dt_norm_2_40\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.28972084836827383, Test 0.24068887531757355\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.21767386065589056, Test 0.21458503603935242\n",
      "Epoch 50, lr 5.625e-06\n",
      "Epoch 50: Train 0.2277436998155382, Test 0.22417430579662323\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.23015078173743353, Test 0.22640299797058105\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.23063244339492586, Test 0.22685278952121735\n",
      "Epoch 125, lr 4.39453125e-08\n",
      "Epoch 125: Train 0.2307343754503462, Test 0.22695107758045197\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.23074761546320385, Test 0.226963609457016\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.23075033277273177, Test 0.22696614265441895\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.23075307640764448, Test 0.22696858644485474\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.2307558321290546, Test 0.22697117924690247\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.230758688516087, Test 0.2269737869501114\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.23076137022839652, Test 0.22697627544403076\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.23076417611704933, Test 0.22697898745536804\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.2307669452495045, Test 0.22698146104812622\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.23076974782678816, Test 0.2269839495420456\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.23077237771617043, Test 0.22698646783828735\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.23077516853809357, Test 0.22698920965194702\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.23077799032131832, Test 0.2269917130470276\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.2307807602816158, Test 0.2269943505525589\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.23078357378641765, Test 0.2269970029592514\n",
      "TRAINING MODEL dt_norm_2_41\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.20333775024390915, Test 0.1712348312139511\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.22028177672798194, Test 0.21657313406467438\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.23943790894688913, Test 0.2351105511188507\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.24339603466316334, Test 0.2389201521873474\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.2442188453905791, Test 0.23971185088157654\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.24438038757703837, Test 0.2398642897605896\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.24440580273716195, Test 0.23988892138004303\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.24442112142021216, Test 0.23990394175052643\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.24443644791552163, Test 0.23991885781288147\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.24445174663390928, Test 0.23993393778800964\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.24446765412983384, Test 0.23994974792003632\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.244484088663916, Test 0.23996596038341522\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.2445005872874584, Test 0.23998208343982697\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.24451712670835477, Test 0.23999829590320587\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.24453363357817084, Test 0.24001465737819672\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.24455010992230722, Test 0.24003095924854279\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.244566660916921, Test 0.24004727602005005\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.24458314015448673, Test 0.2400633990764618\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.24459960275483364, Test 0.2400796264410019\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.24461629740821506, Test 0.24009597301483154\n",
      "TRAINING MODEL dt_norm_2_42\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.3670579024723598, Test 0.31323614716529846\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.2651815048995472, Test 0.26290372014045715\n",
      "Epoch 50, lr 1.125e-05\n",
      "Epoch 50: Train 0.2659344492214067, Test 0.26330170035362244\n",
      "Epoch 75, lr 2.8125e-06\n",
      "Epoch 75: Train 0.26699782269341604, Test 0.26441264152526855\n",
      "Epoch 100, lr 7.03125e-07\n",
      "Epoch 100: Train 0.2672847147498812, Test 0.2646748125553131\n",
      "Epoch 125, lr 1.7578125e-07\n",
      "Epoch 125: Train 0.26734446947063717, Test 0.2647334933280945\n",
      "Epoch 150, lr 2.197265625e-08\n",
      "Epoch 150: Train 0.2673536730664117, Test 0.26474255323410034\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.2673553434156236, Test 0.26474398374557495\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.2673563943022773, Test 0.2647450566291809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.2673573912609191, Test 0.264745831489563\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.2673583797046116, Test 0.2647468149662018\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.2673594430088997, Test 0.2647477388381958\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.2673604516755967, Test 0.2647484838962555\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.26736155968336833, Test 0.26474958658218384\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.2673624895867847, Test 0.26475054025650024\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.2673635958206086, Test 0.2647514045238495\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.2673645895861444, Test 0.2647522985935211\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.2673655762558892, Test 0.264753133058548\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.26736668390887125, Test 0.264754056930542\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.267367670578616, Test 0.2647549510002136\n",
      "TRAINING MODEL dt_norm_2_43\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.324858900272485, Test 0.290029913187027\n",
      "Epoch 25, lr 2.25e-05\n",
      "Epoch 25: Train 0.23353137572606406, Test 0.22786372900009155\n",
      "Epoch 50, lr 1.125e-05\n",
      "Epoch 50: Train 0.23253673315048218, Test 0.22598092257976532\n",
      "Epoch 75, lr 2.8125e-06\n",
      "Epoch 75: Train 0.23275365477258508, Test 0.2260260432958603\n",
      "Epoch 100, lr 3.515625e-07\n",
      "Epoch 100: Train 0.23281680634527496, Test 0.22602516412734985\n",
      "Epoch 125, lr 8.7890625e-08\n",
      "Epoch 125: Train 0.23282516770290607, Test 0.2260194718837738\n",
      "Epoch 150, lr 2.197265625e-08\n",
      "Epoch 150: Train 0.2328295481927467, Test 0.22602131962776184\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.23283000561324033, Test 0.22602133452892303\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.2328305063825665, Test 0.22602151334285736\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.23283103559956406, Test 0.2260216623544693\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.23283156526811194, Test 0.22602149844169617\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.2328320795839483, Test 0.22602155804634094\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.23283280341914206, Test 0.22602173686027527\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.23283316240166174, Test 0.22602181136608124\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.23283373632214285, Test 0.22602204978466034\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.23283436262246335, Test 0.2260221540927887\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.2328348010778427, Test 0.22602222859859467\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.23283529010686008, Test 0.2260221391916275\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.23283587486454935, Test 0.22602224349975586\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.23283642214356046, Test 0.22602245211601257\n",
      "TRAINING MODEL dt_norm_2_44\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.1778660010546446, Test 0.12981641292572021\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.08255710117518902, Test 0.07970055192708969\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 0.08244989402592182, Test 0.07927397638559341\n",
      "Epoch 75, lr 1.125e-05\n",
      "Epoch 75: Train 0.08304179757833481, Test 0.07945054024457932\n",
      "Epoch 100, lr 5.625e-06\n",
      "Epoch 100: Train 0.08266727812588215, Test 0.07866412401199341\n",
      "Epoch 125, lr 2.8125e-06\n",
      "Epoch 125: Train 0.08264455758035183, Test 0.07857111841440201\n",
      "Epoch 150, lr 7.03125e-07\n",
      "Epoch 150: Train 0.08259323611855507, Test 0.07849209755659103\n",
      "Epoch 175, lr 1.7578125e-07\n",
      "Epoch 175: Train 0.08257594965398311, Test 0.07847114652395248\n",
      "Epoch 200, lr 4.39453125e-08\n",
      "Epoch 200: Train 0.08257396891713142, Test 0.07846741378307343\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.08257318772375584, Test 0.07846630364656448\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.08257337249815463, Test 0.07846637070178986\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.08257356621325015, Test 0.07846647500991821\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.08257368057966233, Test 0.07846634089946747\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.08257398009300232, Test 0.07846634089946747\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.08257410451769828, Test 0.07846631109714508\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.08257421068847179, Test 0.0784662663936615\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.08257439695298671, Test 0.07846642285585403\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.08257456570863723, Test 0.07846634089946747\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.08257485739886761, Test 0.07846654206514359\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.08257497623562812, Test 0.07846652716398239\n",
      "TRAINING MODEL dt_norm_2_45\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.23692623187195172, Test 0.20342949032783508\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.1795564777020252, Test 0.1725333034992218\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 0.17721367875734964, Test 0.1686839610338211\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 0.17905508523637598, Test 0.1689315140247345\n",
      "Epoch 100, lr 5.625e-06\n",
      "Epoch 100: Train 0.17913197884053894, Test 0.1688244640827179\n",
      "Epoch 125, lr 7.03125e-07\n",
      "Epoch 125: Train 0.17914883069919818, Test 0.1687319427728653\n",
      "Epoch 150, lr 1.7578125e-07\n",
      "Epoch 150: Train 0.1791729498090166, Test 0.16874319314956665\n",
      "Epoch 175, lr 4.39453125e-08\n",
      "Epoch 175: Train 0.17917619103735144, Test 0.16874051094055176\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.17917590430288605, Test 0.16873963177204132\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.1791758081226638, Test 0.16873925924301147\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.17917576883778427, Test 0.16873906552791595\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.1791757322622068, Test 0.168738454580307\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.17917568439787085, Test 0.16873806715011597\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.17917559499090369, Test 0.16873779892921448\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.17917552454905075, Test 0.16873744130134583\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.1791755078416882, Test 0.16873739659786224\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.17917547216921142, Test 0.16873690485954285\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.17917545681649988, Test 0.1687367558479309\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.17917539540565375, Test 0.1687363237142563\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.17917531232039133, Test 0.16873599588871002\n",
      "TRAINING MODEL dt_norm_3_1\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 13.267944292833576, Test 12.582460403442383\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 11.733455340067545, Test 11.703262329101562\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 11.536026259600106, Test 11.470976829528809\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 11.373563545571882, Test 11.33910083770752\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 11.357871793757724, Test 11.321378707885742\n",
      "Epoch 125, lr 5.625e-06\n",
      "Epoch 125: Train 11.348898790650448, Test 11.311653137207031\n",
      "Epoch 150, lr 2.8125e-06\n",
      "Epoch 150: Train 11.344659649046127, Test 11.305418968200684\n",
      "Epoch 175, lr 1.40625e-06\n",
      "Epoch 175: Train 11.343666830978824, Test 11.304553985595703\n",
      "Epoch 200, lr 1.7578125e-07\n",
      "Epoch 200: Train 11.34281448859953, Test 11.303518295288086\n",
      "Epoch 225, lr 4.39453125e-08\n",
      "Epoch 225: Train 11.342729094338281, Test 11.30342960357666\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 11.342704816053143, Test 11.303400993347168\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 11.342703727679064, Test 11.303400993347168\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 11.342703005688339, Test 11.303400993347168\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 11.342702256757661, Test 11.303400039672852\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 11.342701610198802, Test 11.303401947021484\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 11.342700613420561, Test 11.303400993347168\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 11.342700069233523, Test 11.303400993347168\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 11.342699239482988, Test 11.303400993347168\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 11.342698312748624, Test 11.303401947021484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 11.342697542265984, Test 11.303400039672852\n",
      "TRAINING MODEL dt_norm_3_3\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 16.307332354784013, Test 15.902122497558594\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 12.925718203186989, Test 12.73122501373291\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 12.595598232746124, Test 12.402135848999023\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 12.431997925043106, Test 12.244697570800781\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 12.344263689219952, Test 12.15537166595459\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 12.224025151133537, Test 12.031597137451172\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 12.178842359781266, Test 12.02134895324707\n",
      "Epoch 175, lr 4.5e-05\n",
      "Epoch 175: Train 12.132384023070335, Test 11.948750495910645\n",
      "Epoch 200, lr 4.5e-05\n",
      "Epoch 200: Train 12.10829039812088, Test 11.921371459960938\n",
      "Epoch 225, lr 4.5e-05\n",
      "Epoch 225: Train 12.08933297842741, Test 11.947856903076172\n",
      "Epoch 250, lr 4.5e-05\n",
      "Epoch 250: Train 12.061192356050014, Test 11.901568412780762\n",
      "Epoch 275, lr 4.5e-05\n",
      "Epoch 275: Train 12.035840091109275, Test 11.872906684875488\n",
      "Epoch 300, lr 4.5e-05\n",
      "Epoch 300: Train 12.025172446668147, Test 11.85020637512207\n",
      "Epoch 325, lr 4.5e-05\n",
      "Epoch 325: Train 11.988257181644439, Test 11.83726692199707\n",
      "Epoch 350, lr 2.25e-05\n",
      "Epoch 350: Train 11.947382886707782, Test 11.773106575012207\n",
      "Epoch 375, lr 2.25e-05\n",
      "Epoch 375: Train 11.93809844404459, Test 11.765460968017578\n",
      "Epoch 400, lr 2.25e-05\n",
      "Epoch 400: Train 11.930710890889168, Test 11.756110191345215\n",
      "Epoch 425, lr 2.25e-05\n",
      "Epoch 425: Train 11.918754231929778, Test 11.746033668518066\n",
      "Epoch 450, lr 2.25e-05\n",
      "Epoch 450: Train 11.910257326066494, Test 11.735280990600586\n",
      "Epoch 475, lr 2.25e-05\n",
      "Epoch 475: Train 11.904106679558755, Test 11.73271369934082\n",
      "TRAINING MODEL dt_norm_3_5\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 20.531477418541908, Test 20.158063888549805\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 19.061876833438873, Test 18.81016731262207\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 18.76174623966217, Test 18.538330078125\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 18.556569328904153, Test 18.361583709716797\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 18.405651471018793, Test 18.248838424682617\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 18.392271333932875, Test 18.293594360351562\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 18.18344484269619, Test 18.076766967773438\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 18.148781615495682, Test 18.038930892944336\n",
      "Epoch 200, lr 2.25e-05\n",
      "Epoch 200: Train 18.130767166614532, Test 18.014068603515625\n",
      "Epoch 225, lr 2.25e-05\n",
      "Epoch 225: Train 18.099946370720865, Test 17.994239807128906\n",
      "Epoch 250, lr 2.25e-05\n",
      "Epoch 250: Train 18.08581794798374, Test 17.97431182861328\n",
      "Epoch 275, lr 2.25e-05\n",
      "Epoch 275: Train 18.066987046599387, Test 17.95462417602539\n",
      "Epoch 300, lr 2.25e-05\n",
      "Epoch 300: Train 18.040549755096436, Test 17.940326690673828\n",
      "Epoch 325, lr 2.25e-05\n",
      "Epoch 325: Train 18.031724962592126, Test 17.924108505249023\n",
      "Epoch 350, lr 1.125e-05\n",
      "Epoch 350: Train 17.984249657392503, Test 17.888710021972656\n",
      "Epoch 375, lr 1.125e-05\n",
      "Epoch 375: Train 17.974759578704834, Test 17.879987716674805\n",
      "Epoch 400, lr 5.625e-06\n",
      "Epoch 400: Train 17.956477093696595, Test 17.866331100463867\n",
      "Epoch 425, lr 1.40625e-06\n",
      "Epoch 425: Train 17.94673784971237, Test 17.858369827270508\n",
      "Epoch 450, lr 3.515625e-07\n",
      "Epoch 450: Train 17.944319155812263, Test 17.85634994506836\n",
      "Epoch 475, lr 8.7890625e-08\n",
      "Epoch 475: Train 17.943719501793385, Test 17.855802536010742\n",
      "TRAINING MODEL dt_norm_3_7\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 15.483354434370995, Test 14.895665168762207\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 13.893769890069962, Test 13.618606567382812\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 13.67788274139166, Test 13.514571189880371\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 13.575178344547748, Test 13.621236801147461\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 13.281500568985939, Test 13.078049659729004\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 13.211065055429936, Test 12.965248107910156\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 13.185572773218155, Test 12.937232971191406\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 13.14503961801529, Test 12.903482437133789\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 13.12613640576601, Test 12.881006240844727\n",
      "Epoch 225, lr 1.125e-05\n",
      "Epoch 225: Train 13.111730533838273, Test 12.86590576171875\n",
      "Epoch 250, lr 1.125e-05\n",
      "Epoch 250: Train 13.098446460068226, Test 12.855278015136719\n",
      "Epoch 275, lr 1.125e-05\n",
      "Epoch 275: Train 13.084563630819321, Test 12.838817596435547\n",
      "Epoch 300, lr 1.125e-05\n",
      "Epoch 300: Train 13.076154890656472, Test 12.835773468017578\n",
      "Epoch 325, lr 1.125e-05\n",
      "Epoch 325: Train 13.06545066088438, Test 12.820698738098145\n",
      "Epoch 350, lr 1.125e-05\n",
      "Epoch 350: Train 13.05675560683012, Test 12.816665649414062\n",
      "Epoch 375, lr 1.125e-05\n",
      "Epoch 375: Train 12.976731872558593, Test 12.734929084777832\n",
      "Epoch 400, lr 1.125e-05\n",
      "Epoch 400: Train 12.949053892493248, Test 12.705683708190918\n",
      "Epoch 425, lr 5.625e-06\n",
      "Epoch 425: Train 12.92638111114502, Test 12.675249099731445\n",
      "Epoch 450, lr 5.625e-06\n",
      "Epoch 450: Train 12.921316844224929, Test 12.670219421386719\n",
      "Epoch 475, lr 5.625e-06\n",
      "Epoch 475: Train 12.916231162846088, Test 12.665214538574219\n",
      "TRAINING MODEL dt_norm_3_9\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 13.971562308073043, Test 13.33301830291748\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 12.54388759881258, Test 12.088582038879395\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 12.116318051517009, Test 11.690178871154785\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 12.006430442631245, Test 11.568012237548828\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 11.781191159784793, Test 11.380427360534668\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 11.681091445684434, Test 11.246011734008789\n",
      "Epoch 150, lr 4.5e-05\n",
      "Epoch 150: Train 11.670693618059158, Test 11.253649711608887\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 11.518966215848923, Test 11.158960342407227\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 11.450799083709716, Test 11.112180709838867\n",
      "Epoch 225, lr 1.125e-05\n",
      "Epoch 225: Train 11.446000292897224, Test 11.102909088134766\n",
      "Epoch 250, lr 1.125e-05\n",
      "Epoch 250: Train 11.439268843829632, Test 11.096562385559082\n",
      "Epoch 275, lr 5.625e-06\n",
      "Epoch 275: Train 11.4258622944355, Test 11.085603713989258\n",
      "Epoch 300, lr 1.40625e-06\n",
      "Epoch 300: Train 11.417836736142636, Test 11.079169273376465\n",
      "Epoch 325, lr 3.515625e-07\n",
      "Epoch 325: Train 11.415895941853524, Test 11.07720947265625\n",
      "Epoch 350, lr 8.7890625e-08\n",
      "Epoch 350: Train 11.415424075722694, Test 11.076763153076172\n",
      "Epoch 375, lr 2.197265625e-08\n",
      "Epoch 375: Train 11.415294028818607, Test 11.076647758483887\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 11.41526906490326, Test 11.07662582397461\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 11.415261177718639, Test 11.076619148254395\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 11.415253140032291, Test 11.076610565185547\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 11.415244747698306, Test 11.076605796813965\n",
      "TRAINING MODEL dt_norm_3_11\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 10.292991243302822, Test 9.739266395568848\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 8.686787939071655, Test 8.3748197555542\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 8.47542805224657, Test 8.194271087646484\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 8.37879025861621, Test 8.068824768066406\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 8.238950651139021, Test 7.989418983459473\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 8.213287016749382, Test 7.966401100158691\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 8.189475718140603, Test 7.948666572570801\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 8.155844385921956, Test 7.911281585693359\n",
      "Epoch 200, lr 2.25e-05\n",
      "Epoch 200: Train 8.139573672413826, Test 7.89921760559082\n",
      "Epoch 225, lr 2.25e-05\n",
      "Epoch 225: Train 8.132147756963969, Test 7.8860297203063965\n",
      "Epoch 250, lr 1.125e-05\n",
      "Epoch 250: Train 8.103565583378076, Test 7.872807502746582\n",
      "Epoch 275, lr 5.625e-06\n",
      "Epoch 275: Train 8.09254964441061, Test 7.862287521362305\n",
      "Epoch 300, lr 1.40625e-06\n",
      "Epoch 300: Train 8.086338052898645, Test 7.856931686401367\n",
      "Epoch 325, lr 3.515625e-07\n",
      "Epoch 325: Train 8.08485085144639, Test 7.855709075927734\n",
      "Epoch 350, lr 8.7890625e-08\n",
      "Epoch 350: Train 8.084473711997271, Test 7.855353355407715\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 8.084385148435832, Test 7.855268478393555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 8.084367190301418, Test 7.855254650115967\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 8.084360271692276, Test 7.855247974395752\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 8.084354282170533, Test 7.855243682861328\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 8.084347929805517, Test 7.855238914489746\n",
      "TRAINING MODEL dt_norm_3_13\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 9.150458681583405, Test 8.682182312011719\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 7.5245166450738905, Test 7.398359298706055\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 7.457824593782425, Test 7.344204902648926\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 7.42789709046483, Test 7.395072937011719\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 7.415888821333647, Test 7.294632911682129\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 7.387514527142048, Test 7.266217231750488\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 7.36281366199255, Test 7.238628387451172\n",
      "Epoch 175, lr 2.25e-05\n",
      "Epoch 175: Train 7.358050512522459, Test 7.235260486602783\n",
      "Epoch 200, lr 1.125e-05\n",
      "Epoch 200: Train 7.3439103126525875, Test 7.223720550537109\n",
      "Epoch 225, lr 1.125e-05\n",
      "Epoch 225: Train 7.342086447030306, Test 7.221825122833252\n",
      "Epoch 250, lr 5.625e-06\n",
      "Epoch 250: Train 7.335230319947004, Test 7.217246055603027\n",
      "Epoch 275, lr 1.40625e-06\n",
      "Epoch 275: Train 7.331189077347517, Test 7.213937759399414\n",
      "Epoch 300, lr 3.515625e-07\n",
      "Epoch 300: Train 7.330175890773535, Test 7.2130537033081055\n",
      "Epoch 325, lr 4.39453125e-08\n",
      "Epoch 325: Train 7.329889309406281, Test 7.212807655334473\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 7.329850294440985, Test 7.212775707244873\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 7.329846509546042, Test 7.212772846221924\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 7.3298431664705275, Test 7.212770938873291\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 7.329839638620615, Test 7.212767601013184\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 7.329836788028478, Test 7.212766647338867\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 7.329833190888166, Test 7.212762832641602\n",
      "TRAINING MODEL dt_norm_3_15\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 5.694054679572583, Test 5.304841041564941\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 4.556993299722672, Test 4.426602363586426\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 4.497130215167999, Test 4.371063232421875\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 4.4771256349980835, Test 4.350372791290283\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 4.46081215813756, Test 4.334384918212891\n",
      "Epoch 125, lr 4.5e-05\n",
      "Epoch 125: Train 4.4419862404465675, Test 4.32172966003418\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 4.4197529293596745, Test 4.297805309295654\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 4.410084135830402, Test 4.289090156555176\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 4.403781817853451, Test 4.283965110778809\n",
      "Epoch 225, lr 7.03125e-07\n",
      "Epoch 225: Train 4.402194567024708, Test 4.2825188636779785\n",
      "Epoch 250, lr 1.7578125e-07\n",
      "Epoch 250: Train 4.401793053001166, Test 4.282161712646484\n",
      "Epoch 275, lr 4.39453125e-08\n",
      "Epoch 275: Train 4.40169515684247, Test 4.2820892333984375\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 4.401668104529381, Test 4.282064914703369\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 4.401665554195643, Test 4.282063007354736\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 4.401662120968103, Test 4.282060146331787\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 4.401659008860588, Test 4.2820587158203125\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 4.4016557015478615, Test 4.28205680847168\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 4.401652682572603, Test 4.282055377960205\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 4.401649855822325, Test 4.282052993774414\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 4.401646827161312, Test 4.282052040100098\n",
      "TRAINING MODEL dt_norm_3_17\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 4.947771643102169, Test 4.605482578277588\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 3.787357372790575, Test 3.731879234313965\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 3.7360840126872064, Test 3.672365188598633\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 3.7165813982486724, Test 3.6566710472106934\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 3.7101643279194834, Test 3.6502585411071777\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 3.6906698018312456, Test 3.6291871070861816\n",
      "Epoch 150, lr 1.125e-05\n",
      "Epoch 150: Train 3.683287415653467, Test 3.6230416297912598\n",
      "Epoch 175, lr 5.625e-06\n",
      "Epoch 175: Train 3.6797394417226315, Test 3.6193885803222656\n",
      "Epoch 200, lr 7.03125e-07\n",
      "Epoch 200: Train 3.677232560515404, Test 3.6169447898864746\n",
      "Epoch 225, lr 1.7578125e-07\n",
      "Epoch 225: Train 3.676505470275879, Test 3.6162102222442627\n",
      "Epoch 250, lr 8.7890625e-08\n",
      "Epoch 250: Train 3.6764433085918427, Test 3.616161346435547\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 3.676403398811817, Test 3.6161136627197266\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 3.6763948269188402, Test 3.616107702255249\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 3.676392734795809, Test 3.6161060333251953\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 3.6763905622065067, Test 3.6161048412323\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 3.676388227194548, Test 3.6161036491394043\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 3.6763861164450646, Test 3.6161022186279297\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 3.6763840280473232, Test 3.6161017417907715\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 3.676381739228964, Test 3.616100311279297\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 3.6763797238469125, Test 3.6160988807678223\n",
      "TRAINING MODEL dt_norm_3_19\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 4.448880594223738, Test 4.063498497009277\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 3.42500839009881, Test 3.344294548034668\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 3.3955282092094423, Test 3.312695026397705\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 3.3728051610291003, Test 3.2855782508850098\n",
      "Epoch 100, lr 2.25e-05\n",
      "Epoch 100: Train 3.355505429953337, Test 3.267625331878662\n",
      "Epoch 125, lr 1.125e-05\n",
      "Epoch 125: Train 3.3476858746260403, Test 3.2593870162963867\n",
      "Epoch 150, lr 5.625e-06\n",
      "Epoch 150: Train 3.343990558385849, Test 3.255542278289795\n",
      "Epoch 175, lr 1.40625e-06\n",
      "Epoch 175: Train 3.3413475073873995, Test 3.252727508544922\n",
      "Epoch 200, lr 3.515625e-07\n",
      "Epoch 200: Train 3.340496801584959, Test 3.251880168914795\n",
      "Epoch 225, lr 1.7578125e-07\n",
      "Epoch 225: Train 3.3403112068772316, Test 3.251675605773926\n",
      "Epoch 250, lr 4.39453125e-08\n",
      "Epoch 250: Train 3.3402177721261976, Test 3.251579761505127\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 3.34019292332232, Test 3.2515549659729004\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 3.34018732085824, Test 3.2515501976013184\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 3.340182041376829, Test 3.2515463829040527\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 3.3401765577495097, Test 3.2515413761138916\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 3.340170945972204, Test 3.251537322998047\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 3.340165230631828, Test 3.251533031463623\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 3.340159986168146, Test 3.251528739929199\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 3.340154531598091, Test 3.251525402069092\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 3.340149079635739, Test 3.2515201568603516\n",
      "TRAINING MODEL dt_norm_3_21\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 5.448913249373436, Test 5.070803165435791\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 4.474857528880238, Test 4.450643539428711\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 4.458205603808165, Test 4.43160343170166\n",
      "Epoch 75, lr 4.5e-05\n",
      "Epoch 75: Train 4.447249683737755, Test 4.419620990753174\n",
      "Epoch 100, lr 4.5e-05\n",
      "Epoch 100: Train 4.437052360549569, Test 4.408699989318848\n",
      "Epoch 125, lr 2.25e-05\n",
      "Epoch 125: Train 4.4263050392270085, Test 4.397768497467041\n",
      "Epoch 150, lr 2.25e-05\n",
      "Epoch 150: Train 4.413708675652742, Test 4.386321067810059\n",
      "Epoch 175, lr 1.125e-05\n",
      "Epoch 175: Train 4.408077280968428, Test 4.379957675933838\n",
      "Epoch 200, lr 2.8125e-06\n",
      "Epoch 200: Train 4.403477325662971, Test 4.375444412231445\n",
      "Epoch 225, lr 7.03125e-07\n",
      "Epoch 225: Train 4.4024249270558355, Test 4.3744001388549805\n",
      "Epoch 250, lr 1.7578125e-07\n",
      "Epoch 250: Train 4.402172787860036, Test 4.374139785766602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275, lr 2.197265625e-08\n",
      "Epoch 275: Train 4.402105833217502, Test 4.374085426330566\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 4.402089377865195, Test 4.3740668296813965\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 4.4020884949713945, Test 4.37406587600708\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 4.402087189257145, Test 4.374064922332764\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 4.40208630412817, Test 4.374063491821289\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 4.402085084468126, Test 4.374063014984131\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 4.402084409818054, Test 4.374063014984131\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 4.40208326280117, Test 4.374061584472656\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 4.402082068473101, Test 4.374061107635498\n",
      "TRAINING MODEL dt_norm_3_23\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 3.801257172971964, Test 3.483030319213867\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 3.036909031495452, Test 3.020883083343506\n",
      "Epoch 50, lr 4.5e-05\n",
      "Epoch 50: Train 3.023850193619728, Test 3.004711151123047\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 3.0079658679664134, Test 2.986374855041504\n",
      "Epoch 100, lr 1.125e-05\n",
      "Epoch 100: Train 2.992870081216097, Test 2.9734320640563965\n",
      "Epoch 125, lr 2.8125e-06\n",
      "Epoch 125: Train 2.989359415322542, Test 2.970508575439453\n",
      "Epoch 150, lr 7.03125e-07\n",
      "Epoch 150: Train 2.988374861329794, Test 2.969607353210449\n",
      "Epoch 175, lr 1.7578125e-07\n",
      "Epoch 175: Train 2.9881491262465714, Test 2.9693992137908936\n",
      "Epoch 200, lr 4.39453125e-08\n",
      "Epoch 200: Train 2.9880956918001176, Test 2.969341278076172\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 2.9880809031426905, Test 2.9693245887756348\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 2.988079573586583, Test 2.9693245887756348\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 2.988078287243843, Test 2.9693236351013184\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 2.988077125698328, Test 2.969322681427002\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 2.988076080754399, Test 2.969322443008423\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 2.988074804097414, Test 2.9693210124969482\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 2.9880736380815507, Test 2.9693212509155273\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 2.9880725648254156, Test 2.969320058822632\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 2.988071446865797, Test 2.9693193435668945\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 2.988070171698928, Test 2.9693188667297363\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 2.9880689300596712, Test 2.96931791305542\n",
      "TRAINING MODEL dt_norm_3_25\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 3.3262064844369887, Test 3.0338315963745117\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 2.5355182457715273, Test 2.5070455074310303\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 2.5115091685205697, Test 2.4830026626586914\n",
      "Epoch 75, lr 2.25e-05\n",
      "Epoch 75: Train 2.4896319467574357, Test 2.4590349197387695\n",
      "Epoch 100, lr 5.625e-06\n",
      "Epoch 100: Train 2.4807873599231245, Test 2.450026512145996\n",
      "Epoch 125, lr 2.8125e-06\n",
      "Epoch 125: Train 2.4764838900417088, Test 2.446244239807129\n",
      "Epoch 150, lr 3.515625e-07\n",
      "Epoch 150: Train 2.4756091982126236, Test 2.4453139305114746\n",
      "Epoch 175, lr 8.7890625e-08\n",
      "Epoch 175: Train 2.4753546353429554, Test 2.4450674057006836\n",
      "Epoch 200, lr 2.197265625e-08\n",
      "Epoch 200: Train 2.475324123725295, Test 2.4450340270996094\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 2.4753173403441906, Test 2.44502854347229\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 2.475315199419856, Test 2.445026397705078\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 2.4753129594027996, Test 2.4450244903564453\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 2.4753107290714977, Test 2.4450225830078125\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 2.4753083903342485, Test 2.4450206756591797\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 2.475306338816881, Test 2.445018768310547\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 2.4753040600568057, Test 2.4450178146362305\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 2.475301919505, Test 2.445014715194702\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 2.4752996545284986, Test 2.4450130462646484\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 2.4752974808216095, Test 2.4450111389160156\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 2.4752952504903076, Test 2.445009469985962\n",
      "TRAINING MODEL dt_norm_3_39\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.7253137493604108, Test 0.6482310891151428\n",
      "Epoch 25, lr 4.5e-05\n",
      "Epoch 25: Train 0.49496878618211076, Test 0.47872108221054077\n",
      "Epoch 50, lr 2.25e-05\n",
      "Epoch 50: Train 0.4973466191114041, Test 0.483465313911438\n",
      "Epoch 75, lr 5.625e-06\n",
      "Epoch 75: Train 0.49638333328460393, Test 0.4832649528980255\n",
      "Epoch 100, lr 2.8125e-06\n",
      "Epoch 100: Train 0.4858033969475512, Test 0.4723619222640991\n",
      "Epoch 125, lr 1.40625e-06\n",
      "Epoch 125: Train 0.4829036820875971, Test 0.4697822630405426\n",
      "Epoch 150, lr 3.515625e-07\n",
      "Epoch 150: Train 0.4829461940547876, Test 0.46983763575553894\n",
      "Epoch 175, lr 8.7890625e-08\n",
      "Epoch 175: Train 0.48294734863335625, Test 0.4698362946510315\n",
      "Epoch 200, lr 2.197265625e-08\n",
      "Epoch 200: Train 0.4829440180931175, Test 0.46983394026756287\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.48294138738460707, Test 0.4698317348957062\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.48293973923775185, Test 0.46982988715171814\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.4829379751494056, Test 0.46982842683792114\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.4829362694892967, Test 0.46982645988464355\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.4829345704954967, Test 0.4698249101638794\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.4829328726781042, Test 0.4698232114315033\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.48293122230914604, Test 0.4698215126991272\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.48292958945558784, Test 0.4698198139667511\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.48292779582634304, Test 0.46981799602508545\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.48292620440846995, Test 0.46981632709503174\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.4829245003169043, Test 0.46981489658355713\n",
      "TRAINING MODEL dt_norm_3_40\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.35029305161470436, Test 0.3121112883090973\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.38988033958165297, Test 0.3833092451095581\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.40441404930148944, Test 0.3967864513397217\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.4073216812425401, Test 0.3995314836502075\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.4079156595159812, Test 0.40009447932243347\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.40801075813304855, Test 0.40018436312675476\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.40800482557114376, Test 0.4001775085926056\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.4079942503773359, Test 0.40016674995422363\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.40798377450481355, Test 0.40015602111816406\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.40797320542582477, Test 0.40014520287513733\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.40796272005455425, Test 0.4001343250274658\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.4079522254220043, Test 0.40012386441230774\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.4079416052872441, Test 0.40011274814605713\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.40793110103721164, Test 0.4001019597053528\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.407920730600794, Test 0.400091290473938\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.40791023887723565, Test 0.4000804126262665\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.40789982783366957, Test 0.4000696837902069\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.4078893442434144, Test 0.40005913376808167\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.40787895795596074, Test 0.40004855394363403\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.4078686793011973, Test 0.4000377357006073\n",
      "TRAINING MODEL dt_norm_3_41\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.2429354060089813, Test 0.2112177163362503\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.29171372205018997, Test 0.28854113817214966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.3056881624572682, Test 0.30246633291244507\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.3094779235433857, Test 0.306052029132843\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.3102493551823328, Test 0.3067915737628937\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.31038223445977803, Test 0.3069167733192444\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.3103948598903305, Test 0.3069286048412323\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.31039974037206397, Test 0.3069334328174591\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.3104045258940391, Test 0.30693817138671875\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.3104094369512684, Test 0.3069429397583008\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.31041436030898456, Test 0.3069479465484619\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.3104192469761057, Test 0.306952565908432\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.3104241046140779, Test 0.3069572448730469\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.3104289691403227, Test 0.3069619834423065\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.31043388743726713, Test 0.3069666028022766\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.3104388301243197, Test 0.3069716989994049\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.31044375931598106, Test 0.30697664618492126\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.31044866875657495, Test 0.3069812059402466\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.31045359127082917, Test 0.306986004114151\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.31045848363131845, Test 0.30699074268341064\n",
      "TRAINING MODEL dt_norm_3_42\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.20876108858320447, Test 0.1859070062637329\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.3630032594998678, Test 0.35780811309814453\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.3906428337097168, Test 0.3844578266143799\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.39591299480862086, Test 0.38953152298927307\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.3968197888798184, Test 0.3903878927230835\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.396991026269065, Test 0.3905532658100128\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.3970039694839054, Test 0.39056506752967834\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.39700617763731216, Test 0.39056724309921265\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.397008351220025, Test 0.39056941866874695\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.3970106961992052, Test 0.39057159423828125\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.39701307985517714, Test 0.39057403802871704\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.39701551424132453, Test 0.3905763030052185\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.39701794412400987, Test 0.39057865738868713\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.39702046910921734, Test 0.39058107137680054\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.39702292707231307, Test 0.3905835449695587\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.39702543179194133, Test 0.39058583974838257\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.3970280485683017, Test 0.3905884027481079\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.3970309689309862, Test 0.3905912935733795\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.39703409685028923, Test 0.39059436321258545\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.39703717364205254, Test 0.39059725403785706\n",
      "TRAINING MODEL dt_norm_3_43\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.2221737419848615, Test 0.19399939477443695\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.3127566230111789, Test 0.3041800260543823\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.3326833791683375, Test 0.32349181175231934\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.336781893060615, Test 0.32746589183807373\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.3375839364034524, Test 0.3282439708709717\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.3377202228869799, Test 0.3283740282058716\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.3377285510147174, Test 0.3283843398094177\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.33772893952582167, Test 0.32838666439056396\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.33772921716611004, Test 0.3283891975879669\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.33772963563395286, Test 0.32839152216911316\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.3377300371160161, Test 0.3283941149711609\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.33773047056222827, Test 0.32839658856391907\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.33773096114242634, Test 0.32839930057525635\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.33773140864051066, Test 0.3284018039703369\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.3377318521237744, Test 0.3284042477607727\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.33773236324132416, Test 0.3284068703651428\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.3377328701896371, Test 0.3284095525741577\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.33773333266609074, Test 0.3284120559692383\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.33773388238768504, Test 0.3284146785736084\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.33773439798330396, Test 0.32841724157333374\n",
      "TRAINING MODEL dt_norm_3_44\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.21511171187977401, Test 0.19129814207553864\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.3561018836133334, Test 0.35594767332077026\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.3832838430392499, Test 0.3824012577533722\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.3889727136310266, Test 0.3879380524158478\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.3900747361535929, Test 0.3890102505683899\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.3902733290986139, Test 0.38919681310653687\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.39029023112082967, Test 0.3892131447792053\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.39029588002939614, Test 0.3892187476158142\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.39030163446251226, Test 0.3892245888710022\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.39030763734968343, Test 0.3892305791378021\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.3903134991319812, Test 0.3892366290092468\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.39031947586609395, Test 0.38924258947372437\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.39032543952367743, Test 0.38924863934516907\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.39033136258319934, Test 0.38925445079803467\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.39033738827827025, Test 0.38926053047180176\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.39034340302554926, Test 0.3892664313316345\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.39034932213170187, Test 0.3892725110054016\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.3903553137669758, Test 0.3892785310745239\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.39036144072912177, Test 0.389284610748291\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.39036758532937693, Test 0.3892906606197357\n",
      "TRAINING MODEL dt_norm_3_45\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.235026591858327, Test 0.2016104757785797\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.23948877941298957, Test 0.23265910148620605\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.25432750069542437, Test 0.24667143821716309\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.25689858632371915, Test 0.24913527071475983\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.25742531355643117, Test 0.24963556230068207\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.2575207428624298, Test 0.24972620606422424\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.2575304423341688, Test 0.24973508715629578\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.25753437703018944, Test 0.24973879754543304\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.25753820041157555, Test 0.24974223971366882\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.25754203750992455, Test 0.2497459501028061\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.257545967863885, Test 0.24974948167800903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.2575498581524716, Test 0.24975314736366272\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.25755371202696237, Test 0.24975675344467163\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.2575576349796838, Test 0.2497604638338089\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.25756155842582124, Test 0.24976396560668945\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.2575653949320711, Test 0.24976760149002075\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.25756924357635297, Test 0.24977107346057892\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.2575731624830638, Test 0.24977481365203857\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.25757713290239803, Test 0.24977853894233704\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.2575810360197989, Test 0.24978211522102356\n",
      "TRAINING MODEL dt_norm_3_46\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.23015981838569877, Test 0.1966964602470398\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.30118109137745375, Test 0.2981046140193939\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.3276742685655629, Test 0.3239360749721527\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.33325643541279787, Test 0.3293209373950958\n",
      "Epoch 100, lr 8.7890625e-08\n",
      "Epoch 100: Train 0.3344064541855214, Test 0.330422043800354\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.33461949451369527, Test 0.3306235671043396\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.33464905675153556, Test 0.33065205812454224\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.334665251259478, Test 0.33066800236701965\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.3346816573453986, Test 0.3306843340396881\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.33469810952310974, Test 0.330700546503067\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.3347143279099316, Test 0.33071643114089966\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.33473060238435404, Test 0.3307324945926666\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.3347468772289916, Test 0.33074861764907837\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.3347633455110633, Test 0.33076488971710205\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.3347797323457943, Test 0.3307809829711914\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.33479614454026546, Test 0.33079734444618225\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.33481261078615365, Test 0.3308134078979492\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.3348290166869667, Test 0.33082976937294006\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.33484538501093847, Test 0.3308458626270294\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.33486178683938445, Test 0.3308618664741516\n",
      "TRAINING MODEL dt_norm_3_47\n",
      "Epoch 0, lr 4.5e-05\n",
      "Epoch 0: Train 0.17775240999417027, Test 0.15142540633678436\n",
      "Epoch 25, lr 1.125e-05\n",
      "Epoch 25: Train 0.2635904817261558, Test 0.2594510316848755\n",
      "Epoch 50, lr 2.8125e-06\n",
      "Epoch 50: Train 0.28588582164999365, Test 0.2807048261165619\n",
      "Epoch 75, lr 7.03125e-07\n",
      "Epoch 75: Train 0.29168727821198065, Test 0.28624671697616577\n",
      "Epoch 100, lr 1.7578125e-07\n",
      "Epoch 100: Train 0.29291407893533294, Test 0.2874208390712738\n",
      "Epoch 125, lr 2.197265625e-08\n",
      "Epoch 125: Train 0.2931515613327856, Test 0.28764230012893677\n",
      "Epoch 150, lr 1.0986328125e-08\n",
      "Epoch 150: Train 0.2931828527994778, Test 0.2876720130443573\n",
      "Epoch 175, lr 1.0986328125e-08\n",
      "Epoch 175: Train 0.2931989339598711, Test 0.2876878082752228\n",
      "Epoch 200, lr 1.0986328125e-08\n",
      "Epoch 200: Train 0.29321522200885025, Test 0.28770363330841064\n",
      "Epoch 225, lr 1.0986328125e-08\n",
      "Epoch 225: Train 0.2932314198950063, Test 0.2877195477485657\n",
      "Epoch 250, lr 1.0986328125e-08\n",
      "Epoch 250: Train 0.2932476323583852, Test 0.2877352833747864\n",
      "Epoch 275, lr 1.0986328125e-08\n",
      "Epoch 275: Train 0.29326382052639255, Test 0.28775134682655334\n",
      "Epoch 300, lr 1.0986328125e-08\n",
      "Epoch 300: Train 0.29328013135903125, Test 0.287767231464386\n",
      "Epoch 325, lr 1.0986328125e-08\n",
      "Epoch 325: Train 0.2932962756873905, Test 0.2877829372882843\n",
      "Epoch 350, lr 1.0986328125e-08\n",
      "Epoch 350: Train 0.29331252497175464, Test 0.2877989411354065\n",
      "Epoch 375, lr 1.0986328125e-08\n",
      "Epoch 375: Train 0.2933288417432619, Test 0.28781479597091675\n",
      "Epoch 400, lr 1.0986328125e-08\n",
      "Epoch 400: Train 0.293345110139985, Test 0.28783082962036133\n",
      "Epoch 425, lr 1.0986328125e-08\n",
      "Epoch 425: Train 0.2933612648343694, Test 0.28784653544425964\n",
      "Epoch 450, lr 1.0986328125e-08\n",
      "Epoch 450: Train 0.2933774718987769, Test 0.2878623306751251\n",
      "Epoch 475, lr 1.0986328125e-08\n",
      "Epoch 475: Train 0.29339361665905384, Test 0.28787821531295776\n"
     ]
    }
   ],
   "source": [
    "# train_models(data,model_params = [],epochs = 500, lr = 0.000045, dir_label = 'wafer_layer_split_mip_std_1_mean_nonzero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0cd4ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating MSE of trained models\n",
    "path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models/cond_AE_wafer_layer_split_mip_std_1_mean_nonzero'\n",
    "models = os.listdir(path)\n",
    "cur_model = models[1]\n",
    "def get_data(data,cur_model):\n",
    "    cur_data = []\n",
    "    for d in data:\n",
    "        if d[1] == cur_model:\n",
    "            cur_data = d[0]\n",
    "            break\n",
    "    return cur_data\n",
    "test_set = []\n",
    "pred_set = []\n",
    "\n",
    "for m in models:\n",
    "    cur_model = torch.load(os.path.join(path,m)).to('cpu')\n",
    "    cur_data = get_data(data_smol,m).to('cpu')\n",
    "    if len(cur_data)< 20000:\n",
    "        test_data = cur_data[-int(0.19*len(cur_data)):]\n",
    "        test_set.append(test_data)\n",
    "        pred_set.append(cur_model(test_data[:,0:48]))\n",
    "    else:\n",
    "        test_data = cur_data[-int(0.19*20000):]\n",
    "        test_set.append(test_data)\n",
    "        pred_set.append(cur_model(test_data[:,0:48]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "189f5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all past 29\n",
    "high_p_error = []\n",
    "for m in models:\n",
    "    if int(m[10:]) >= 29 and int(m[8]) == 3:\n",
    "        high_p_error.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "38f6b36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dt_norm_3_36: 1.7781712449073792\n",
      "model dt_norm_3_30: 2.926579457788468\n",
      "model dt_norm_3_31: 3.2355160546989445\n",
      "model dt_norm_3_33: 3.217410323478699\n",
      "model dt_norm_3_35: 1.9059564955472947\n",
      "model dt_norm_3_37: 2.063708011110306\n",
      "model dt_norm_3_27: 1.595603267054558\n",
      "model dt_norm_3_29: 4.254838676010132\n",
      "model dt_norm_3_32: 2.868783745493889\n",
      "model dt_norm_3_34: 2.577686728151322\n",
      "model dt_norm_3_38: 2.293306006024361\n",
      "model dt_norm_1_1: 0.5635025843582154\n",
      "model dt_norm_1_3: 4.245519473276139\n",
      "model dt_norm_1_5: 6.042746063661576\n",
      "model dt_norm_1_7: 6.7547323364639285\n",
      "model dt_norm_1_9: 6.21603508610344\n",
      "model dt_norm_1_11: 6.593338716373444\n",
      "model dt_norm_1_13: 6.620123031646729\n",
      "model dt_norm_1_15: 4.57067295929718\n",
      "model dt_norm_1_17: 3.43209436681366\n",
      "model dt_norm_1_19: 2.8003740536813737\n",
      "model dt_norm_1_21: 2.113133949457169\n",
      "model dt_norm_1_23: 1.6971079431095124\n",
      "model dt_norm_1_25: 1.623607799554825\n",
      "model dt_norm_1_27: 0.9262356635956764\n",
      "model dt_norm_1_29: 2.825539758871079\n",
      "model dt_norm_1_30: 2.1416472303771976\n",
      "model dt_norm_1_31: 2.1364688651275636\n",
      "model dt_norm_1_32: 1.8154727235574724\n",
      "model dt_norm_1_33: 1.6802864167957308\n",
      "model dt_norm_2_1: 0.13860620479863883\n",
      "model dt_norm_2_3: 1.6893016316843035\n",
      "model dt_norm_2_5: 2.209098590769768\n",
      "model dt_norm_2_7: 3.714969987436295\n",
      "model dt_norm_2_9: 2.8471588773736958\n",
      "model dt_norm_2_11: 3.111865150039673\n",
      "model dt_norm_2_13: 3.339299619501114\n",
      "model dt_norm_2_15: 2.0100201223669054\n",
      "model dt_norm_2_17: 2.047404692264557\n",
      "model dt_norm_2_19: 1.8750966287698747\n",
      "model dt_norm_2_21: 1.6104448847351076\n",
      "model dt_norm_2_23: 1.6108851941585542\n",
      "model dt_norm_2_25: 1.0826234069638254\n",
      "model dt_norm_2_27: 3.356507668493271\n",
      "model dt_norm_2_29: 2.3614265477142338\n",
      "model dt_norm_2_30: 2.1178258228712084\n",
      "model dt_norm_2_31: 2.4095935052833557\n",
      "model dt_norm_2_32: 1.8861453535652162\n",
      "model dt_norm_2_34: 2.025849561441422\n",
      "model dt_norm_2_35: 1.4298414213781359\n",
      "model dt_norm_2_36: 1.2757038186693193\n",
      "model dt_norm_2_37: 1.5777575669260027\n",
      "model dt_norm_2_38: 0.97850821905756\n",
      "model dt_norm_2_39: 0.746887425217867\n",
      "model dt_norm_2_40: 0.4530609160344601\n",
      "model dt_norm_2_41: 0.2200983051928282\n",
      "model dt_norm_2_42: 0.3861557693116665\n",
      "model dt_norm_2_43: 0.5724441670475007\n",
      "model dt_norm_2_44: 12.077339540542603\n",
      "model dt_norm_2_45: 1.0503738019766808\n",
      "model dt_norm_3_1: 0.0864075597433448\n",
      "model dt_norm_3_3: 11.316161394470216\n",
      "model dt_norm_3_5: 20.24182765853119\n",
      "model dt_norm_3_7: 30.280510086273196\n",
      "model dt_norm_3_9: 27.031535475128177\n",
      "model dt_norm_3_11: 20.01407009765625\n",
      "model dt_norm_3_13: 18.658804369262697\n",
      "model dt_norm_3_15: 14.093751939353943\n",
      "model dt_norm_3_17: 12.789619971199036\n",
      "model dt_norm_3_19: 12.752915896362305\n",
      "model dt_norm_3_21: 17.37801065526581\n",
      "model dt_norm_3_23: 16.05442940009308\n",
      "model dt_norm_3_25: 8.90072998715973\n",
      "model dt_norm_3_39: 1.9421425483875276\n",
      "model dt_norm_3_40: 2.0587900478954317\n",
      "model dt_norm_3_41: 1.3534766699695588\n",
      "model dt_norm_3_42: 0.9083595461354257\n",
      "model dt_norm_3_43: 0.7660377321217061\n",
      "model dt_norm_3_44: 1.2312008512687684\n",
      "model dt_norm_3_45: 1.563490169116974\n",
      "model dt_norm_3_46: 1.1419821806173325\n",
      "model dt_norm_3_47: 0.9077792057032585\n"
     ]
    }
   ],
   "source": [
    "stats_per = []\n",
    "for i in range(len(pred_set)):\n",
    "    stats_per.append(AE_MSE(pred_set[i],test_set[i][:,0:48]).item()*3.5280**2)\n",
    "    print(f'model {models[i]}: {stats_per[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd44985",
   "metadata": {},
   "source": [
    "## Training Larger Model on only bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7887ef31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2ebdd976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_smol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "099ba6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "80b73a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ae_train' from '/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py'>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(ae_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d32a80da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.8976e+04,\n",
      "          2.0000e+00,  1.5498e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.8676e+04,\n",
      "          2.0000e+00,  2.8697e+00],\n",
      "        [ 1.6895e-01,  0.0000e+00,  2.7909e-01,  ..., -6.6576e+04,\n",
      "          2.0000e+00,  1.3304e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.5938e-01,  0.0000e+00,  ...,  7.1076e+04,\n",
      "          2.0000e+00,  3.5521e+00],\n",
      "        [ 0.0000e+00,  9.9419e-01,  5.3750e-01,  ...,  7.1196e+04,\n",
      "          2.0000e+00,  5.6863e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.3476e+04,\n",
      "          2.0000e+00,  1.5335e+00]]), 8000, 1900, 'dt_norm_3_36', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_36\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 1.090, Test 1.134\n",
      "MSE NON-NORMALIZED: Train MSE 23.363, Test MSE 22.221\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 0.695, Test 0.638\n",
      "MSE NON-NORMALIZED: Train MSE 11.977, Test MSE 11.818\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.584, Test 0.543\n",
      "MSE NON-NORMALIZED: Train MSE 10.326, Test MSE 10.196\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.544, Test 0.504\n",
      "MSE NON-NORMALIZED: Train MSE 9.696, Test MSE 9.575\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.8330e+04,\n",
      "          2.0000e+00,  1.5180e+01],\n",
      "        [ 2.5604e-01,  0.0000e+00,  2.1648e-01,  ..., -7.5930e+04,\n",
      "          2.0000e+00,  3.2482e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.5870e+04,\n",
      "          2.0000e+00,  4.5606e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.0950e+04,\n",
      "          2.0000e+00,  2.0725e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.1010e+04,\n",
      "          2.0000e+00,  1.2476e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.3530e+04,\n",
      "          2.0000e+00,  4.0203e+00]]), 8000, 1900, 'dt_norm_3_30', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_30\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 2.699, Test 2.684\n",
      "MSE NON-NORMALIZED: Train MSE 65.587, Test MSE 64.175\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 1.707, Test 1.876\n",
      "MSE NON-NORMALIZED: Train MSE 44.526, Test MSE 44.197\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 1.550, Test 1.713\n",
      "MSE NON-NORMALIZED: Train MSE 40.960, Test MSE 40.736\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 1.469, Test 1.635\n",
      "MSE NON-NORMALIZED: Train MSE 39.312, Test MSE 39.142\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.3771e+04,\n",
      "          2.0000e+00,  3.7594e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1011e+04,\n",
      "          2.0000e+00,  9.5197e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.8431e+04,\n",
      "          2.0000e+00,  4.7105e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8611e+04,\n",
      "          2.0000e+00,  1.8193e+01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.1011e+04,\n",
      "          2.0000e+00,  6.3131e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.3711e+04,\n",
      "          2.0000e+00,  3.1471e+00]]), 8000, 1900, 'dt_norm_3_31', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_31\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 2.452, Test 2.678\n",
      "MSE NON-NORMALIZED: Train MSE 72.667, Test MSE 70.078\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 1.305, Test 1.462\n",
      "MSE NON-NORMALIZED: Train MSE 33.077, Test MSE 32.472\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 1.073, Test 1.191\n",
      "MSE NON-NORMALIZED: Train MSE 26.081, Test MSE 25.504\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.965, Test 1.058\n",
      "MSE NON-NORMALIZED: Train MSE 23.023, Test MSE 22.433\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.8853e+04,\n",
      "          2.0000e+00,  1.2404e+00],\n",
      "        [ 4.2523e-01,  1.3107e-01,  0.0000e+00,  ..., -6.6513e+04,\n",
      "          2.0000e+00,  5.0201e+00],\n",
      "        [ 1.2300e-01,  0.0000e+00,  0.0000e+00,  ..., -6.6273e+04,\n",
      "          2.0000e+00,  1.8881e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.0833e+04,\n",
      "          2.0000e+00,  1.3092e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.3646e-01,  ...,  7.1013e+04,\n",
      "          2.0000e+00,  5.1343e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.1073e+04,\n",
      "          2.0000e+00,  1.9227e+00]]), 8000, 1900, 'dt_norm_3_33', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_33\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 2.355, Test 1.956\n",
      "MSE NON-NORMALIZED: Train MSE 58.629, Test MSE 54.824\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 1.047, Test 0.990\n",
      "MSE NON-NORMALIZED: Train MSE 20.364, Test MSE 19.843\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.864, Test 0.831\n",
      "MSE NON-NORMALIZED: Train MSE 17.024, Test MSE 16.621\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.766, Test 0.741\n",
      "MSE NON-NORMALIZED: Train MSE 15.108, Test MSE 14.793\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  2.2722e-01,  1.4015e-01,  ..., -6.8915e+04,\n",
      "          2.0000e+00,  1.1377e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  2.4152e-01,  ..., -6.8855e+04,\n",
      "          2.0000e+00,  3.7856e+00],\n",
      "        [ 0.0000e+00,  2.4230e-01,  1.0626e-01,  ..., -6.6515e+04,\n",
      "          2.0000e+00,  1.8337e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8615e+04,\n",
      "          2.0000e+00,  5.3268e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8675e+04,\n",
      "          2.0000e+00,  1.5508e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8735e+04,\n",
      "          2.0000e+00,  5.9645e+00]]), 8000, 1900, 'dt_norm_3_35', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_35\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 0.976, Test 1.073\n",
      "MSE NON-NORMALIZED: Train MSE 22.068, Test MSE 21.127\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 0.691, Test 0.693\n",
      "MSE NON-NORMALIZED: Train MSE 13.726, Test MSE 13.459\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.604, Test 0.600\n",
      "MSE NON-NORMALIZED: Train MSE 12.086, Test MSE 11.871\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.562, Test 0.558\n",
      "MSE NON-NORMALIZED: Train MSE 11.399, Test MSE 11.183\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.3877e+04,\n",
      "          2.0000e+00,  1.7207e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  6.2950e-02,  ..., -5.9257e+04,\n",
      "          2.0000e+00,  1.0486e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.9077e+04,\n",
      "          2.0000e+00,  4.8287e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.1717e+04,\n",
      "          2.0000e+00,  3.6001e+00],\n",
      "        [ 8.1937e-01,  0.0000e+00,  4.4586e-01,  ...,  6.8617e+04,\n",
      "          2.0000e+00,  4.4966e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8737e+04,\n",
      "          2.0000e+00,  2.1766e+00]]), 7699, 1828, 'dt_norm_3_37', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_37\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9527\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 0.891, Test 0.864\n",
      "MSE NON-NORMALIZED: Train MSE 17.672, Test MSE 16.648\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 0.537, Test 0.503\n",
      "MSE NON-NORMALIZED: Train MSE 9.849, Test MSE 9.764\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.467, Test 0.428\n",
      "MSE NON-NORMALIZED: Train MSE 8.633, Test MSE 8.564\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.434, Test 0.355\n",
      "MSE NON-NORMALIZED: Train MSE 7.328, Test MSE 7.258\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1307e+04,\n",
      "          2.0000e+00,  1.8293e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1247e+04,\n",
      "          2.0000e+00,  1.2351e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1127e+04,\n",
      "          2.0000e+00,  1.0411e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.6387e+04,\n",
      "          2.0000e+00,  1.4601e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8607e+04,\n",
      "          2.0000e+00,  2.1904e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8727e+04,\n",
      "          2.0000e+00,  4.7179e+00]]), 8000, 1900, 'dt_norm_3_27', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_27\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 2.714, Test 2.560\n",
      "MSE NON-NORMALIZED: Train MSE 54.586, Test MSE 52.723\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 1.917, Test 1.751\n",
      "MSE NON-NORMALIZED: Train MSE 36.061, Test MSE 35.221\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 1.674, Test 1.480\n",
      "MSE NON-NORMALIZED: Train MSE 30.406, Test MSE 29.629\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 1.508, Test 1.319\n",
      "MSE NON-NORMALIZED: Train MSE 27.084, Test MSE 26.317\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  6.4552e-02,  0.0000e+00,  ..., -7.3409e+04,\n",
      "          2.0000e+00,  5.0281e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1009e+04,\n",
      "          2.0000e+00,  6.7558e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.8789e+04,\n",
      "          2.0000e+00,  4.8549e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8729e+04,\n",
      "          2.0000e+00,  5.0884e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.1309e+04,\n",
      "          2.0000e+00,  2.0607e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.3709e+04,\n",
      "          2.0000e+00,  7.2930e+00]]), 8000, 1900, 'dt_norm_3_29', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_29\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 1.465, Test 1.718\n",
      "MSE NON-NORMALIZED: Train MSE 39.612, Test MSE 37.533\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 1.028, Test 1.233\n",
      "MSE NON-NORMALIZED: Train MSE 27.110, Test MSE 25.644\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.895, Test 1.093\n",
      "MSE NON-NORMALIZED: Train MSE 24.175, Test MSE 22.761\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.815, Test 1.012\n",
      "MSE NON-NORMALIZED: Train MSE 22.549, Test MSE 21.176\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  3.4941e-02,  ..., -7.8332e+04,\n",
      "          2.0000e+00,  8.6412e+00],\n",
      "        [ 0.0000e+00,  8.8586e-01,  8.1723e-01,  ..., -7.1132e+04,\n",
      "          2.0000e+00,  2.1555e+00],\n",
      "        [ 0.0000e+00,  3.3070e-01,  0.0000e+00,  ..., -6.6392e+04,\n",
      "          2.0000e+00,  1.3730e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  1.1080e-01,  3.4470e-01,  ...,  6.8312e+04,\n",
      "          2.0000e+00,  3.1685e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.8852e+04,\n",
      "          2.0000e+00,  5.9466e+00],\n",
      "        [ 0.0000e+00,  2.2691e-01,  2.9827e-01,  ...,  7.3532e+04,\n",
      "          2.0000e+00,  4.2605e+00]]), 8000, 1900, 'dt_norm_3_32', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_32\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 2.780, Test 2.761\n",
      "MSE NON-NORMALIZED: Train MSE 67.980, Test MSE 65.294\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 1.526, Test 1.590\n",
      "MSE NON-NORMALIZED: Train MSE 35.240, Test MSE 34.721\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 1.273, Test 1.332\n",
      "MSE NON-NORMALIZED: Train MSE 29.076, Test MSE 28.671\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 1.156, Test 1.182\n",
      "MSE NON-NORMALIZED: Train MSE 25.834, Test MSE 25.453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.3774e+04,\n",
      "          2.0000e+00,  2.4247e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.1254e+04,\n",
      "          2.0000e+00,  5.5180e+00],\n",
      "        [ 8.3322e-01,  0.0000e+00,  6.7710e-02,  ..., -6.8974e+04,\n",
      "          2.0000e+00,  1.5539e+00],\n",
      "        ...,\n",
      "        [ 3.2210e-01,  6.0924e-02,  3.5944e-01,  ...,  6.8854e+04,\n",
      "          2.0000e+00,  1.0692e+00],\n",
      "        [ 8.9757e-02,  1.7645e-01,  1.8574e-01,  ...,  7.1074e+04,\n",
      "          2.0000e+00,  8.8503e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.1194e+04,\n",
      "          2.0000e+00,  5.0756e+00]]), 8000, 1900, 'dt_norm_3_34', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_34\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 0.855, Test 1.018\n",
      "MSE NON-NORMALIZED: Train MSE 20.504, Test MSE 18.654\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 0.573, Test 0.623\n",
      "MSE NON-NORMALIZED: Train MSE 11.726, Test MSE 10.668\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.505, Test 0.537\n",
      "MSE NON-NORMALIZED: Train MSE 10.058, Test MSE 9.115\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.450, Test 0.476\n",
      "MSE NON-NORMALIZED: Train MSE 8.937, Test MSE 8.073\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 1.1736e-01,  0.0000e+00,  0.0000e+00,  ..., -6.1538e+04,\n",
      "          2.0000e+00,  3.1641e+00],\n",
      "        [ 0.0000e+00,  6.0771e-02,  6.8127e-02,  ..., -6.1178e+04,\n",
      "          2.0000e+00,  2.6774e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  5.4719e-02,  ..., -5.9138e+04,\n",
      "          2.0000e+00,  1.2262e+01],\n",
      "        ...,\n",
      "        [ 3.6763e-02,  0.0000e+00,  0.0000e+00,  ...,  6.4178e+04,\n",
      "          2.0000e+00,  5.2385e+00],\n",
      "        [ 3.6284e-02,  1.4263e-01,  0.0000e+00,  ...,  7.1078e+04,\n",
      "          2.0000e+00,  1.7011e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.3478e+04,\n",
      "          2.0000e+00,  3.6724e+00]]), 8000, 1900, 'dt_norm_3_38', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_3_38\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 1.452, Test 1.472\n",
      "MSE NON-NORMALIZED: Train MSE 33.185, Test MSE 31.751\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 0.623, Test 0.678\n",
      "MSE NON-NORMALIZED: Train MSE 13.678, Test MSE 13.532\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 0.493, Test 0.550\n",
      "MSE NON-NORMALIZED: Train MSE 11.206, Test MSE 11.120\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 0.427, Test 0.486\n",
      "MSE NON-NORMALIZED: Train MSE 10.131, Test MSE 10.039\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  7.7518e-02,  ..., -6.1261e+04,\n",
      "          0.0000e+00,  1.1250e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  2.3256e-01,  ..., -6.1261e+04,\n",
      "          0.0000e+00,  1.0906e+00],\n",
      "        [ 0.0000e+00,  5.1842e-02,  8.1094e-01,  ..., -5.9101e+04,\n",
      "          0.0000e+00,  1.5127e+00],\n",
      "        ...,\n",
      "        [ 2.5083e+00,  9.1423e-01,  4.9318e-02,  ...,  6.1321e+04,\n",
      "          0.0000e+00,  1.1818e+00],\n",
      "        [ 1.0808e-01,  0.0000e+00,  2.8176e-01,  ...,  6.1441e+04,\n",
      "          0.0000e+00,  1.9533e+00],\n",
      "        [ 3.2424e-01,  1.8423e-01,  1.2295e+00,  ...,  6.1441e+04,\n",
      "          0.0000e+00,  2.5864e+00]]), 8000, 1900, 'dt_norm_1_1', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_1_1\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 29.975, Test 32.445\n",
      "MSE NON-NORMALIZED: Train MSE 1097.299, Test MSE 1054.064\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 20.397, Test 22.225\n",
      "MSE NON-NORMALIZED: Train MSE 760.320, Test MSE 727.551\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 17.396, Test 18.645\n",
      "MSE NON-NORMALIZED: Train MSE 639.288, Test MSE 606.405\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 15.949, Test 16.909\n",
      "MSE NON-NORMALIZED: Train MSE 578.174, Test MSE 545.045\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 2.4103e-01,  0.0000e+00,  6.0930e-01,  ..., -6.1443e+04,\n",
      "          0.0000e+00,  2.1830e+00],\n",
      "        [ 5.3563e-02,  1.8260e-01,  1.5233e-01,  ..., -6.1443e+04,\n",
      "          0.0000e+00,  1.0103e+00],\n",
      "        [ 1.0713e-01,  4.6954e-01,  1.0155e-01,  ..., -6.1443e+04,\n",
      "          0.0000e+00,  4.2265e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  4.1736e-01,  1.7771e-01,  ...,  6.1443e+04,\n",
      "          0.0000e+00,  1.8785e+01],\n",
      "        [ 2.9995e+00,  1.3564e+00,  3.6558e+00,  ...,  6.1443e+04,\n",
      "          0.0000e+00,  1.9884e+01],\n",
      "        [ 1.9283e+00,  0.0000e+00,  2.5388e-02,  ...,  6.1443e+04,\n",
      "          0.0000e+00,  1.9369e+00]]), 8000, 1900, 'dt_norm_1_3', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_1_3\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 40.757, Test 46.838\n",
      "MSE NON-NORMALIZED: Train MSE 1757.972, Test MSE 1726.656\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 26.127, Test 31.183\n",
      "MSE NON-NORMALIZED: Train MSE 1228.500, Test MSE 1206.345\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 21.494, Test 26.312\n",
      "MSE NON-NORMALIZED: Train MSE 1058.114, Test MSE 1035.649\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 19.680, Test 24.146\n",
      "MSE NON-NORMALIZED: Train MSE 976.399, Test MSE 953.190\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 7.9642e-02,  3.3613e-01,  0.0000e+00,  ..., -6.1445e+04,\n",
      "          0.0000e+00,  1.5419e+00],\n",
      "        [ 1.0619e-01,  2.8442e-01,  0.0000e+00,  ..., -6.1445e+04,\n",
      "          0.0000e+00,  3.1230e+00],\n",
      "        [ 0.0000e+00,  2.8442e-01,  5.0328e-02,  ..., -6.1445e+04,\n",
      "          0.0000e+00,  1.0681e+00],\n",
      "        ...,\n",
      "        [ 5.0971e+00,  2.0685e+00,  2.4158e+00,  ...,  6.1445e+04,\n",
      "          0.0000e+00,  1.5602e+01],\n",
      "        [ 0.0000e+00,  2.0685e-01,  0.0000e+00,  ...,  6.1445e+04,\n",
      "          0.0000e+00,  1.0506e+00],\n",
      "        [ 8.4952e-01,  0.0000e+00,  5.0328e-02,  ...,  6.1445e+04,\n",
      "          0.0000e+00,  2.3945e+00]]), 8000, 1900, 'dt_norm_1_5', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_1_5\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 52.712, Test 61.832\n",
      "MSE NON-NORMALIZED: Train MSE 2882.512, Test MSE 3001.291\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 36.118, Test 45.684\n",
      "MSE NON-NORMALIZED: Train MSE 2142.577, Test MSE 2223.191\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 31.275, Test 39.433\n",
      "MSE NON-NORMALIZED: Train MSE 1847.382, Test MSE 1903.134\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 28.791, Test 36.201\n",
      "MSE NON-NORMALIZED: Train MSE 1682.566, Test MSE 1719.244\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 2.1054e+00,  1.0253e-01,  2.9933e-01,  ..., -6.1447e+04,\n",
      "          0.0000e+00,  1.3607e+00],\n",
      "        [ 3.9476e-01,  0.0000e+00,  0.0000e+00,  ..., -6.1447e+04,\n",
      "          0.0000e+00,  1.1121e+00],\n",
      "        [ 2.6317e-02,  5.1263e-02,  3.4922e-01,  ..., -6.1447e+04,\n",
      "          0.0000e+00,  1.5073e+00],\n",
      "        ...,\n",
      "        [ 7.5793e+00,  2.8707e+00,  2.5942e+00,  ...,  6.1447e+04,\n",
      "          0.0000e+00,  1.7242e+01],\n",
      "        [ 4.7371e-01,  3.5884e-01,  1.7461e-01,  ...,  6.1447e+04,\n",
      "          0.0000e+00,  2.9248e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  6.1447e+04,\n",
      "          0.0000e+00,  2.0947e+00]]), 8000, 1900, 'dt_norm_1_7', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_1_7\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 55.070, Test 58.942\n",
      "MSE NON-NORMALIZED: Train MSE 3790.510, Test MSE 3690.342\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 44.901, Test 47.512\n",
      "MSE NON-NORMALIZED: Train MSE 3038.489, Test MSE 2937.707\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 39.860, Test 41.707\n",
      "MSE NON-NORMALIZED: Train MSE 2617.877, Test MSE 2521.554\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 37.657, Test 38.425\n",
      "MSE NON-NORMALIZED: Train MSE 2359.312, Test MSE 2259.679\n",
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "    (ReLU): ReLU()\n",
      "  )\n",
      "), tensor([[ 5.2182e-01,  0.0000e+00,  0.0000e+00,  ..., -6.1449e+04,\n",
      "          0.0000e+00,  4.4769e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  2.1761e+00,  ..., -6.1449e+04,\n",
      "          0.0000e+00,  1.0612e+00],\n",
      "        [ 7.8272e-02,  0.0000e+00,  0.0000e+00,  ..., -6.1449e+04,\n",
      "          0.0000e+00,  1.1898e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  5.0821e-02,  5.4403e-01,  ...,  6.1449e+04,\n",
      "          0.0000e+00,  4.7058e+00],\n",
      "        [ 2.6091e-02,  0.0000e+00,  2.9674e-01,  ...,  6.1449e+04,\n",
      "          0.0000e+00,  5.5506e+00],\n",
      "        [ 1.5654e-01,  0.0000e+00,  2.7201e-01,  ...,  6.1449e+04,\n",
      "          0.0000e+00,  1.6291e+00]]), 8000, 1900, 'dt_norm_1_9', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial']\n",
      "TRAINING MODEL dt_norm_1_9\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9900\n",
      "Epoch 0, lr 8.5e-07\n",
      "Epoch 0: Train 36.748, Test 52.436\n",
      "MSE NON-NORMALIZED: Train MSE 3552.085, Test MSE 3703.666\n",
      "Epoch 25, lr 8.5e-07\n",
      "Epoch 25: Train 30.611, Test 41.754\n",
      "MSE NON-NORMALIZED: Train MSE 2791.129, Test MSE 2831.368\n",
      "Epoch 50, lr 8.5e-07\n",
      "Epoch 50: Train 27.763, Test 36.628\n",
      "MSE NON-NORMALIZED: Train MSE 2417.741, Test MSE 2400.285\n",
      "Epoch 75, lr 8.5e-07\n",
      "Epoch 75: Train 26.236, Test 33.588\n",
      "MSE NON-NORMALIZED: Train MSE 2185.291, Test MSE 2136.379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [238], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mae_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_smol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_1_high_occupancy_greater_7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_2_high_occupancy_greater_7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_3_high_occupancy_greater_7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43mappend_ReLU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8.5e-7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmax_dt_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdir_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py:249\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(data, batch, override, model_params, loss, path_1, path_2, path_3, append_ReLU, max_dt_size, epochs, lr, dir_label, path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINING MODEL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#Training model\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#clear cuda after training each model\u001b[39;00m\n\u001b[1;32m    251\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py:131\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dt, size_train, size_test, label, cur_directory, path, loss, num_epochs, lr, batch)\u001b[0m\n\u001b[1;32m    129\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    130\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 131\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(batch_loss)\n\u001b[1;32m    136\u001b[0m data_test \u001b[38;5;241m=\u001b[39m test\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ae_train.train_models(data_smol,\n",
    "             override = False,\n",
    "             model_params = [],\n",
    "             path_1 = 'models/dt_1_high_occupancy_greater_7',\n",
    "             path_2 = 'models/dt_2_high_occupancy_greater_7',\n",
    "             path_3 = 'models/dt_3_high_occupancy_greater_7',\n",
    "             epochs = 100, \n",
    "             append_ReLU = True, \n",
    "             lr = 8.5e-7,\n",
    "             max_dt_size = 10000,\n",
    "             dir_label = 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc838f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Naive_DAE(\n",
      "  (encoders): ModuleList(\n",
      "    (0): Linear(in_features=48, out_features=450, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=450, out_features=250, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=250, out_features=100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (7): ReLU()\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=16, out_features=100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=100, out_features=250, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=250, out_features=450, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=450, out_features=48, bias=True)\n",
      "  )\n",
      "), tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.8330e+04,\n",
      "          2.0000e+00,  1.5180e+01],\n",
      "        [ 2.5604e-01,  0.0000e+00,  2.1648e-01,  ..., -7.5930e+04,\n",
      "          2.0000e+00,  3.2482e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.5870e+04,\n",
      "          2.0000e+00,  4.5606e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.0950e+04,\n",
      "          2.0000e+00,  2.0725e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.1010e+04,\n",
      "          2.0000e+00,  1.2476e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  7.3530e+04,\n",
      "          2.0000e+00,  4.0203e+00]]), 22809, 5417, 'dt_norm_3_30', 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_old_retrain']\n",
      "TRAINING MODEL dt_norm_3_30\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 28226\n",
      "Epoch 0, lr 0\n",
      "Epoch 0: Train 11.813, Test 6.399\n",
      "MSE NON-NORMALIZED: Train MSE 166.921, Test MSE 166.621\n",
      "Epoch 25, lr 0\n",
      "Epoch 25: Train 11.813, Test 6.399\n",
      "MSE NON-NORMALIZED: Train MSE 166.921, Test MSE 166.621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mae_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_smol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_1_greater_0_450_250_100_dif_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_2_greater_0_450_250_100_dif_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_3_greater_0_450_250_100_dif_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43mappend_ReLU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmax_dt_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdir_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_old_retrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py:249\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(data, batch, override, model_params, loss, path_1, path_2, path_3, append_ReLU, max_dt_size, epochs, lr, dir_label, path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINING MODEL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m#Training model\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#clear cuda after training each model\u001b[39;00m\n\u001b[1;32m    251\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py:131\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dt, size_train, size_test, label, cur_directory, path, loss, num_epochs, lr, batch)\u001b[0m\n\u001b[1;32m    129\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    130\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 131\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(batch_loss)\n\u001b[1;32m    136\u001b[0m data_test \u001b[38;5;241m=\u001b[39m test\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/optim/adam.py:354\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    351\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 354\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    357\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ae_train.train_models(data_smol,\n",
    "             override = False,\n",
    "             model_params = [],\n",
    "             path_1 = 'models/dt_1_greater_0_450_250_100_dif_2',\n",
    "             path_2 = 'models/dt_2_greater_0_450_250_100_dif_2',\n",
    "             path_3 = 'models/dt_3_greater_0_450_250_100_dif_2',\n",
    "             epochs = 250, \n",
    "             append_ReLU = True, \n",
    "             lr = 0,\n",
    "             max_dt_size = 30000,\n",
    "             dir_label = 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_old_retrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8ff107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_3_35\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 18260\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.804, Test 0.912\n",
      "MSE NON-NORMALIZED: Train MSE 20.610, Test MSE 17.459\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.254, Test 0.305\n",
      "MSE NON-NORMALIZED: Train MSE 5.677, Test MSE 5.474\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.222, Test 0.260\n",
      "MSE NON-NORMALIZED: Train MSE 4.928, Test MSE 4.722\n",
      "TRAINING MODEL dt_norm_3_37\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9527\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.870, Test 0.943\n",
      "MSE NON-NORMALIZED: Train MSE 23.199, Test MSE 19.161\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.218, Test 0.263\n",
      "MSE NON-NORMALIZED: Train MSE 5.314, Test MSE 5.226\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.183, Test 0.218\n",
      "MSE NON-NORMALIZED: Train MSE 4.593, Test MSE 4.510\n",
      "TRAINING MODEL dt_norm_3_27\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 20249\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 3.578, Test 2.947\n",
      "MSE NON-NORMALIZED: Train MSE 70.525, Test MSE 61.118\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 1.012, Test 0.984\n",
      "MSE NON-NORMALIZED: Train MSE 20.545, Test MSE 19.903\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.834, Test 0.829\n",
      "MSE NON-NORMALIZED: Train MSE 17.683, Test MSE 17.123\n",
      "TRAINING MODEL dt_norm_3_29\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 2.742, Test 1.956\n",
      "MSE NON-NORMALIZED: Train MSE 51.969, Test MSE 44.972\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 1.396, Test 0.942\n",
      "MSE NON-NORMALIZED: Train MSE 24.144, Test MSE 23.711\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 1.307, Test 0.841\n",
      "MSE NON-NORMALIZED: Train MSE 22.250, Test MSE 21.782\n",
      "TRAINING MODEL dt_norm_3_32\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 22648\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 1.262, Test 2.223\n",
      "MSE NON-NORMALIZED: Train MSE 58.166, Test MSE 49.578\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.320, Test 0.512\n",
      "MSE NON-NORMALIZED: Train MSE 9.852, Test MSE 9.581\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.240, Test 0.448\n",
      "MSE NON-NORMALIZED: Train MSE 8.619, Test MSE 8.420\n",
      "TRAINING MODEL dt_norm_3_34\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 25789\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.531, Test 0.875\n",
      "MSE NON-NORMALIZED: Train MSE 19.412, Test MSE 16.484\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.234, Test 0.345\n",
      "MSE NON-NORMALIZED: Train MSE 7.083, Test MSE 6.512\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.210, Test 0.319\n",
      "MSE NON-NORMALIZED: Train MSE 6.590, Test MSE 6.056\n",
      "TRAINING MODEL dt_norm_3_38\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 12299\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 1.861, Test 1.772\n",
      "MSE NON-NORMALIZED: Train MSE 45.125, Test MSE 39.967\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.362, Test 0.432\n",
      "MSE NON-NORMALIZED: Train MSE 9.740, Test MSE 9.667\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.293, Test 0.380\n",
      "MSE NON-NORMALIZED: Train MSE 8.840, Test MSE 8.776\n",
      "TRAINING MODEL dt_norm_1_1\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 3.847, Test 4.220\n",
      "MSE NON-NORMALIZED: Train MSE 133.102, Test MSE 120.142\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 1.265, Test 1.179\n",
      "MSE NON-NORMALIZED: Train MSE 33.524, Test MSE 31.878\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 0.666, Test 0.563\n",
      "MSE NON-NORMALIZED: Train MSE 17.212, Test MSE 15.931\n",
      "TRAINING MODEL dt_norm_1_3\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 9.253, Test 8.547\n",
      "MSE NON-NORMALIZED: Train MSE 270.235, Test MSE 263.286\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 7.171, Test 6.990\n",
      "MSE NON-NORMALIZED: Train MSE 205.779, Test MSE 202.033\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 6.517, Test 6.350\n",
      "MSE NON-NORMALIZED: Train MSE 184.677, Test MSE 180.443\n",
      "TRAINING MODEL dt_norm_1_5\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 11.087, Test 10.074\n",
      "MSE NON-NORMALIZED: Train MSE 363.852, Test MSE 352.656\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 9.195, Test 8.265\n",
      "MSE NON-NORMALIZED: Train MSE 270.368, Test MSE 263.957\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 8.305, Test 7.461\n",
      "MSE NON-NORMALIZED: Train MSE 237.328, Test MSE 230.331\n",
      "TRAINING MODEL dt_norm_1_7\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 9.110, Test 8.332\n",
      "MSE NON-NORMALIZED: Train MSE 313.225, Test MSE 301.044\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 7.383, Test 6.683\n",
      "MSE NON-NORMALIZED: Train MSE 220.442, Test MSE 215.554\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 6.638, Test 5.965\n",
      "MSE NON-NORMALIZED: Train MSE 191.093, Test MSE 186.077\n",
      "TRAINING MODEL dt_norm_1_9\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 6.370, Test 6.155\n",
      "MSE NON-NORMALIZED: Train MSE 224.506, Test MSE 209.158\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 4.971, Test 4.799\n",
      "MSE NON-NORMALIZED: Train MSE 146.636, Test MSE 141.309\n",
      "Epoch 50, lr 4.5e-06\n",
      "Epoch 50: Train 4.418, Test 4.230\n",
      "MSE NON-NORMALIZED: Train MSE 125.085, Test MSE 120.473\n",
      "TRAINING MODEL dt_norm_1_11\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 29700\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 4.403, Test 4.533\n",
      "MSE NON-NORMALIZED: Train MSE 154.290, Test MSE 143.142\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 3.429, Test 3.364\n",
      "MSE NON-NORMALIZED: Train MSE 94.293, Test MSE 91.240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mae_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_models_telescope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_smol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_1_greater_0_450_250_100_dif_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_2_greater_0_450_250_100_dif_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m             \u001b[49m\u001b[43mpath_3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/dt_3_greater_0_450_250_100_dif_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m51\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m             \u001b[49m\u001b[43mappend_ReLU\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4.5e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmax_dt_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdir_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_tele\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py:738\u001b[0m, in \u001b[0;36mtrain_models_telescope\u001b[0;34m(data, batch_size, override, model_params, path_1, path_2, path_3, append_ReLU, max_dt_size, epochs, lr, dir_label, path)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRAINING MODEL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m#Training model\u001b[39;00m\n\u001b[0;32m--> 738\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m#clear cuda after training each model\u001b[39;00m\n\u001b[1;32m    740\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/ae_train.py:130\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dt, size_train, size_test, label, cur_directory, path, loss, num_epochs, lr, batch)\u001b[0m\n\u001b[1;32m    128\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(batch_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    129\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    134\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(batch_loss)\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ae_train.train_models_telescope(data_smol,\n",
    "             override = False,\n",
    "             model_params = [],\n",
    "             path_1 = 'models/dt_1_greater_0_450_250_100_dif_2',\n",
    "             path_2 = 'models/dt_2_greater_0_450_250_100_dif_2',\n",
    "             path_3 = 'models/dt_3_greater_0_450_250_100_dif_2',\n",
    "             epochs = 51,\n",
    "             append_ReLU = True, \n",
    "             lr = 4.5e-6,\n",
    "             max_dt_size = 30000,\n",
    "             dir_label = 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_tele')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f202352b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_3_41\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26195\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.182, Test 0.184\n",
      "MSE NON-NORMALIZED: Train MSE 3.406, Test MSE 2.846\n",
      "Epoch 25, lr 2.25e-06\n",
      "Epoch 25: Train 0.108, Test 0.119\n",
      "MSE NON-NORMALIZED: Train MSE 1.782, Test MSE 1.769\n",
      "TRAINING MODEL dt_norm_3_42\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 27786\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.173, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.799, Test MSE 2.405\n",
      "Epoch 25, lr 2.25e-06\n",
      "Epoch 25: Train 0.110, Test 0.113\n",
      "MSE NON-NORMALIZED: Train MSE 1.699, Test MSE 1.657\n",
      "TRAINING MODEL dt_norm_3_43\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 23805\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.149, Test 0.196\n",
      "MSE NON-NORMALIZED: Train MSE 3.708, Test MSE 3.174\n",
      "Epoch 25, lr 2.25e-06\n",
      "Epoch 25: Train 0.102, Test 0.120\n",
      "MSE NON-NORMALIZED: Train MSE 2.009, Test MSE 1.951\n",
      "TRAINING MODEL dt_norm_3_44\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 24190\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.221, Test 0.183\n",
      "MSE NON-NORMALIZED: Train MSE 3.118, Test MSE 2.741\n",
      "Epoch 25, lr 2.25e-06\n",
      "Epoch 25: Train 0.126, Test 0.116\n",
      "MSE NON-NORMALIZED: Train MSE 1.699, Test MSE 1.701\n",
      "TRAINING MODEL dt_norm_3_45\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 18646\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.198, Test 0.172\n",
      "MSE NON-NORMALIZED: Train MSE 3.320, Test MSE 2.734\n",
      "Epoch 25, lr 2.25e-06\n",
      "Epoch 25: Train 0.090, Test 0.077\n",
      "MSE NON-NORMALIZED: Train MSE 1.202, Test MSE 1.158\n",
      "TRAINING MODEL dt_norm_3_46\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 19839\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.216, Test 0.222\n",
      "MSE NON-NORMALIZED: Train MSE 3.914, Test MSE 3.421\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.092, Test 0.095\n",
      "MSE NON-NORMALIZED: Train MSE 1.442, Test MSE 1.427\n",
      "TRAINING MODEL dt_norm_3_47\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 17061\n",
      "Epoch 0, lr 4.5e-06\n",
      "Epoch 0: Train 0.206, Test 0.189\n",
      "MSE NON-NORMALIZED: Train MSE 3.931, Test MSE 2.996\n",
      "Epoch 25, lr 4.5e-06\n",
      "Epoch 25: Train 0.092, Test 0.085\n",
      "MSE NON-NORMALIZED: Train MSE 1.242, Test MSE 1.224\n"
     ]
    }
   ],
   "source": [
    "ae_train.train_models_telescope(data_smol_high,\n",
    "             override = False,\n",
    "             model_params = [],\n",
    "             path_1 = 'models/dt_1_greater_0_450_250_100_dif_2',\n",
    "             path_2 = 'models/dt_2_greater_0_450_250_100_dif_2',\n",
    "             path_3 = 'models/dt_3_greater_0_450_250_100_dif_2',\n",
    "             epochs = 26,\n",
    "             append_ReLU = True, \n",
    "             lr = 4.5e-6,\n",
    "             max_dt_size = 50000,\n",
    "             dir_label = 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_tele_only_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa1297e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_1_29\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.715, Test 0.511\n",
      "MSE NON-NORMALIZED: Train MSE 9.976, Test MSE 9.699\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.546, Test 0.404\n",
      "MSE NON-NORMALIZED: Train MSE 7.438, Test MSE 7.311\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.500, Test 0.372\n",
      "MSE NON-NORMALIZED: Train MSE 6.832, Test MSE 6.696\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.477, Test 0.359\n",
      "MSE NON-NORMALIZED: Train MSE 6.774, Test MSE 8.232\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.464, Test 0.352\n",
      "MSE NON-NORMALIZED: Train MSE 6.749, Test MSE 8.923\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.456, Test 0.347\n",
      "MSE NON-NORMALIZED: Train MSE 6.653, Test MSE 8.520\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.450, Test 0.345\n",
      "MSE NON-NORMALIZED: Train MSE 6.607, Test MSE 8.303\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.448, Test 0.344\n",
      "MSE NON-NORMALIZED: Train MSE 6.588, Test MSE 8.164\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.447, Test 0.344\n",
      "MSE NON-NORMALIZED: Train MSE 6.593, Test MSE 8.084\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.447, Test 0.345\n",
      "MSE NON-NORMALIZED: Train MSE 6.612, Test MSE 8.041\n",
      "Epoch 250, lr 2.25e-07\n",
      "Epoch 250: Train 0.447, Test 0.346\n",
      "MSE NON-NORMALIZED: Train MSE 6.637, Test MSE 8.030\n",
      "Epoch 275, lr 2.25e-07\n",
      "Epoch 275: Train 0.447, Test 0.346\n",
      "MSE NON-NORMALIZED: Train MSE 6.649, Test MSE 8.026\n",
      "Epoch 300, lr 1.125e-07\n",
      "Epoch 300: Train 0.447, Test 0.347\n",
      "MSE NON-NORMALIZED: Train MSE 6.663, Test MSE 8.026\n",
      "TRAINING MODEL dt_norm_1_30\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.478, Test 0.494\n",
      "MSE NON-NORMALIZED: Train MSE 9.557, Test MSE 9.253\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.354, Test 0.354\n",
      "MSE NON-NORMALIZED: Train MSE 6.221, Test MSE 6.148\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.319, Test 0.313\n",
      "MSE NON-NORMALIZED: Train MSE 5.419, Test MSE 5.321\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.306, Test 0.297\n",
      "MSE NON-NORMALIZED: Train MSE 5.248, Test MSE 6.536\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.301, Test 0.291\n",
      "MSE NON-NORMALIZED: Train MSE 5.203, Test MSE 6.891\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.300, Test 0.287\n",
      "MSE NON-NORMALIZED: Train MSE 5.118, Test MSE 6.465\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.299, Test 0.286\n",
      "MSE NON-NORMALIZED: Train MSE 5.094, Test MSE 6.267\n",
      "Epoch 175, lr 2.25e-07\n",
      "Epoch 175: Train 0.300, Test 0.286\n",
      "MSE NON-NORMALIZED: Train MSE 5.090, Test MSE 6.173\n",
      "Epoch 200, lr 5.625e-08\n",
      "Epoch 200: Train 0.300, Test 0.286\n",
      "MSE NON-NORMALIZED: Train MSE 5.093, Test MSE 6.156\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.300, Test 0.286\n",
      "MSE NON-NORMALIZED: Train MSE 5.092, Test MSE 6.151\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.300, Test 0.286\n",
      "MSE NON-NORMALIZED: Train MSE 5.088, Test MSE 6.147\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.300, Test 0.286\n",
      "MSE NON-NORMALIZED: Train MSE 5.085, Test MSE 6.142\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.300, Test 0.285\n",
      "MSE NON-NORMALIZED: Train MSE 5.081, Test MSE 6.138\n",
      "TRAINING MODEL dt_norm_1_31\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.575, Test 0.639\n",
      "MSE NON-NORMALIZED: Train MSE 16.863, Test MSE 16.151\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.356, Test 0.386\n",
      "MSE NON-NORMALIZED: Train MSE 8.182, Test MSE 8.136\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.286, Test 0.314\n",
      "MSE NON-NORMALIZED: Train MSE 6.470, Test MSE 6.410\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.255, Test 0.284\n",
      "MSE NON-NORMALIZED: Train MSE 5.959, Test MSE 7.799\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.240, Test 0.270\n",
      "MSE NON-NORMALIZED: Train MSE 5.752, Test MSE 8.314\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.231, Test 0.264\n",
      "MSE NON-NORMALIZED: Train MSE 5.598, Test MSE 7.888\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.225, Test 0.261\n",
      "MSE NON-NORMALIZED: Train MSE 5.502, Test MSE 7.556\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.223, Test 0.260\n",
      "MSE NON-NORMALIZED: Train MSE 5.483, Test MSE 7.446\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.223, Test 0.260\n",
      "MSE NON-NORMALIZED: Train MSE 5.489, Test MSE 7.404\n",
      "Epoch 225, lr 2.25e-07\n",
      "Epoch 225: Train 0.223, Test 0.261\n",
      "MSE NON-NORMALIZED: Train MSE 5.509, Test MSE 7.403\n",
      "Epoch 250, lr 5.625e-08\n",
      "Epoch 250: Train 0.223, Test 0.261\n",
      "MSE NON-NORMALIZED: Train MSE 5.516, Test MSE 7.412\n",
      "Epoch 275, lr 2.8125e-08\n",
      "Epoch 275: Train 0.223, Test 0.261\n",
      "MSE NON-NORMALIZED: Train MSE 5.517, Test MSE 7.411\n",
      "Epoch 300, lr 2.8125e-08\n",
      "Epoch 300: Train 0.223, Test 0.261\n",
      "MSE NON-NORMALIZED: Train MSE 5.517, Test MSE 7.411\n",
      "TRAINING MODEL dt_norm_1_32\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.491, Test 0.450\n",
      "MSE NON-NORMALIZED: Train MSE 8.877, Test MSE 8.479\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.319, Test 0.301\n",
      "MSE NON-NORMALIZED: Train MSE 5.397, Test MSE 5.334\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.276, Test 0.261\n",
      "MSE NON-NORMALIZED: Train MSE 4.672, Test MSE 4.608\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.261, Test 0.246\n",
      "MSE NON-NORMALIZED: Train MSE 4.588, Test MSE 5.991\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.254, Test 0.240\n",
      "MSE NON-NORMALIZED: Train MSE 4.563, Test MSE 6.485\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.250, Test 0.237\n",
      "MSE NON-NORMALIZED: Train MSE 4.484, Test MSE 6.103\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.248, Test 0.235\n",
      "MSE NON-NORMALIZED: Train MSE 4.449, Test MSE 5.929\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.246, Test 0.235\n",
      "MSE NON-NORMALIZED: Train MSE 4.428, Test MSE 5.819\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.244, Test 0.234\n",
      "MSE NON-NORMALIZED: Train MSE 4.417, Test MSE 5.753\n",
      "Epoch 225, lr 2.25e-07\n",
      "Epoch 225: Train 0.243, Test 0.235\n",
      "MSE NON-NORMALIZED: Train MSE 4.420, Test MSE 5.732\n",
      "Epoch 250, lr 5.625e-08\n",
      "Epoch 250: Train 0.244, Test 0.235\n",
      "MSE NON-NORMALIZED: Train MSE 4.427, Test MSE 5.737\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.244, Test 0.235\n",
      "MSE NON-NORMALIZED: Train MSE 4.426, Test MSE 5.736\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.243, Test 0.235\n",
      "MSE NON-NORMALIZED: Train MSE 4.423, Test MSE 5.731\n",
      "TRAINING MODEL dt_norm_1_33\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.441, Test 0.388\n",
      "MSE NON-NORMALIZED: Train MSE 7.730, Test MSE 7.372\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.256, Test 0.244\n",
      "MSE NON-NORMALIZED: Train MSE 4.149, Test MSE 4.099\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.214, Test 0.210\n",
      "MSE NON-NORMALIZED: Train MSE 3.555, Test MSE 3.507\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.196, Test 0.198\n",
      "MSE NON-NORMALIZED: Train MSE 3.530, Test MSE 4.899\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.186, Test 0.194\n",
      "MSE NON-NORMALIZED: Train MSE 3.554, Test MSE 5.647\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.181, Test 0.192\n",
      "MSE NON-NORMALIZED: Train MSE 3.499, Test MSE 5.289\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.179, Test 0.192\n",
      "MSE NON-NORMALIZED: Train MSE 3.489, Test MSE 5.105\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.178, Test 0.193\n",
      "MSE NON-NORMALIZED: Train MSE 3.506, Test MSE 5.016\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.177, Test 0.194\n",
      "MSE NON-NORMALIZED: Train MSE 3.510, Test MSE 4.965\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.176, Test 0.195\n",
      "MSE NON-NORMALIZED: Train MSE 3.531, Test MSE 4.939\n",
      "Epoch 250, lr 2.25e-07\n",
      "Epoch 250: Train 0.176, Test 0.197\n",
      "MSE NON-NORMALIZED: Train MSE 3.562, Test MSE 4.945\n",
      "Epoch 275, lr 5.625e-08\n",
      "Epoch 275: Train 0.177, Test 0.198\n",
      "MSE NON-NORMALIZED: Train MSE 3.574, Test MSE 4.951\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.177, Test 0.198\n",
      "MSE NON-NORMALIZED: Train MSE 3.575, Test MSE 4.950\n",
      "TRAINING MODEL dt_norm_2_27\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 19047\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 1.013, Test 0.976\n",
      "MSE NON-NORMALIZED: Train MSE 23.424, Test MSE 22.669\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.502, Test 0.524\n",
      "MSE NON-NORMALIZED: Train MSE 11.472, Test MSE 11.315\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.314, Test 0.346\n",
      "MSE NON-NORMALIZED: Train MSE 7.444, Test MSE 7.311\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.228, Test 0.253\n",
      "MSE NON-NORMALIZED: Train MSE 5.698, Test MSE 7.089\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.187, Test 0.207\n",
      "MSE NON-NORMALIZED: Train MSE 4.789, Test MSE 6.297\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.162, Test 0.179\n",
      "MSE NON-NORMALIZED: Train MSE 4.142, Test MSE 5.185\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.146, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.725, Test MSE 4.454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.136, Test 0.147\n",
      "MSE NON-NORMALIZED: Train MSE 3.412, Test MSE 3.994\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.130, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 3.261, Test MSE 3.762\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.125, Test 0.134\n",
      "MSE NON-NORMALIZED: Train MSE 3.159, Test MSE 3.565\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.122, Test 0.130\n",
      "MSE NON-NORMALIZED: Train MSE 3.082, Test MSE 3.423\n",
      "Epoch 275, lr 4.5e-07\n",
      "Epoch 275: Train 0.119, Test 0.127\n",
      "MSE NON-NORMALIZED: Train MSE 3.026, Test MSE 3.318\n",
      "Epoch 300, lr 4.5e-07\n",
      "Epoch 300: Train 0.117, Test 0.125\n",
      "MSE NON-NORMALIZED: Train MSE 2.986, Test MSE 3.230\n",
      "TRAINING MODEL dt_norm_2_29\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 36150\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.305, Test 0.364\n",
      "MSE NON-NORMALIZED: Train MSE 8.327, Test MSE 7.794\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.117, Test 0.207\n",
      "MSE NON-NORMALIZED: Train MSE 3.773, Test MSE 3.692\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.105, Test 0.177\n",
      "MSE NON-NORMALIZED: Train MSE 3.224, Test MSE 3.138\n",
      "Epoch 75, lr 2.25e-07\n",
      "Epoch 75: Train 0.104, Test 0.167\n",
      "MSE NON-NORMALIZED: Train MSE 3.190, Test MSE 4.035\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.106, Test 0.167\n",
      "MSE NON-NORMALIZED: Train MSE 3.268, Test MSE 4.674\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.107, Test 0.167\n",
      "MSE NON-NORMALIZED: Train MSE 3.258, Test MSE 4.615\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.107, Test 0.167\n",
      "MSE NON-NORMALIZED: Train MSE 3.250, Test MSE 4.582\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.106, Test 0.166\n",
      "MSE NON-NORMALIZED: Train MSE 3.243, Test MSE 4.552\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.106, Test 0.166\n",
      "MSE NON-NORMALIZED: Train MSE 3.236, Test MSE 4.525\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.106, Test 0.166\n",
      "MSE NON-NORMALIZED: Train MSE 3.229, Test MSE 4.498\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.106, Test 0.165\n",
      "MSE NON-NORMALIZED: Train MSE 3.222, Test MSE 4.473\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.106, Test 0.165\n",
      "MSE NON-NORMALIZED: Train MSE 3.215, Test MSE 4.449\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.106, Test 0.165\n",
      "MSE NON-NORMALIZED: Train MSE 3.208, Test MSE 4.425\n",
      "TRAINING MODEL dt_norm_2_30\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 24996\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.701, Test 0.697\n",
      "MSE NON-NORMALIZED: Train MSE 24.307, Test MSE 22.784\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.245, Test 0.257\n",
      "MSE NON-NORMALIZED: Train MSE 5.500, Test MSE 5.398\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.172, Test 0.180\n",
      "MSE NON-NORMALIZED: Train MSE 3.684, Test MSE 3.648\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.149, Test 0.155\n",
      "MSE NON-NORMALIZED: Train MSE 3.466, Test MSE 4.726\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.135, Test 0.146\n",
      "MSE NON-NORMALIZED: Train MSE 3.299, Test MSE 4.515\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.128, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 3.094, Test MSE 3.895\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.126, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.029, Test MSE 3.632\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.125, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.015, Test MSE 3.522\n",
      "Epoch 200, lr 2.25e-07\n",
      "Epoch 200: Train 0.126, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.018, Test MSE 3.473\n",
      "Epoch 225, lr 5.625e-08\n",
      "Epoch 225: Train 0.126, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.020, Test MSE 3.453\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.126, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.019, Test MSE 3.446\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.126, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.017, Test MSE 3.443\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.126, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 3.016, Test MSE 3.441\n",
      "TRAINING MODEL dt_norm_2_31\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 9924\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.412, Test 0.428\n",
      "MSE NON-NORMALIZED: Train MSE 9.139, Test MSE 8.822\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.273, Test 0.272\n",
      "MSE NON-NORMALIZED: Train MSE 4.887, Test MSE 4.827\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.225, Test 0.207\n",
      "MSE NON-NORMALIZED: Train MSE 3.713, Test MSE 3.665\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.189, Test 0.167\n",
      "MSE NON-NORMALIZED: Train MSE 3.119, Test MSE 3.520\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.165, Test 0.141\n",
      "MSE NON-NORMALIZED: Train MSE 2.626, Test MSE 2.899\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.148, Test 0.125\n",
      "MSE NON-NORMALIZED: Train MSE 2.278, Test MSE 2.377\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.136, Test 0.113\n",
      "MSE NON-NORMALIZED: Train MSE 2.051, Test MSE 2.054\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.127, Test 0.106\n",
      "MSE NON-NORMALIZED: Train MSE 1.905, Test MSE 1.860\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.122, Test 0.101\n",
      "MSE NON-NORMALIZED: Train MSE 1.820, Test MSE 1.765\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.117, Test 0.098\n",
      "MSE NON-NORMALIZED: Train MSE 1.762, Test MSE 1.702\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.114, Test 0.095\n",
      "MSE NON-NORMALIZED: Train MSE 1.721, Test MSE 1.660\n",
      "Epoch 275, lr 4.5e-07\n",
      "Epoch 275: Train 0.112, Test 0.093\n",
      "MSE NON-NORMALIZED: Train MSE 1.693, Test MSE 1.628\n",
      "Epoch 300, lr 4.5e-07\n",
      "Epoch 300: Train 0.110, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.675, Test MSE 1.608\n",
      "TRAINING MODEL dt_norm_2_32\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 8327\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.342, Test 0.255\n",
      "MSE NON-NORMALIZED: Train MSE 4.310, Test MSE 4.216\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.220, Test 0.166\n",
      "MSE NON-NORMALIZED: Train MSE 2.722, Test MSE 2.685\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.169, Test 0.131\n",
      "MSE NON-NORMALIZED: Train MSE 2.119, Test MSE 2.092\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.140, Test 0.113\n",
      "MSE NON-NORMALIZED: Train MSE 1.920, Test MSE 2.221\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.126, Test 0.103\n",
      "MSE NON-NORMALIZED: Train MSE 1.746, Test MSE 1.989\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.116, Test 0.097\n",
      "MSE NON-NORMALIZED: Train MSE 1.596, Test MSE 1.703\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.110, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.505, Test MSE 1.553\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.105, Test 0.089\n",
      "MSE NON-NORMALIZED: Train MSE 1.453, Test MSE 1.474\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.102, Test 0.087\n",
      "MSE NON-NORMALIZED: Train MSE 1.420, Test MSE 1.427\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.100, Test 0.086\n",
      "MSE NON-NORMALIZED: Train MSE 1.399, Test MSE 1.392\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.099, Test 0.085\n",
      "MSE NON-NORMALIZED: Train MSE 1.385, Test MSE 1.366\n",
      "Epoch 275, lr 4.5e-07\n",
      "Epoch 275: Train 0.097, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.374, Test MSE 1.348\n",
      "Epoch 300, lr 4.5e-07\n",
      "Epoch 300: Train 0.097, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.369, Test MSE 1.337\n",
      "TRAINING MODEL dt_norm_2_34\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.276, Test 0.272\n",
      "MSE NON-NORMALIZED: Train MSE 5.792, Test MSE 5.418\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.172, Test 0.172\n",
      "MSE NON-NORMALIZED: Train MSE 3.051, Test MSE 2.986\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.160, Test 0.159\n",
      "MSE NON-NORMALIZED: Train MSE 2.787, Test MSE 2.722\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.159, Test 0.160\n",
      "MSE NON-NORMALIZED: Train MSE 2.935, Test MSE 4.196\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.048, Test MSE 5.052\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.040, Test MSE 4.962\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.034, Test MSE 4.907\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.028, Test MSE 4.862\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.023, Test MSE 4.822\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.019, Test MSE 4.786\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.015, Test MSE 4.755\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.011, Test MSE 4.727\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.160, Test 0.161\n",
      "MSE NON-NORMALIZED: Train MSE 3.008, Test MSE 4.701\n",
      "TRAINING MODEL dt_norm_2_35\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.280, Test 0.265\n",
      "MSE NON-NORMALIZED: Train MSE 6.467, Test MSE 6.003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.132, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 2.356, Test MSE 2.313\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.124, Test 0.133\n",
      "MSE NON-NORMALIZED: Train MSE 2.196, Test MSE 2.152\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.128, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 2.421, Test MSE 3.493\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.129, Test 0.141\n",
      "MSE NON-NORMALIZED: Train MSE 2.537, Test MSE 4.345\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.129, Test 0.141\n",
      "MSE NON-NORMALIZED: Train MSE 2.534, Test MSE 4.274\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.130, Test 0.141\n",
      "MSE NON-NORMALIZED: Train MSE 2.532, Test MSE 4.225\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.130, Test 0.141\n",
      "MSE NON-NORMALIZED: Train MSE 2.530, Test MSE 4.180\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.130, Test 0.142\n",
      "MSE NON-NORMALIZED: Train MSE 2.529, Test MSE 4.143\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.130, Test 0.142\n",
      "MSE NON-NORMALIZED: Train MSE 2.528, Test MSE 4.109\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.130, Test 0.142\n",
      "MSE NON-NORMALIZED: Train MSE 2.527, Test MSE 4.079\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.130, Test 0.142\n",
      "MSE NON-NORMALIZED: Train MSE 2.527, Test MSE 4.050\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.130, Test 0.142\n",
      "MSE NON-NORMALIZED: Train MSE 2.527, Test MSE 4.024\n",
      "TRAINING MODEL dt_norm_2_36\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49500\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.269, Test 0.256\n",
      "MSE NON-NORMALIZED: Train MSE 5.528, Test MSE 5.080\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.114, Test 0.138\n",
      "MSE NON-NORMALIZED: Train MSE 2.330, Test MSE 2.295\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.107, Test 0.133\n",
      "MSE NON-NORMALIZED: Train MSE 2.203, Test MSE 2.166\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.111, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 2.401, Test MSE 3.541\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.113, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 2.523, Test MSE 4.448\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.114, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 2.519, Test MSE 4.371\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.515, Test MSE 4.325\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.512, Test MSE 4.285\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.510, Test MSE 4.250\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.508, Test MSE 4.219\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.506, Test MSE 4.191\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.504, Test MSE 4.166\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.114, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.503, Test MSE 4.144\n",
      "TRAINING MODEL dt_norm_2_37\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 48784\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.306, Test 0.240\n",
      "MSE NON-NORMALIZED: Train MSE 6.337, Test MSE 5.670\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.105, Test 0.126\n",
      "MSE NON-NORMALIZED: Train MSE 2.245, Test MSE 2.235\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.099, Test 0.126\n",
      "MSE NON-NORMALIZED: Train MSE 2.238, Test MSE 2.228\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.103, Test 0.132\n",
      "MSE NON-NORMALIZED: Train MSE 2.489, Test MSE 3.714\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.104, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 2.623, Test MSE 4.603\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.104, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 2.622, Test MSE 4.538\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.104, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 2.620, Test MSE 4.499\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.104, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 2.619, Test MSE 4.464\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.104, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 2.618, Test MSE 4.433\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.104, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 2.616, Test MSE 4.396\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.103, Test 0.136\n",
      "MSE NON-NORMALIZED: Train MSE 2.613, Test MSE 4.368\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.103, Test 0.136\n",
      "MSE NON-NORMALIZED: Train MSE 2.612, Test MSE 4.341\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.103, Test 0.136\n",
      "MSE NON-NORMALIZED: Train MSE 2.610, Test MSE 4.318\n",
      "TRAINING MODEL dt_norm_2_38\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 13487\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.222, Test 0.182\n",
      "MSE NON-NORMALIZED: Train MSE 4.008, Test MSE 3.870\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.088, Test 0.086\n",
      "MSE NON-NORMALIZED: Train MSE 1.468, Test MSE 1.451\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.066, Test 0.069\n",
      "MSE NON-NORMALIZED: Train MSE 1.111, Test MSE 1.103\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.138, Test MSE 1.487\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.173, Test MSE 1.641\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.164, Test MSE 1.598\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.158, Test MSE 1.572\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.152, Test MSE 1.551\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.148, Test MSE 1.533\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.143, Test MSE 1.517\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.139, Test MSE 1.502\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.062, Test 0.065\n",
      "MSE NON-NORMALIZED: Train MSE 1.135, Test MSE 1.488\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.062, Test 0.064\n",
      "MSE NON-NORMALIZED: Train MSE 1.131, Test MSE 1.477\n",
      "TRAINING MODEL dt_norm_2_39\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 8335\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.275, Test 0.205\n",
      "MSE NON-NORMALIZED: Train MSE 3.727, Test MSE 3.633\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.162, Test 0.122\n",
      "MSE NON-NORMALIZED: Train MSE 2.044, Test MSE 2.016\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.121, Test 0.090\n",
      "MSE NON-NORMALIZED: Train MSE 1.503, Test MSE 1.481\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.094, Test 0.072\n",
      "MSE NON-NORMALIZED: Train MSE 1.277, Test MSE 1.499\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.069, Test 0.056\n",
      "MSE NON-NORMALIZED: Train MSE 0.953, Test MSE 1.081\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.058, Test 0.049\n",
      "MSE NON-NORMALIZED: Train MSE 0.802, Test MSE 0.836\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.053, Test 0.045\n",
      "MSE NON-NORMALIZED: Train MSE 0.727, Test MSE 0.712\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.049, Test 0.042\n",
      "MSE NON-NORMALIZED: Train MSE 0.686, Test MSE 0.651\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.046, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.647, Test MSE 0.605\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.044, Test 0.039\n",
      "MSE NON-NORMALIZED: Train MSE 0.634, Test MSE 0.589\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.043, Test 0.038\n",
      "MSE NON-NORMALIZED: Train MSE 0.630, Test MSE 0.581\n",
      "Epoch 275, lr 4.5e-07\n",
      "Epoch 275: Train 0.043, Test 0.038\n",
      "MSE NON-NORMALIZED: Train MSE 0.632, Test MSE 0.579\n",
      "Epoch 300, lr 4.5e-07\n",
      "Epoch 300: Train 0.043, Test 0.038\n",
      "MSE NON-NORMALIZED: Train MSE 0.636, Test MSE 0.579\n",
      "TRAINING MODEL dt_norm_2_40\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 11106\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.078, Test 0.095\n",
      "MSE NON-NORMALIZED: Train MSE 1.597, Test MSE 1.543\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.040, Test 0.044\n",
      "MSE NON-NORMALIZED: Train MSE 0.688, Test MSE 0.681\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.034, Test 0.035\n",
      "MSE NON-NORMALIZED: Train MSE 0.539, Test MSE 0.534\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.576, Test MSE 0.833\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.615, Test MSE 1.003\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.609, Test MSE 0.972\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.605, Test MSE 0.952\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.600, Test MSE 0.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.596, Test MSE 0.917\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.593, Test MSE 0.902\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.589, Test MSE 0.888\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.586, Test MSE 0.875\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.032, Test 0.033\n",
      "MSE NON-NORMALIZED: Train MSE 0.582, Test MSE 0.862\n",
      "TRAINING MODEL dt_norm_2_41\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 12696\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.095, Test 0.087\n",
      "MSE NON-NORMALIZED: Train MSE 1.440, Test MSE 1.392\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.048, Test 0.049\n",
      "MSE NON-NORMALIZED: Train MSE 0.739, Test MSE 0.729\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.040, Test 0.042\n",
      "MSE NON-NORMALIZED: Train MSE 0.616, Test MSE 0.609\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.039, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.644, Test MSE 0.826\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.039, Test 0.041\n",
      "MSE NON-NORMALIZED: Train MSE 0.676, Test MSE 0.973\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.039, Test 0.041\n",
      "MSE NON-NORMALIZED: Train MSE 0.673, Test MSE 0.957\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.039, Test 0.041\n",
      "MSE NON-NORMALIZED: Train MSE 0.670, Test MSE 0.947\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.039, Test 0.041\n",
      "MSE NON-NORMALIZED: Train MSE 0.668, Test MSE 0.938\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.039, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.666, Test MSE 0.930\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.039, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.664, Test MSE 0.922\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.039, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.663, Test MSE 0.914\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.039, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.661, Test MSE 0.907\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.039, Test 0.040\n",
      "MSE NON-NORMALIZED: Train MSE 0.659, Test MSE 0.900\n",
      "TRAINING MODEL dt_norm_2_42\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 5153\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.067, Test 0.071\n",
      "MSE NON-NORMALIZED: Train MSE 1.028, Test MSE 1.018\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.046, Test 0.050\n",
      "MSE NON-NORMALIZED: Train MSE 0.705, Test MSE 0.702\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.038, Test 0.042\n",
      "MSE NON-NORMALIZED: Train MSE 0.592, Test MSE 0.590\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.035, Test 0.038\n",
      "MSE NON-NORMALIZED: Train MSE 0.575, Test MSE 0.654\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.033, Test 0.036\n",
      "MSE NON-NORMALIZED: Train MSE 0.529, Test MSE 0.584\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.031, Test 0.034\n",
      "MSE NON-NORMALIZED: Train MSE 0.486, Test MSE 0.513\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.030, Test 0.032\n",
      "MSE NON-NORMALIZED: Train MSE 0.457, Test MSE 0.471\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.029, Test 0.031\n",
      "MSE NON-NORMALIZED: Train MSE 0.438, Test MSE 0.440\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.028, Test 0.031\n",
      "MSE NON-NORMALIZED: Train MSE 0.428, Test MSE 0.425\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.028, Test 0.030\n",
      "MSE NON-NORMALIZED: Train MSE 0.423, Test MSE 0.419\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.028, Test 0.030\n",
      "MSE NON-NORMALIZED: Train MSE 0.421, Test MSE 0.417\n",
      "Epoch 275, lr 2.25e-07\n",
      "Epoch 275: Train 0.028, Test 0.030\n",
      "MSE NON-NORMALIZED: Train MSE 0.421, Test MSE 0.416\n",
      "Epoch 300, lr 5.625e-08\n",
      "Epoch 300: Train 0.028, Test 0.030\n",
      "MSE NON-NORMALIZED: Train MSE 0.421, Test MSE 0.416\n",
      "TRAINING MODEL dt_norm_2_43\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 3968\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.086, Test 0.072\n",
      "MSE NON-NORMALIZED: Train MSE 1.071, Test MSE 1.049\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.056, Test 0.047\n",
      "MSE NON-NORMALIZED: Train MSE 0.667, Test MSE 0.657\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.045, Test 0.037\n",
      "MSE NON-NORMALIZED: Train MSE 0.521, Test MSE 0.514\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.039, Test 0.032\n",
      "MSE NON-NORMALIZED: Train MSE 0.475, Test MSE 0.517\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.035, Test 0.029\n",
      "MSE NON-NORMALIZED: Train MSE 0.427, Test MSE 0.452\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.032, Test 0.027\n",
      "MSE NON-NORMALIZED: Train MSE 0.384, Test MSE 0.374\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.031, Test 0.025\n",
      "MSE NON-NORMALIZED: Train MSE 0.363, Test MSE 0.352\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.029, Test 0.024\n",
      "MSE NON-NORMALIZED: Train MSE 0.348, Test MSE 0.337\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.028, Test 0.024\n",
      "MSE NON-NORMALIZED: Train MSE 0.337, Test MSE 0.326\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.028, Test 0.023\n",
      "MSE NON-NORMALIZED: Train MSE 0.328, Test MSE 0.316\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.026, Test 0.022\n",
      "MSE NON-NORMALIZED: Train MSE 0.310, Test MSE 0.298\n",
      "Epoch 275, lr 4.5e-07\n",
      "Epoch 275: Train 0.025, Test 0.021\n",
      "MSE NON-NORMALIZED: Train MSE 0.300, Test MSE 0.288\n",
      "Epoch 300, lr 4.5e-07\n",
      "Epoch 300: Train 0.025, Test 0.021\n",
      "MSE NON-NORMALIZED: Train MSE 0.295, Test MSE 0.282\n",
      "TRAINING MODEL dt_norm_2_44\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 2380\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.050, Test 0.047\n",
      "MSE NON-NORMALIZED: Train MSE 0.663, Test MSE 0.656\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.036, Test 0.035\n",
      "MSE NON-NORMALIZED: Train MSE 0.494, Test MSE 0.491\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.026, Test 0.026\n",
      "MSE NON-NORMALIZED: Train MSE 0.371, Test MSE 0.368\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.021, Test 0.022\n",
      "MSE NON-NORMALIZED: Train MSE 0.308, Test MSE 0.307\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.019, Test 0.019\n",
      "MSE NON-NORMALIZED: Train MSE 0.275, Test MSE 0.272\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.017, Test 0.018\n",
      "MSE NON-NORMALIZED: Train MSE 0.256, Test MSE 0.252\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.016, Test 0.016\n",
      "MSE NON-NORMALIZED: Train MSE 0.235, Test MSE 0.232\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.014, Test 0.015\n",
      "MSE NON-NORMALIZED: Train MSE 0.216, Test MSE 0.212\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.013, Test 0.014\n",
      "MSE NON-NORMALIZED: Train MSE 0.201, Test MSE 0.197\n",
      "Epoch 225, lr 4.5e-07\n",
      "Epoch 225: Train 0.013, Test 0.013\n",
      "MSE NON-NORMALIZED: Train MSE 0.190, Test MSE 0.186\n",
      "Epoch 250, lr 4.5e-07\n",
      "Epoch 250: Train 0.012, Test 0.013\n",
      "MSE NON-NORMALIZED: Train MSE 0.181, Test MSE 0.177\n",
      "Epoch 275, lr 4.5e-07\n",
      "Epoch 275: Train 0.012, Test 0.012\n",
      "MSE NON-NORMALIZED: Train MSE 0.173, Test MSE 0.169\n",
      "Epoch 300, lr 4.5e-07\n",
      "Epoch 300: Train 0.011, Test 0.012\n",
      "MSE NON-NORMALIZED: Train MSE 0.167, Test MSE 0.163\n",
      "TRAINING MODEL dt_norm_2_45\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 3969\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.089, Test 0.063\n",
      "MSE NON-NORMALIZED: Train MSE 1.072, Test MSE 1.026\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.034, Test 0.035\n",
      "MSE NON-NORMALIZED: Train MSE 0.512, Test MSE 0.505\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.023, Test 0.028\n",
      "MSE NON-NORMALIZED: Train MSE 0.391, Test MSE 0.387\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.019, Test 0.024\n",
      "MSE NON-NORMALIZED: Train MSE 0.364, Test MSE 0.415\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.017, Test 0.022\n",
      "MSE NON-NORMALIZED: Train MSE 0.330, Test MSE 0.358\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.015, Test 0.021\n",
      "MSE NON-NORMALIZED: Train MSE 0.297, Test MSE 0.302\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.015, Test 0.020\n",
      "MSE NON-NORMALIZED: Train MSE 0.280, Test MSE 0.278\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.014, Test 0.019\n",
      "MSE NON-NORMALIZED: Train MSE 0.269, Test MSE 0.262\n",
      "Epoch 200, lr 4.5e-07\n",
      "Epoch 200: Train 0.014, Test 0.018\n",
      "MSE NON-NORMALIZED: Train MSE 0.262, Test MSE 0.253\n",
      "Epoch 225, lr 1.125e-07\n",
      "Epoch 225: Train 0.014, Test 0.018\n",
      "MSE NON-NORMALIZED: Train MSE 0.261, Test MSE 0.251\n",
      "Epoch 250, lr 2.8125e-08\n",
      "Epoch 250: Train 0.014, Test 0.018\n",
      "MSE NON-NORMALIZED: Train MSE 0.260, Test MSE 0.250\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.014, Test 0.018\n",
      "MSE NON-NORMALIZED: Train MSE 0.260, Test MSE 0.250\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.014, Test 0.018\n",
      "MSE NON-NORMALIZED: Train MSE 0.260, Test MSE 0.250\n",
      "TRAINING MODEL dt_norm_3_39\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 28168\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.560, Test 0.641\n",
      "MSE NON-NORMALIZED: Train MSE 12.348, Test MSE 12.058\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.285, Test 0.356\n",
      "MSE NON-NORMALIZED: Train MSE 6.428, Test MSE 6.342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.222, Test 0.255\n",
      "MSE NON-NORMALIZED: Train MSE 4.468, Test MSE 4.404\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.191, Test 0.207\n",
      "MSE NON-NORMALIZED: Train MSE 3.820, Test MSE 5.426\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.175, Test 0.187\n",
      "MSE NON-NORMALIZED: Train MSE 3.538, Test MSE 5.583\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.167, Test 0.178\n",
      "MSE NON-NORMALIZED: Train MSE 3.321, Test MSE 4.994\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.163, Test 0.173\n",
      "MSE NON-NORMALIZED: Train MSE 3.221, Test MSE 4.750\n",
      "Epoch 175, lr 4.5e-07\n",
      "Epoch 175: Train 0.162, Test 0.170\n",
      "MSE NON-NORMALIZED: Train MSE 3.169, Test MSE 4.610\n",
      "Epoch 200, lr 2.25e-07\n",
      "Epoch 200: Train 0.162, Test 0.169\n",
      "MSE NON-NORMALIZED: Train MSE 3.142, Test MSE 4.510\n",
      "Epoch 225, lr 5.625e-08\n",
      "Epoch 225: Train 0.163, Test 0.169\n",
      "MSE NON-NORMALIZED: Train MSE 3.139, Test MSE 4.497\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.163, Test 0.169\n",
      "MSE NON-NORMALIZED: Train MSE 3.138, Test MSE 4.494\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.163, Test 0.169\n",
      "MSE NON-NORMALIZED: Train MSE 3.136, Test MSE 4.490\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.163, Test 0.169\n",
      "MSE NON-NORMALIZED: Train MSE 3.135, Test MSE 4.488\n",
      "TRAINING MODEL dt_norm_3_40\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 30947\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.190, Test 0.357\n",
      "MSE NON-NORMALIZED: Train MSE 6.435, Test MSE 6.200\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.124, Test 0.203\n",
      "MSE NON-NORMALIZED: Train MSE 3.136, Test MSE 3.103\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.105, Test 0.169\n",
      "MSE NON-NORMALIZED: Train MSE 2.591, Test MSE 2.565\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.100, Test 0.162\n",
      "MSE NON-NORMALIZED: Train MSE 2.685, Test MSE 4.075\n",
      "Epoch 100, lr 1.125e-07\n",
      "Epoch 100: Train 0.100, Test 0.163\n",
      "MSE NON-NORMALIZED: Train MSE 2.790, Test MSE 4.776\n",
      "Epoch 125, lr 2.8125e-08\n",
      "Epoch 125: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.778, Test MSE 4.600\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.774, Test MSE 4.550\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.770, Test MSE 4.510\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.767, Test MSE 4.474\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.764, Test MSE 4.442\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.762, Test MSE 4.412\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.100, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.759, Test MSE 4.386\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.099, Test 0.164\n",
      "MSE NON-NORMALIZED: Train MSE 2.757, Test MSE 4.362\n",
      "TRAINING MODEL dt_norm_3_41\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26195\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.214, Test 0.246\n",
      "MSE NON-NORMALIZED: Train MSE 4.091, Test MSE 3.992\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.134, Test 0.136\n",
      "MSE NON-NORMALIZED: Train MSE 2.043, Test MSE 2.026\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.112, Test 0.111\n",
      "MSE NON-NORMALIZED: Train MSE 1.631, Test MSE 1.621\n",
      "Epoch 75, lr 1.125e-07\n",
      "Epoch 75: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.718, Test MSE 2.723\n",
      "Epoch 100, lr 2.8125e-08\n",
      "Epoch 100: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.845, Test MSE 3.626\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.836, Test MSE 3.551\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.828, Test MSE 3.495\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.822, Test MSE 3.445\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.816, Test MSE 3.398\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.810, Test MSE 3.354\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.804, Test MSE 3.312\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.799, Test MSE 3.275\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.108, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.795, Test MSE 3.240\n",
      "TRAINING MODEL dt_norm_3_42\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 27786\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.199, Test 0.215\n",
      "MSE NON-NORMALIZED: Train MSE 3.297, Test MSE 3.200\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.118, Test 0.124\n",
      "MSE NON-NORMALIZED: Train MSE 1.821, Test MSE 1.796\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.094, Test 0.098\n",
      "MSE NON-NORMALIZED: Train MSE 1.435, Test MSE 1.414\n",
      "Epoch 75, lr 2.25e-07\n",
      "Epoch 75: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.436, Test MSE 1.956\n",
      "Epoch 100, lr 5.625e-08\n",
      "Epoch 100: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.492, Test MSE 2.352\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.489, Test MSE 2.312\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.487, Test MSE 2.297\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.485, Test MSE 2.284\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.484, Test MSE 2.272\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.483, Test MSE 2.262\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.482, Test MSE 2.252\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.480, Test MSE 2.242\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.088, Test 0.092\n",
      "MSE NON-NORMALIZED: Train MSE 1.480, Test MSE 2.233\n",
      "TRAINING MODEL dt_norm_3_43\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 23805\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.188, Test 0.258\n",
      "MSE NON-NORMALIZED: Train MSE 4.337, Test MSE 4.236\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.106, Test 0.149\n",
      "MSE NON-NORMALIZED: Train MSE 2.435, Test MSE 2.400\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.083, Test 0.120\n",
      "MSE NON-NORMALIZED: Train MSE 1.983, Test MSE 1.952\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.076, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.011, Test MSE 2.783\n",
      "Epoch 100, lr 1.125e-07\n",
      "Epoch 100: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.086, Test MSE 3.262\n",
      "Epoch 125, lr 1.40625e-08\n",
      "Epoch 125: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.077, Test MSE 3.174\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.074, Test MSE 3.154\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.072, Test MSE 3.136\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.070, Test MSE 3.120\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.067, Test MSE 3.105\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.066, Test MSE 3.091\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.064, Test MSE 3.079\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.075, Test 0.112\n",
      "MSE NON-NORMALIZED: Train MSE 2.062, Test MSE 3.067\n",
      "TRAINING MODEL dt_norm_3_44\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 24190\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.257, Test 0.227\n",
      "MSE NON-NORMALIZED: Train MSE 3.532, Test MSE 3.469\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.144, Test 0.141\n",
      "MSE NON-NORMALIZED: Train MSE 2.105, Test MSE 2.088\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.106, Test 0.111\n",
      "MSE NON-NORMALIZED: Train MSE 1.623, Test MSE 1.616\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.095, Test 0.101\n",
      "MSE NON-NORMALIZED: Train MSE 1.572, Test MSE 2.115\n",
      "Epoch 100, lr 2.25e-07\n",
      "Epoch 100: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.603, Test MSE 2.412\n",
      "Epoch 125, lr 2.8125e-08\n",
      "Epoch 125: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.599, Test MSE 2.363\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.599, Test MSE 2.356\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.598, Test MSE 2.351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.597, Test MSE 2.346\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.597, Test MSE 2.342\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.596, Test MSE 2.338\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.596, Test MSE 2.334\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.094, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.595, Test MSE 2.331\n",
      "TRAINING MODEL dt_norm_3_45\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 18646\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.215, Test 0.233\n",
      "MSE NON-NORMALIZED: Train MSE 3.999, Test MSE 3.887\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.116, Test 0.121\n",
      "MSE NON-NORMALIZED: Train MSE 1.885, Test MSE 1.851\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.086, Test 0.091\n",
      "MSE NON-NORMALIZED: Train MSE 1.392, Test MSE 1.367\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.074, Test 0.079\n",
      "MSE NON-NORMALIZED: Train MSE 1.339, Test MSE 1.906\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.306, Test MSE 1.943\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.068, Test 0.074\n",
      "MSE NON-NORMALIZED: Train MSE 1.273, Test MSE 1.796\n",
      "Epoch 150, lr 1.125e-07\n",
      "Epoch 150: Train 0.069, Test 0.074\n",
      "MSE NON-NORMALIZED: Train MSE 1.275, Test MSE 1.780\n",
      "Epoch 175, lr 2.8125e-08\n",
      "Epoch 175: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.275, Test MSE 1.764\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.274, Test MSE 1.755\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.273, Test MSE 1.750\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.273, Test MSE 1.747\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.272, Test MSE 1.745\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.069, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 1.272, Test MSE 1.743\n",
      "TRAINING MODEL dt_norm_3_46\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 19839\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.309, Test 0.276\n",
      "MSE NON-NORMALIZED: Train MSE 4.478, Test MSE 4.377\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.187, Test 0.167\n",
      "MSE NON-NORMALIZED: Train MSE 2.559, Test MSE 2.532\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.139, Test 0.123\n",
      "MSE NON-NORMALIZED: Train MSE 1.868, Test MSE 1.851\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.117, Test 0.103\n",
      "MSE NON-NORMALIZED: Train MSE 1.659, Test MSE 2.115\n",
      "Epoch 100, lr 4.5e-07\n",
      "Epoch 100: Train 0.109, Test 0.097\n",
      "MSE NON-NORMALIZED: Train MSE 1.585, Test MSE 2.117\n",
      "Epoch 125, lr 4.5e-07\n",
      "Epoch 125: Train 0.107, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.519, Test MSE 1.911\n",
      "Epoch 150, lr 4.5e-07\n",
      "Epoch 150: Train 0.105, Test 0.093\n",
      "MSE NON-NORMALIZED: Train MSE 1.492, Test MSE 1.815\n",
      "Epoch 175, lr 2.25e-07\n",
      "Epoch 175: Train 0.105, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.495, Test MSE 1.786\n",
      "Epoch 200, lr 5.625e-08\n",
      "Epoch 200: Train 0.106, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.501, Test MSE 1.788\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.106, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.503, Test MSE 1.789\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.106, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.503, Test MSE 1.788\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.106, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.503, Test MSE 1.788\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.106, Test 0.094\n",
      "MSE NON-NORMALIZED: Train MSE 1.504, Test MSE 1.788\n",
      "TRAINING MODEL dt_norm_3_47\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 17061\n",
      "Epoch 0, lr 4.5e-07\n",
      "Epoch 0: Train 0.257, Test 0.272\n",
      "MSE NON-NORMALIZED: Train MSE 5.225, Test MSE 5.035\n",
      "Epoch 25, lr 4.5e-07\n",
      "Epoch 25: Train 0.115, Test 0.140\n",
      "MSE NON-NORMALIZED: Train MSE 2.085, Test MSE 2.063\n",
      "Epoch 50, lr 4.5e-07\n",
      "Epoch 50: Train 0.090, Test 0.107\n",
      "MSE NON-NORMALIZED: Train MSE 1.549, Test MSE 1.535\n",
      "Epoch 75, lr 4.5e-07\n",
      "Epoch 75: Train 0.081, Test 0.091\n",
      "MSE NON-NORMALIZED: Train MSE 1.396, Test MSE 1.760\n",
      "Epoch 100, lr 2.25e-07\n",
      "Epoch 100: Train 0.078, Test 0.086\n",
      "MSE NON-NORMALIZED: Train MSE 1.364, Test MSE 1.937\n",
      "Epoch 125, lr 2.8125e-08\n",
      "Epoch 125: Train 0.078, Test 0.085\n",
      "MSE NON-NORMALIZED: Train MSE 1.346, Test MSE 1.891\n",
      "Epoch 150, lr 1.40625e-08\n",
      "Epoch 150: Train 0.078, Test 0.085\n",
      "MSE NON-NORMALIZED: Train MSE 1.342, Test MSE 1.881\n",
      "Epoch 175, lr 1.40625e-08\n",
      "Epoch 175: Train 0.078, Test 0.085\n",
      "MSE NON-NORMALIZED: Train MSE 1.339, Test MSE 1.874\n",
      "Epoch 200, lr 1.40625e-08\n",
      "Epoch 200: Train 0.078, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.336, Test MSE 1.867\n",
      "Epoch 225, lr 1.40625e-08\n",
      "Epoch 225: Train 0.078, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.333, Test MSE 1.861\n",
      "Epoch 250, lr 1.40625e-08\n",
      "Epoch 250: Train 0.078, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.331, Test MSE 1.855\n",
      "Epoch 275, lr 1.40625e-08\n",
      "Epoch 275: Train 0.078, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.329, Test MSE 1.849\n",
      "Epoch 300, lr 1.40625e-08\n",
      "Epoch 300: Train 0.077, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.326, Test MSE 1.843\n"
     ]
    }
   ],
   "source": [
    "ae_train.train_models_telescope(data_smol_high,\n",
    "             override = False,\n",
    "             model_params = [],\n",
    "             path_1 = 'models/dt_1_greater_0_450_250_100_dif_2',\n",
    "             path_2 = 'models/dt_2_greater_0_450_250_100_dif_2',\n",
    "             path_3 = 'models/dt_3_greater_0_450_250_100_dif_2',\n",
    "             epochs = 301,\n",
    "             append_ReLU = True, \n",
    "             lr = 4.5e-7,\n",
    "             max_dt_size = 50000,\n",
    "             dir_label = 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero_greater_7_tele_only_high_longer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "96f8971d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ae_train.retrain_models(data_smol,\n",
    "             stats_per,\n",
    "             override = False,\n",
    "             mse_threshold = 0,\n",
    "             model_params = 'retrain',\n",
    "             epochs = 100, \n",
    "             lr = 8.5e-7,\n",
    "             max_dt_size = 30000,\n",
    "             dir_label = 'cond_AE_wafer_layer_split_mip_std_1_mean_nonzero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e1f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
