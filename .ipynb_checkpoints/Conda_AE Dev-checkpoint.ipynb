{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb462a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from Naive_DAE import Naive_DAE,Dropout_DAE\n",
    "import AE_Stats\n",
    "from load_data_fn import load_data,load_data_no_filter\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import importlib\n",
    "from telescope_torch import *\n",
    "import time\n",
    "from losses import new_loss,AE_MSE, combo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab27cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dt, size_train, size_test):\n",
    "    \n",
    "    test_loc = dt[-size_test:]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "    test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "    train_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_loc)),\n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_loc)),\n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train = dt[0:size_train,0:48]\n",
    "    test = dt[-size_test:,0:48]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train=train[torch.randperm(train.size()[0])]\n",
    "    test=test[torch.randperm(test.size()[0])]\n",
    "    train_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train)),\n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test)),\n",
    "        batch_size=100,\n",
    "        shuffle=False\n",
    "    )\n",
    "    #fine-tune autoencoder\n",
    "    #batch 500\n",
    "    test = dt[-size_test:,0:48]\n",
    "    optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "    test = dt[-size_test:,0:48]\n",
    "\n",
    "    AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19c1e203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to try non-normalized with this thing to see if its better\n",
    "def train_models_prime(data,batch = 100, model_params = [],max_dt_size = 40000,epochs = 100,lr = 0.001, dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()), path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        params = []\n",
    "        if model_params == []:\n",
    "            if int(d[1][3]) == 1:\n",
    "                params.append(torch.load('models/dt_1_greater_0_450_250_100_dif_2'))\n",
    "                \n",
    "            elif int(d[1][3]) == 2:\n",
    "                params.append(torch.load('models/dt_2_greater_0_450_250_100_dif_2'))\n",
    "            else:\n",
    "                params.append(torch.load('models/dt_3_greater_0_450_250_100_dif_2'))\n",
    "        else:\n",
    "            print(f'created model {d[1]} with {model_params}')\n",
    "            params.append(Naive_DAE(model_params))\n",
    "        params.append(dt)\n",
    "        \n",
    "        #Limiting size of training/testing to limit runtime\n",
    "        if len(dt) <= max_dt_size:\n",
    "            params.append(int(0.8*len(dt)))\n",
    "            params.append(int(0.19*len(dt)))\n",
    "        else:\n",
    "            params.append(int(0.8*max_dt_size))\n",
    "            params.append(int(0.19*max_dt_size))\n",
    "        \n",
    "        params.append(d[1])\n",
    "        params.append(dir_label)\n",
    "        train_info.append(params)\n",
    "\n",
    "    for params in train_info:\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        #Training model\n",
    "        train_model(params[0].to(device),params[1].to(device),params[2],params[3],params[4],params[5],path,batch = batch, num_epochs = epochs, lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d472d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_models(data,mse,mse_threshold = 5, batch = 100,override = True, model_params = [],max_dt_size = 40000,epochs = 100,lr = 0.001, dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()), path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    mse_to_retrain = []\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        cur_mse = mse[i]\n",
    "        i = i + 1\n",
    "        params = []\n",
    "        if cur_mse > mse_threshold:\n",
    "            mse_to_retrain.append(cur_mse)\n",
    "            print(f'Model {d[1]} has MSE of {cur_mse} and is being retrained')\n",
    "            if params == 'retrain':\n",
    "                params.append(torch.load(os.path.join(path,d[1])))\n",
    "            elif params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    mt = torch.load('models/dt_1_greater_0_450_250_100_dif_2')\n",
    "                    mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load('models/dt_2_greater_0_450_250_100_dif_2')\n",
    "                    mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load('models/dt_3_greater_0_450_250_100_dif_2')\n",
    "                    mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                print(f'created model {d[1]} with {model_params}')\n",
    "                params.append(Naive_DAE(model_params))\n",
    "               \n",
    "\n",
    "            params.append(dt)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "            \n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        cur_mse = mse_to_retrain[i]\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        i=i+1\n",
    "        #Training mode\n",
    "        retrain_model(params[0].to(device),\n",
    "                      cur_mse,\n",
    "                      params[1].to(device),\n",
    "                      params[2],\n",
    "                      params[3],\n",
    "                      params[4],\n",
    "                      params[5],\n",
    "                      path,batch = batch,\n",
    "                      num_epochs = epochs,\n",
    "                      lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7b4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_models_specified(data,\n",
    "                             mse,\n",
    "                             specified,\n",
    "                             loss = new_loss,\n",
    "                             batch = 100,\n",
    "                             override = True,\n",
    "                             model_params = [],\n",
    "                             append_ReLU = False,\n",
    "                             max_dt_size = 40000,\n",
    "                             epochs = 100,\n",
    "                             path_1 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                             path_2 = 'models/MIP_dt_1_450_250_greater_2',\n",
    "                             path_3 = 'models/MIP_dt_1_450_250_greater_3',\n",
    "                             lr = 0.001,\n",
    "                             dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()),\n",
    "                             path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    \n",
    "    mse_to_retrain = []\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        cur_mse = mse[i]\n",
    "        i = i + 1\n",
    "        params = []\n",
    "        if d[1] in specified:\n",
    "            mse_to_retrain.append(cur_mse)\n",
    "            print(f'Model {d[1]} is being retrained')\n",
    "            if model_params == 'retrain':\n",
    "                params.append(torch.load(os.path.join(path,d[1])))\n",
    "            elif model_params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    mt = torch.load(path_1)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                    \n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load(path_2)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load(path_3)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                print(f'created model {d[1]} with {model_params}')\n",
    "                params.append(Naive_DAE(model_params))\n",
    "               \n",
    "\n",
    "            params.append(dt)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            \n",
    "            train_info.append(params)\n",
    "            \n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        cur_mse = mse_to_retrain[i]\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        i=i+1\n",
    "        #Training mode\n",
    "        retrain_model(params[0].to(device),\n",
    "                      cur_mse,\n",
    "                      params[1].to(device),\n",
    "                      params[2],\n",
    "                      params[3],\n",
    "                      params[4],\n",
    "                      params[5],\n",
    "                      path,batch = batch,\n",
    "                      num_epochs = epochs,\n",
    "                      loss = loss,\n",
    "                      lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2340392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(model, mse,dt, size_train, size_test,label,cur_directory,path, loss = new_loss, num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    print(f'TRAINING INFO:')\n",
    "    print(f'Total Dataset Size: {size_train + size_test}')\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    train_loc = dt[0:size_train]\n",
    "    test_loc = dt[-size_test:]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "    test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "    train_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train = dt[0:size_train,0:48]\n",
    "    test = dt[-size_test:,0:48]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train=train[torch.randperm(train.size()[0])]\n",
    "    test=test[torch.randperm(test.size()[0])]\n",
    "    train_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "    #fine-tune autoencoder\n",
    "    #batch 500\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "    \n",
    "    all_test_losses = []\n",
    "    all_train_losses = []\n",
    "    # train\n",
    "    running_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for i, data_list in enumerate(train_d1_flat):\n",
    "            \n",
    "            model.train()\n",
    "            data = data_list[0]\n",
    "            v_pred = model(data)\n",
    "\n",
    "            \n",
    "            batch_loss = torch.mean(telescopeMSE2(data[:,0:48], v_pred))\n",
    "            all_train_losses.append(batch_loss.item())\n",
    "            losses.append(batch_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        scheduler.step(batch_loss)\n",
    "        \n",
    "        data_test = test\n",
    "        model.eval()\n",
    "        test_pred = model(data_test)\n",
    "        batch_test = torch.mean(telescopeMSE2(data_test[:,0:48], test_pred))\n",
    "        all_test_losses.append(batch_test.item())\n",
    "        running_loss = np.mean(losses)\n",
    "        runningtest_loss = batch_test.item()\n",
    "        if epoch % 25 == 0:\n",
    "            print('Epoch {}, lr {}'.format(\n",
    "                epoch, optimizer.param_groups[0]['lr']))\n",
    "            print(f\"Epoch {epoch}: Train {AE_MSE(v_pred, data):.3f}, Test {AE_MSE(test_pred, data_test):.3f}\")\n",
    "            print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "#     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "    if AE_MSE(test_pred, data_test)*3.5280**2 < mse:\n",
    "        torch.save(model,f'{path}/{label}')\n",
    "    else:\n",
    "        print(f'MSE of {AE_MSE(test_pred, data_test)*3.5280**2} was larger than initial of {mse} and was not saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c580e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b292bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide path to data\n",
    "data_path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/MIT_TTbar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dec7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "782999df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(data_path)\n",
    "dt_files = []\n",
    "for f in all_files:\n",
    "    if f[0:7] == 'dt_norm' and (f[-3:] != 'low' and f[-4:] !='high'):\n",
    "        dt_files.append(f)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ff9fb02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_3_36: 1/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26476\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AE_MSE() got an unexpected keyword argument 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_condAEs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdt_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcombo_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/combo_loss_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/combo_loss_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath_3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/combo_loss_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdir_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcondAE_mean_split_450_250_combo_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtele\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8.5e-8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 106\u001b[0m, in \u001b[0;36mtrain_condAEs\u001b[0;34m(data_path, dt_files, batch, override, model_params, loss, tele, path_1, path_2, path_3, append_ReLU, max_dt_size, epochs, lr, dir_label, path)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtrain_condAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_low\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHIGH\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[1;32m    108\u001b[0m     train_condAE(params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mhigh_train\u001b[38;5;241m.\u001b[39mto(device),params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mhigh_test\u001b[38;5;241m.\u001b[39mto(device),params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_high\u001b[39m\u001b[38;5;124m'\u001b[39m,os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname),path, batch \u001b[38;5;241m=\u001b[39m batch, num_epochs \u001b[38;5;241m=\u001b[39m epochs, lr \u001b[38;5;241m=\u001b[39m lr)\n",
      "Cell \u001b[0;32mIn [10], line 65\u001b[0m, in \u001b[0;36mtrain_condAE\u001b[0;34m(model, dt_train, dt_test, label, cur_directory, path, loss, num_epochs, lr, batch)\u001b[0m\n\u001b[1;32m     63\u001b[0m data \u001b[38;5;241m=\u001b[39m data_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m v_pred \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m---> 65\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m all_train_losses\u001b[38;5;241m.\u001b[39mappend(batch_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     67\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(batch_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/losses.py:46\u001b[0m, in \u001b[0;36mnew_loss\u001b[0;34m(data, pred, epoch, mean, std, alpha, cut)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_loss\u001b[39m(data,pred,epoch,mean,std,alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,cut \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#Reconstruction\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[43mAE_MSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m cut:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m error\n",
      "\u001b[0;31mTypeError\u001b[0m: AE_MSE() got an unexpected keyword argument 'alpha'"
     ]
    }
   ],
   "source": [
    "train_condAEs(data_path,\n",
    "              dt_files,\n",
    "               model_params = [],\n",
    "              override = True,\n",
    "              loss = combo_loss, \n",
    "              path_1 = 'models/combo_loss_all',\n",
    "              path_2 = 'models/combo_loss_all',\n",
    "              path_3 = 'models/combo_loss_all',\n",
    "             dir_label ='condAE_mean_split_450_250_combo_loss',\n",
    "             epochs= 300,\n",
    "             tele = False,\n",
    "             batch = 100, \n",
    "             lr = 8.5e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcbd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_condAE_tele(model, dt_train, dt_test,label,cur_directory,path, loss = new_loss,num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    size_train = len(dt_train)\n",
    "    size_test = len(dt_test)\n",
    "    if size_train !=0 and size_test != 0:\n",
    "\n",
    "        print(f'TRAINING INFO:')\n",
    "        print(f'Total Dataset Size: {size_train + size_test}')\n",
    "        mean = 0\n",
    "        std = 1\n",
    "\n",
    "        train_loc =dt_train\n",
    "        test_loc = dt_test\n",
    "\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "        test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "        train_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        train = dt_train[:,0:48]\n",
    "        test = dt_test[:,0:48]\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train=train[torch.randperm(train.size()[0])]\n",
    "        test=test[torch.randperm(test.size()[0])]\n",
    "        train_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "        #fine-tune autoencoder\n",
    "        #batch 500\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "\n",
    "        all_test_losses = []\n",
    "        all_train_losses = []\n",
    "        # train\n",
    "        running_loss = float(\"inf\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i, data_list in enumerate(train_d1_flat):\n",
    "\n",
    "                model.train()\n",
    "                data = data_list[0]\n",
    "                v_pred = model(data)\n",
    "                batch_loss = torch.mean(telescopeMSE2(data[:,0:48], v_pred))\n",
    "                all_train_losses.append(batch_loss.item())\n",
    "                losses.append(batch_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            scheduler.step(batch_loss)\n",
    "\n",
    "            data_test = test\n",
    "            model.eval()\n",
    "            test_pred = model(data_test)\n",
    "            batch_test = torch.mean(telescopeMSE2(data_test[:,0:48], test_pred))\n",
    "            all_test_losses.append(batch_test.item())\n",
    "            running_loss = np.mean(losses)\n",
    "            runningtest_loss = batch_test.item()\n",
    "\n",
    "            if epoch % 25 == 0:\n",
    "                print('Epoch {}, lr {}'.format(\n",
    "                    epoch, optimizer.param_groups[0]['lr']))\n",
    "                print(f\"Epoch {epoch}: Train Tele {batch_loss:.3f}, Test Tele {batch_test:.3f}\")\n",
    "                print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "    #     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    else:\n",
    "        print('dataset too small to train')\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4ff058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_condAE(model, dt_train, dt_test,label,cur_directory,path, loss = new_loss,num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    size_train = len(dt_train)\n",
    "    size_test = len(dt_test)\n",
    "    if size_train !=0 and size_test != 0:\n",
    "\n",
    "        print(f'TRAINING INFO:')\n",
    "        print(f'Total Dataset Size: {size_train + size_test}')\n",
    "        mean = 0\n",
    "        std = 1\n",
    "\n",
    "        train_loc =dt_train\n",
    "        test_loc = dt_test\n",
    "\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "        test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "        train_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        train = dt_train[:,0:48]\n",
    "        test = dt_test[:,0:48]\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train=train[torch.randperm(train.size()[0])]\n",
    "        test=test[torch.randperm(test.size()[0])]\n",
    "        train_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "        #fine-tune autoencoder\n",
    "        #batch 500\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "\n",
    "        all_test_losses = []\n",
    "        all_train_losses = []\n",
    "        # train\n",
    "        running_loss = float(\"inf\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i, data_list in enumerate(train_d1_flat):\n",
    "\n",
    "                model.train()\n",
    "                data = data_list[0]\n",
    "                v_pred = model(data)\n",
    "                batch_loss = loss(data[:,0:48], v_pred,epoch,mean,std)\n",
    "                all_train_losses.append(batch_loss.item())\n",
    "                losses.append(batch_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            scheduler.step(batch_loss)\n",
    "\n",
    "            data_test = test\n",
    "            model.eval()\n",
    "            test_pred = model(data_test)\n",
    "            batch_test = loss(data_test[:,0:48], test_pred,epoch,mean,std)\n",
    "            all_test_losses.append(batch_test.item())\n",
    "            running_loss = np.mean(losses)\n",
    "            runningtest_loss = batch_test.item()\n",
    "\n",
    "            if epoch % 25 == 0:\n",
    "                print('Epoch {}, lr {}'.format(\n",
    "                    epoch, optimizer.param_groups[0]['lr']))\n",
    "                print(f\"Epoch {epoch}: Train {AE_MSE(v_pred, data):.3f}, Test {AE_MSE(test_pred, data_test):.3f}\")\n",
    "                print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "    #     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    else:\n",
    "        print('dataset too small to train')\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a91e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass a path to the folde\n",
    "def train_condAEs(data_path,\n",
    "                  dt_files,\n",
    "                 batch = 100,\n",
    "                 override = True,\n",
    "                 model_params = [],\n",
    "                 loss = new_loss,\n",
    "                 tele = False,\n",
    "                 path_1 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 path_2 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 path_3 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 append_ReLU = False,\n",
    "                 max_dt_size = 40000,\n",
    "                 epochs = 100,\n",
    "                 lr = 0.001,\n",
    "                 dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\",\n",
    "                                           time.gmtime()),\n",
    "                 path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "        for f in dt_files:\n",
    "            os.mkdir(os.path.join(path, f))\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for d in dt_files:\n",
    "        low = torch.load(os.path.join(data_path,d+'_low'))\n",
    "        high = torch.load(os.path.join(data_path,d+'_high'))\n",
    "        data.append(cond_storage(d,low,high, max_dt_size))\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        params = []\n",
    "        if override == True: \n",
    "            if model_params == []:\n",
    "                if int(d.name[8]) == 1:\n",
    "                    mt = torch.load(path_1)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d.name[8]) == 2:\n",
    "                    mt = torch.load(path_2)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load(path_3)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                \n",
    "                params.append(Naive_DAE(model_params))\n",
    "            params.append(d)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            \n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "        else:\n",
    "            if not d[1] in trained_models:\n",
    "                \n",
    "                if model_params == []:\n",
    "                    if int(d[1][8]) == 1:\n",
    "                        params.append(torch.load(path_1))\n",
    "                    elif int(d[1][8]) == 2:\n",
    "                        params.append(torch.load(path_2))\n",
    "                    else:\n",
    "                        params.append(torch.load(path_3))\n",
    "                else:\n",
    "                    params.append(Naive_DAE(model_params))\n",
    "                \n",
    "                params.append(dt)\n",
    "                \n",
    "                #Limiting size of training/testing to limit runtime\n",
    "                if len(dt) <= max_dt_size:\n",
    "                    params.append(int(0.8*len(dt)))\n",
    "                    params.append(int(0.19*len(dt)))\n",
    "                else:\n",
    "                    params.append(int(0.8*max_dt_size))\n",
    "                    params.append(int(0.19*max_dt_size))\n",
    "                \n",
    "                params.append(d[1])\n",
    "                params.append(dir_label)\n",
    "                \n",
    "                train_info.append(params)\n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        i = i+1\n",
    "        print(f'TRAINING MODEL {params[1].name}: {i}/{len(train_info)}')\n",
    "        #Training model\n",
    "        if tele:\n",
    "            print('LOW')\n",
    "            train_condAE_tele(params[0].to(device),params[1].low_train.to(device),params[1].low_test.to(device),params[1].name+'_low',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs,loss = loss, lr = lr)\n",
    "            print('HIGH')\n",
    "            train_condAE_tele(params[0].to(device),params[1].high_train.to(device),params[1].high_test.to(device),params[1].name+'_high',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs, loss = loss,lr = lr)\n",
    "\n",
    "        else:\n",
    "            print('LOW')\n",
    "\n",
    "            train_condAE(params[0].to(device),params[1].low_train.to(device),params[1].low_test.to(device),params[1].name+'_low',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs,loss = loss, lr = lr)\n",
    "            print('HIGH')   \n",
    "            train_condAE(params[0].to(device),params[1].high_train.to(device),params[1].high_test.to(device),params[1].name+'_high',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs, loss = loss,lr = lr)\n",
    "\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04528dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cond_storage():\n",
    "    \n",
    "    def __init__(self,name, low, high,max_dt_size):\n",
    "        self.name = name\n",
    "        \n",
    "        if len(low) < max_dt_size:\n",
    "            self.low_train = low[0: int(0.8*len(low))]\n",
    "            self.low_test = low[int(0.8*len(low)):]\n",
    "        else:\n",
    "            self.low_train = low[0: int(0.8*max_dt_size)]\n",
    "            self.low_test = low[int(0.8*max_dt_size):]\n",
    "        \n",
    "        if len(high) < max_dt_size:\n",
    "            self.high_train = low[0: int(0.8*len(high))]\n",
    "            self.high_test = low[int(0.8*len(high)):]\n",
    "        else:\n",
    "            self.high_train = low[0: int(0.8*max_dt_size)]\n",
    "            self.high_test = low[int(0.8*max_dt_size):]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cfdf17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_telescope(data,\n",
    "                 batch_size = 100,\n",
    "                 override = True,\n",
    "                 model_params = [],\n",
    "                 path_1 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 path_2 = 'models/MIP_dt_1_450_250_greater_2',\n",
    "                 path_3 = 'models/MIP_dt_1_450_250_greater_3',\n",
    "                 append_ReLU = False,\n",
    "                 max_dt_size = 40000,\n",
    "                 epochs = 100,\n",
    "                 lr = 0.001,\n",
    "                 dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\",\n",
    "                                           time.gmtime()),\n",
    "                 path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    \n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        params = []\n",
    "        if override == True: \n",
    "            if model_params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    mt = torch.load(path_1)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load(path_2)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load(path_3)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                \n",
    "                params.append(Naive_DAE(model_params))\n",
    "            params.append(dt)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "        else:\n",
    "            if not d[1] in trained_models:\n",
    "                if model_params == []:\n",
    "                    if int(d[1][8]) == 1:\n",
    "                        params.append(torch.load('models/dt_1_greater_0_450_250_100_dif_2_tele'))\n",
    "                    elif int(d[1][8]) == 2:\n",
    "                        params.append(torch.load('models/dt_2_greater_0_450_250_100_dif_2_tele'))\n",
    "                    else:\n",
    "                        params.append(torch.load('models/dt_3_greater_0_450_250_100_dif_2_tele'))\n",
    "                else:\n",
    "                    \n",
    "                    params.append(Naive_DAE(model_params))\n",
    "                params.append(dt)\n",
    "\n",
    "                #Limiting size of training/testing to limit runtime\n",
    "                if len(dt) <= max_dt_size:\n",
    "                    params.append(int(0.8*len(dt)))\n",
    "                    params.append(int(0.19*len(dt)))\n",
    "                else:\n",
    "                    params.append(int(0.8*max_dt_size))\n",
    "                    params.append(int(0.19*max_dt_size))\n",
    "\n",
    "                params.append(d[1])\n",
    "                params.append(dir_label)\n",
    "                train_info.append(params)\n",
    "\n",
    "    for params in train_info:\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        #Training model\n",
    "        train_model(params[0].to(device),params[1].to(device),params[2],params[3],params[4],params[5],path,batch = batch_size, num_epochs = epochs, lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def retrain_models_telescope(data,mse,mse_threshold = 5, batch = 100,override = True, model_params = [],max_dt_size = 40000,epochs = 100,lr = 0.001, dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()), path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    mse_to_retrain = []\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        cur_mse = mse[i]\n",
    "        i = i + 1\n",
    "        params = []\n",
    "        if cur_mse > mse_threshold:\n",
    "            mse_to_retrain.append(cur_mse)\n",
    "            print(f'Model {d[1]} has Telescope of {cur_mse} and is being retrained')\n",
    "            if params == 'retrain':\n",
    "                params.append(torch.load(os.path.join(path,d[1])))\n",
    "            elif params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    print('models/high_layer_tele')\n",
    "                    mt = torch.load('models/dt_1_greater_0_450_250_100_dif_2_tele')\n",
    "#                     mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load('models/dt_2_greater_0_450_250_100_dif_2_tele')\n",
    "#                     mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load('models/dt_3_greater_0_450_250_100_dif_2_tele')\n",
    "#                     mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                print(f'created model {d[1]} with {model_params}')\n",
    "                params.append(Naive_DAE(model_params))\n",
    "               \n",
    "\n",
    "            params.append(dt)\n",
    "\n",
    "                #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "            \n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        cur_mse = mse_to_retrain[i]\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        i=i+1\n",
    "        #Training mode\n",
    "        retrain_model_telescope(params[0].to(device),\n",
    "                      cur_mse,\n",
    "                      params[1].to(device),\n",
    "                      params[2],\n",
    "                      params[3],\n",
    "                      params[4],\n",
    "                      params[5],\n",
    "                      path,batch = batch,\n",
    "                      num_epochs = epochs,\n",
    "                      lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def retrain_model_telescope(model, mse,dt, size_train, size_test,label,cur_directory,path,num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    print(f'TRAINING INFO:')\n",
    "    print(f'Total Dataset Size: {size_train + size_test}')\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    train_loc = dt[0:size_train]\n",
    "    test_loc = dt[-size_test:]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "    test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "    train_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train = dt[0:size_train,0:48]\n",
    "    test = dt[-size_test:,0:48]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train=train[torch.randperm(train.size()[0])]\n",
    "    test=test[torch.randperm(test.size()[0])]\n",
    "    train_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "    #fine-tune autoencoder\n",
    "    #batch 500\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "    \n",
    "    all_test_losses = []\n",
    "    all_train_losses = []\n",
    "    # train\n",
    "    running_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for i, data_list in enumerate(train_d1_flat):\n",
    "            \n",
    "            model.train()\n",
    "            data = data_list[0]\n",
    "            v_pred = model(data)\n",
    "\n",
    "            \n",
    "            batch_loss = torch.mean(telescopeMSE2(data[:,0:48].cpu(), v_pred.cpu()))\n",
    "            all_train_losses.append(batch_loss.item())\n",
    "            losses.append(batch_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        scheduler.step(batch_loss)\n",
    "        \n",
    "        data_test = test\n",
    "        model.eval()\n",
    "        test_pred = model(data_test)\n",
    "        batch_test = torch.mean(telescopeMSE2(data_test[:,0:48].cpu(), test_pred.cpu()))\n",
    "        all_test_losses.append(batch_test.item())\n",
    "        running_loss = np.mean(losses)\n",
    "        runningtest_loss = batch_test.item()\n",
    "        if epoch % 25 == 0:\n",
    "            print('Epoch {}, lr {}'.format(\n",
    "                epoch, optimizer.param_groups[0]['lr']))\n",
    "            print(f\"Epoch {epoch}: Train {AE_MSE(v_pred, data):.3f}, Test {AE_MSE(test_pred, data_test):.3f}\")\n",
    "            print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "#     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "    if running_loss*3.5280**2 < mse:\n",
    "        torch.save(model,f'{path}/{label}')\n",
    "    else:\n",
    "        print(f'Telescope of {running_loss*3.5280**2} was larger than initial of {mse} and was not saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
