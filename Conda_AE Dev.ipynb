{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ecf028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from Naive_DAE import Naive_DAE,Dropout_DAE\n",
    "import AE_Stats\n",
    "from load_data_fn import load_data,load_data_no_filter\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import importlib\n",
    "from telescope_torch import *\n",
    "import time\n",
    "from losses import new_loss,AE_MSE, combo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83d1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_models(data,mse,mse_threshold = 5, batch = 100,override = True, model_params = [],max_dt_size = 40000,epochs = 100,lr = 0.001, dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()), path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    mse_to_retrain = []\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        cur_mse = mse[i]\n",
    "        i = i + 1\n",
    "        params = []\n",
    "        if cur_mse > mse_threshold:\n",
    "            mse_to_retrain.append(cur_mse)\n",
    "            print(f'Model {d[1]} has MSE of {cur_mse} and is being retrained')\n",
    "            if params == 'retrain':\n",
    "                params.append(torch.load(os.path.join(path,d[1])))\n",
    "            elif params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    mt = torch.load('models/dt_1_greater_0_450_250_100_dif_2')\n",
    "                    mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load('models/dt_2_greater_0_450_250_100_dif_2')\n",
    "                    mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load('models/dt_3_greater_0_450_250_100_dif_2')\n",
    "                    mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                print(f'created model {d[1]} with {model_params}')\n",
    "                params.append(Naive_DAE(model_params))\n",
    "               \n",
    "\n",
    "            params.append(dt)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "            \n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        cur_mse = mse_to_retrain[i]\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        i=i+1\n",
    "        #Training mode\n",
    "        retrain_model(params[0].to(device),\n",
    "                      cur_mse,\n",
    "                      params[1].to(device),\n",
    "                      params[2],\n",
    "                      params[3],\n",
    "                      params[4],\n",
    "                      params[5],\n",
    "                      path,batch = batch,\n",
    "                      num_epochs = epochs,\n",
    "                      lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d9f6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_models_specified(data,\n",
    "                             mse,\n",
    "                             specified,\n",
    "                             loss = new_loss,\n",
    "                             batch = 100,\n",
    "                             override = True,\n",
    "                             model_params = [],\n",
    "                             append_ReLU = False,\n",
    "                             max_dt_size = 40000,\n",
    "                             epochs = 100,\n",
    "                             path_1 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                             path_2 = 'models/MIP_dt_1_450_250_greater_2',\n",
    "                             path_3 = 'models/MIP_dt_1_450_250_greater_3',\n",
    "                             lr = 0.001,\n",
    "                             dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()),\n",
    "                             path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    \n",
    "    mse_to_retrain = []\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        cur_mse = mse[i]\n",
    "        i = i + 1\n",
    "        params = []\n",
    "        if d[1] in specified:\n",
    "            mse_to_retrain.append(cur_mse)\n",
    "            print(f'Model {d[1]} is being retrained')\n",
    "            if model_params == 'retrain':\n",
    "                params.append(torch.load(os.path.join(path,d[1])))\n",
    "            elif model_params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    mt = torch.load(path_1)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                    \n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load(path_2)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load(path_3)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                print(f'created model {d[1]} with {model_params}')\n",
    "                params.append(Naive_DAE(model_params))\n",
    "               \n",
    "\n",
    "            params.append(dt)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            \n",
    "            train_info.append(params)\n",
    "            \n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        cur_mse = mse_to_retrain[i]\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        i=i+1\n",
    "        #Training mode\n",
    "        retrain_model(params[0].to(device),\n",
    "                      cur_mse,\n",
    "                      params[1].to(device),\n",
    "                      params[2],\n",
    "                      params[3],\n",
    "                      params[4],\n",
    "                      params[5],\n",
    "                      path,batch = batch,\n",
    "                      num_epochs = epochs,\n",
    "                      loss = loss,\n",
    "                      lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3773f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(model, mse,dt, size_train, size_test,label,cur_directory,path, loss = new_loss, num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    print(f'TRAINING INFO:')\n",
    "    print(f'Total Dataset Size: {size_train + size_test}')\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    train_loc = dt[0:size_train]\n",
    "    test_loc = dt[-size_test:]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "    test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "    train_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train = dt[0:size_train,0:48]\n",
    "    test = dt[-size_test:,0:48]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train=train[torch.randperm(train.size()[0])]\n",
    "    test=test[torch.randperm(test.size()[0])]\n",
    "    train_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "    #fine-tune autoencoder\n",
    "    #batch 500\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "    \n",
    "    all_test_losses = []\n",
    "    all_train_losses = []\n",
    "    # train\n",
    "    running_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for i, data_list in enumerate(train_d1_flat):\n",
    "            \n",
    "            model.train()\n",
    "            data = data_list[0]\n",
    "            v_pred = model(data)\n",
    "\n",
    "            \n",
    "            batch_loss = torch.mean(telescopeMSE2(data[:,0:48], v_pred))\n",
    "            all_train_losses.append(batch_loss.item())\n",
    "            losses.append(batch_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        scheduler.step(batch_loss)\n",
    "        \n",
    "        data_test = test\n",
    "        model.eval()\n",
    "        test_pred = model(data_test)\n",
    "        batch_test = torch.mean(telescopeMSE2(data_test[:,0:48], test_pred))\n",
    "        all_test_losses.append(batch_test.item())\n",
    "        running_loss = np.mean(losses)\n",
    "        runningtest_loss = batch_test.item()\n",
    "        if epoch % 25 == 0:\n",
    "            print('Epoch {}, lr {}'.format(\n",
    "                epoch, optimizer.param_groups[0]['lr']))\n",
    "            print(f\"Epoch {epoch}: Train {AE_MSE(v_pred, data):.3f}, Test {AE_MSE(test_pred, data_test):.3f}\")\n",
    "            print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "#     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "    if AE_MSE(test_pred, data_test)*3.5280**2 < mse:\n",
    "        torch.save(model,f'{path}/{label}')\n",
    "    else:\n",
    "        print(f'MSE of {AE_MSE(test_pred, data_test)*3.5280**2} was larger than initial of {mse} and was not saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ee082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a188a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide path to data\n",
    "data_path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/MIT_TTbar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33392868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd66c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(data_path)\n",
    "dt_files = []\n",
    "for f in all_files:\n",
    "    if f[0:7] == 'dt_norm' and (f[-3:] != 'low' and f[-4:] !='high'):\n",
    "        dt_files.append(f)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cacbcd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL dt_norm_3_36: 1/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26476\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.125, Test 0.128\n",
      "MSE NON-NORMALIZED: Train MSE 1.691, Test MSE 1.692\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.122, Test 0.124\n",
      "MSE NON-NORMALIZED: Train MSE 1.636, Test MSE 1.636\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.119, Test 0.121\n",
      "MSE NON-NORMALIZED: Train MSE 1.591, Test MSE 1.592\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.116, Test 0.118\n",
      "MSE NON-NORMALIZED: Train MSE 1.555, Test MSE 1.556\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26476\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.137, Test 0.116\n",
      "MSE NON-NORMALIZED: Train MSE 1.527, Test MSE 1.526\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.135, Test 0.115\n",
      "MSE NON-NORMALIZED: Train MSE 1.516, Test MSE 1.515\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.135, Test 0.115\n",
      "MSE NON-NORMALIZED: Train MSE 1.506, Test MSE 1.505\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.134, Test 0.114\n",
      "MSE NON-NORMALIZED: Train MSE 1.497, Test MSE 1.496\n",
      "TRAINING MODEL dt_norm_3_30: 2/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 61386\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.152, Test 0.144\n",
      "MSE NON-NORMALIZED: Train MSE 1.884, Test MSE 1.878\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.148, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 1.821, Test MSE 1.816\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.145, Test 0.136\n",
      "MSE NON-NORMALIZED: Train MSE 1.781, Test MSE 1.775\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.143, Test 0.134\n",
      "MSE NON-NORMALIZED: Train MSE 1.754, Test MSE 1.748\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 61386\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.170, Test 0.133\n",
      "MSE NON-NORMALIZED: Train MSE 1.741, Test MSE 1.731\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.169, Test 0.132\n",
      "MSE NON-NORMALIZED: Train MSE 1.733, Test MSE 1.723\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.169, Test 0.131\n",
      "MSE NON-NORMALIZED: Train MSE 1.727, Test MSE 1.717\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.168, Test 0.131\n",
      "MSE NON-NORMALIZED: Train MSE 1.722, Test MSE 1.712\n",
      "TRAINING MODEL dt_norm_3_31: 3/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 65367\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.118, Test 0.149\n",
      "MSE NON-NORMALIZED: Train MSE 1.940, Test MSE 1.940\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.113, Test 0.143\n",
      "MSE NON-NORMALIZED: Train MSE 1.869, Test MSE 1.869\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.110, Test 0.139\n",
      "MSE NON-NORMALIZED: Train MSE 1.819, Test MSE 1.819\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.107, Test 0.137\n",
      "MSE NON-NORMALIZED: Train MSE 1.784, Test MSE 1.785\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 65367\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.100, Test 0.135\n",
      "MSE NON-NORMALIZED: Train MSE 1.758, Test MSE 1.759\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.099, Test 0.134\n",
      "MSE NON-NORMALIZED: Train MSE 1.745, Test MSE 1.746\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.098, Test 0.133\n",
      "MSE NON-NORMALIZED: Train MSE 1.735, Test MSE 1.736\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.097, Test 0.132\n",
      "MSE NON-NORMALIZED: Train MSE 1.727, Test MSE 1.728\n",
      "TRAINING MODEL dt_norm_3_33: 4/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 56147\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.091, Test 0.110\n",
      "MSE NON-NORMALIZED: Train MSE 1.444, Test MSE 1.443\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.085, Test 0.105\n",
      "MSE NON-NORMALIZED: Train MSE 1.373, Test MSE 1.374\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.082, Test 0.102\n",
      "MSE NON-NORMALIZED: Train MSE 1.329, Test MSE 1.330\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.080, Test 0.100\n",
      "MSE NON-NORMALIZED: Train MSE 1.301, Test MSE 1.302\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 56147\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.084, Test 0.099\n",
      "MSE NON-NORMALIZED: Train MSE 1.283, Test MSE 1.285\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.084, Test 0.098\n",
      "MSE NON-NORMALIZED: Train MSE 1.276, Test MSE 1.278\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.083, Test 0.098\n",
      "MSE NON-NORMALIZED: Train MSE 1.271, Test MSE 1.272\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.083, Test 0.097\n",
      "MSE NON-NORMALIZED: Train MSE 1.267, Test MSE 1.268\n",
      "TRAINING MODEL dt_norm_3_35: 5/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 40899\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.083, Test 0.078\n",
      "MSE NON-NORMALIZED: Train MSE 1.024, Test MSE 1.023\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.080, Test 0.074\n",
      "MSE NON-NORMALIZED: Train MSE 0.980, Test MSE 0.979\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.079, Test 0.072\n",
      "MSE NON-NORMALIZED: Train MSE 0.951, Test MSE 0.950\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.077, Test 0.071\n",
      "MSE NON-NORMALIZED: Train MSE 0.932, Test MSE 0.931\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 40899\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.077, Test 0.070\n",
      "MSE NON-NORMALIZED: Train MSE 0.918, Test MSE 0.917\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.077, Test 0.070\n",
      "MSE NON-NORMALIZED: Train MSE 0.913, Test MSE 0.912\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.076, Test 0.069\n",
      "MSE NON-NORMALIZED: Train MSE 0.909, Test MSE 0.908\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.076, Test 0.069\n",
      "MSE NON-NORMALIZED: Train MSE 0.906, Test MSE 0.905\n",
      "TRAINING MODEL dt_norm_3_37: 6/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26873\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.085, Test 0.084\n",
      "MSE NON-NORMALIZED: Train MSE 1.103, Test MSE 1.100\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.082, Test 0.081\n",
      "MSE NON-NORMALIZED: Train MSE 1.062, Test MSE 1.060\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.080, Test 0.079\n",
      "MSE NON-NORMALIZED: Train MSE 1.032, Test MSE 1.030\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.079, Test 0.077\n",
      "MSE NON-NORMALIZED: Train MSE 1.009, Test MSE 1.007\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 26873\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.063, Test 0.076\n",
      "MSE NON-NORMALIZED: Train MSE 0.990, Test MSE 0.992\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.062, Test 0.076\n",
      "MSE NON-NORMALIZED: Train MSE 0.983, Test MSE 0.985\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.061, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 0.977, Test MSE 0.979\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.061, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 0.972, Test MSE 0.973\n",
      "TRAINING MODEL dt_norm_3_27: 7/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 23273\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.434, Test 0.477\n",
      "MSE NON-NORMALIZED: Train MSE 6.295, Test MSE 6.302\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.416, Test 0.457\n",
      "MSE NON-NORMALIZED: Train MSE 6.036, Test MSE 6.043\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.405, Test 0.443\n",
      "MSE NON-NORMALIZED: Train MSE 5.857, Test MSE 5.865\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.397, Test 0.433\n",
      "MSE NON-NORMALIZED: Train MSE 5.723, Test MSE 5.730\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 23273\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.418, Test 0.424\n",
      "MSE NON-NORMALIZED: Train MSE 5.622, Test MSE 5.618\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.413, Test 0.419\n",
      "MSE NON-NORMALIZED: Train MSE 5.561, Test MSE 5.557\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.408, Test 0.415\n",
      "MSE NON-NORMALIZED: Train MSE 5.506, Test MSE 5.502\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.403, Test 0.411\n",
      "MSE NON-NORMALIZED: Train MSE 5.457, Test MSE 5.453\n",
      "TRAINING MODEL dt_norm_3_29: 8/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 113120\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.111, Test 0.096\n",
      "MSE NON-NORMALIZED: Train MSE 1.245, Test MSE 1.245\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.107, Test 0.093\n",
      "MSE NON-NORMALIZED: Train MSE 1.210, Test MSE 1.210\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.105, Test 0.091\n",
      "MSE NON-NORMALIZED: Train MSE 1.186, Test MSE 1.185\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.103, Test 0.090\n",
      "MSE NON-NORMALIZED: Train MSE 1.169, Test MSE 1.169\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 113120\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.073, Test 0.089\n",
      "MSE NON-NORMALIZED: Train MSE 1.159, Test MSE 1.159\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.072, Test 0.089\n",
      "MSE NON-NORMALIZED: Train MSE 1.153, Test MSE 1.153\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.072, Test 0.089\n",
      "MSE NON-NORMALIZED: Train MSE 1.150, Test MSE 1.150\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.072, Test 0.088\n",
      "MSE NON-NORMALIZED: Train MSE 1.149, Test MSE 1.149\n",
      "TRAINING MODEL dt_norm_3_32: 9/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 52950\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.114, Test 0.114\n",
      "MSE NON-NORMALIZED: Train MSE 1.490, Test MSE 1.489\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.111, Test 0.111\n",
      "MSE NON-NORMALIZED: Train MSE 1.449, Test MSE 1.449\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.109, Test 0.109\n",
      "MSE NON-NORMALIZED: Train MSE 1.423, Test MSE 1.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.108, Test 0.108\n",
      "MSE NON-NORMALIZED: Train MSE 1.407, Test MSE 1.407\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 52950\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.147, Test 0.107\n",
      "MSE NON-NORMALIZED: Train MSE 1.397, Test MSE 1.397\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.147, Test 0.107\n",
      "MSE NON-NORMALIZED: Train MSE 1.392, Test MSE 1.392\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.146, Test 0.107\n",
      "MSE NON-NORMALIZED: Train MSE 1.388, Test MSE 1.388\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.146, Test 0.106\n",
      "MSE NON-NORMALIZED: Train MSE 1.385, Test MSE 1.385\n",
      "TRAINING MODEL dt_norm_3_34: 10/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49695\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.090, Test 0.090\n",
      "MSE NON-NORMALIZED: Train MSE 1.187, Test MSE 1.184\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.086, Test 0.086\n",
      "MSE NON-NORMALIZED: Train MSE 1.134, Test MSE 1.132\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.083, Test 0.083\n",
      "MSE NON-NORMALIZED: Train MSE 1.098, Test MSE 1.096\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.082, Test 0.082\n",
      "MSE NON-NORMALIZED: Train MSE 1.074, Test MSE 1.072\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 49695\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.061, Test 0.081\n",
      "MSE NON-NORMALIZED: Train MSE 1.060, Test MSE 1.058\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.061, Test 0.080\n",
      "MSE NON-NORMALIZED: Train MSE 1.055, Test MSE 1.053\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.061, Test 0.080\n",
      "MSE NON-NORMALIZED: Train MSE 1.051, Test MSE 1.049\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.061, Test 0.080\n",
      "MSE NON-NORMALIZED: Train MSE 1.049, Test MSE 1.047\n",
      "TRAINING MODEL dt_norm_3_38: 11/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 23653\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.090, Test 0.086\n",
      "MSE NON-NORMALIZED: Train MSE 1.125, Test MSE 1.123\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.085, Test 0.082\n",
      "MSE NON-NORMALIZED: Train MSE 1.074, Test MSE 1.072\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.082, Test 0.079\n",
      "MSE NON-NORMALIZED: Train MSE 1.034, Test MSE 1.032\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.079, Test 0.077\n",
      "MSE NON-NORMALIZED: Train MSE 1.001, Test MSE 0.999\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 23653\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 0.083, Test 0.075\n",
      "MSE NON-NORMALIZED: Train MSE 0.974, Test MSE 0.975\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 0.083, Test 0.074\n",
      "MSE NON-NORMALIZED: Train MSE 0.965, Test MSE 0.966\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 0.082, Test 0.074\n",
      "MSE NON-NORMALIZED: Train MSE 0.957, Test MSE 0.958\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 0.082, Test 0.073\n",
      "MSE NON-NORMALIZED: Train MSE 0.949, Test MSE 0.950\n",
      "TRAINING MODEL dt_norm_1_1: 12/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 37266\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 5.868, Test 5.319\n",
      "MSE NON-NORMALIZED: Train MSE 73.307, Test MSE 73.234\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 5.773, Test 5.187\n",
      "MSE NON-NORMALIZED: Train MSE 71.812, Test MSE 71.764\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 5.733, Test 5.121\n",
      "MSE NON-NORMALIZED: Train MSE 70.921, Test MSE 70.877\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 5.702, Test 5.072\n",
      "MSE NON-NORMALIZED: Train MSE 70.249, Test MSE 70.204\n",
      "HIGH\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 37266\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 4.885, Test 5.031\n",
      "MSE NON-NORMALIZED: Train MSE 69.673, Test MSE 69.627\n",
      "Epoch 25, lr 8.5e-09\n",
      "Epoch 25: Train 4.863, Test 5.007\n",
      "MSE NON-NORMALIZED: Train MSE 69.322, Test MSE 69.275\n",
      "Epoch 50, lr 8.5e-09\n",
      "Epoch 50: Train 4.842, Test 4.984\n",
      "MSE NON-NORMALIZED: Train MSE 68.993, Test MSE 68.946\n",
      "Epoch 75, lr 8.5e-09\n",
      "Epoch 75: Train 4.822, Test 4.963\n",
      "MSE NON-NORMALIZED: Train MSE 68.680, Test MSE 68.632\n",
      "TRAINING MODEL dt_norm_1_3: 13/82\n",
      "LOW\n",
      "TRAINING INFO:\n",
      "Total Dataset Size: 372884\n",
      "Epoch 0, lr 8.5e-09\n",
      "Epoch 0: Train 6.972, Test 7.128\n",
      "MSE NON-NORMALIZED: Train MSE 99.207, Test MSE 99.064\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 586.00 MiB (GPU 0; 11.91 GiB total capacity; 4.52 GiB already allocated; 555.38 MiB free; 6.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_condAEs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdt_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43moverride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcombo_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/combo_loss_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath_2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/combo_loss_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m              \u001b[49m\u001b[43mpath_3\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/combo_loss_all\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdir_label\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcondAE_mean_split_450_250_combo_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtele\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8.5e-9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 106\u001b[0m, in \u001b[0;36mtrain_condAEs\u001b[0;34m(data_path, dt_files, batch, override, model_params, loss, tele, path_1, path_2, path_3, append_ReLU, max_dt_size, epochs, lr, dir_label, path)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLOW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtrain_condAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_low\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHIGH\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n\u001b[1;32m    108\u001b[0m     train_condAE(params[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mhigh_train\u001b[38;5;241m.\u001b[39mto(device),params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mhigh_test\u001b[38;5;241m.\u001b[39mto(device),params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_high\u001b[39m\u001b[38;5;124m'\u001b[39m,os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,params[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname),path, batch \u001b[38;5;241m=\u001b[39m batch, num_epochs \u001b[38;5;241m=\u001b[39m epochs, loss \u001b[38;5;241m=\u001b[39m loss,lr \u001b[38;5;241m=\u001b[39m lr)\n",
      "Cell \u001b[0;32mIn [10], line 77\u001b[0m, in \u001b[0;36mtrain_condAE\u001b[0;34m(model, dt_train, dt_test, label, cur_directory, path, loss, num_epochs, lr, batch)\u001b[0m\n\u001b[1;32m     75\u001b[0m data_test \u001b[38;5;241m=\u001b[39m test\n\u001b[1;32m     76\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 77\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m batch_test \u001b[38;5;241m=\u001b[39m loss(data_test[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m48\u001b[39m], test_pred,epoch,mean,std)\n\u001b[1;32m     79\u001b[0m all_test_losses\u001b[38;5;241m.\u001b[39mappend(batch_test\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/Naive_DAE.py:187\u001b[0m, in \u001b[0;36mNaive_DAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward step\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m x_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m--> 187\u001b[0m x_reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_reconstructed\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/Notebooks/AE_Dev/Naive_DAE.py:226\u001b[0m, in \u001b[0;36mNaive_DAE.decode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode the representation x\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders:\n\u001b[0;32m--> 226\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/nswood/mambaforge/envs/myenv/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 586.00 MiB (GPU 0; 11.91 GiB total capacity; 4.52 GiB already allocated; 555.38 MiB free; 6.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_condAEs(data_path,\n",
    "              dt_files,\n",
    "               model_params = [],\n",
    "              override = False,\n",
    "              loss = combo_loss, \n",
    "              path_1 = 'models/combo_loss_all',\n",
    "              path_2 = 'models/combo_loss_all',\n",
    "              path_3 = 'models/combo_loss_all',\n",
    "             dir_label ='condAE_mean_split_450_250_combo_loss',\n",
    "             epochs= 100,\n",
    "             tele = False,\n",
    "             batch = 100, \n",
    "             lr = 8.5e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb213d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_condAE_tele(model, dt_train, dt_test,label,cur_directory,path, loss = new_loss,num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    size_train = len(dt_train)\n",
    "    size_test = len(dt_test)\n",
    "    if size_train !=0 and size_test != 0:\n",
    "\n",
    "        print(f'TRAINING INFO:')\n",
    "        print(f'Total Dataset Size: {size_train + size_test}')\n",
    "        mean = 0\n",
    "        std = 1\n",
    "\n",
    "        train_loc =dt_train\n",
    "        test_loc = dt_test\n",
    "\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "        test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "        train_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        train = dt_train[:,0:48]\n",
    "        test = dt_test[:,0:48]\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train=train[torch.randperm(train.size()[0])]\n",
    "        test=test[torch.randperm(test.size()[0])]\n",
    "        train_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "        #fine-tune autoencoder\n",
    "        #batch 500\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "\n",
    "        all_test_losses = []\n",
    "        all_train_losses = []\n",
    "        # train\n",
    "        running_loss = float(\"inf\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i, data_list in enumerate(train_d1_flat):\n",
    "\n",
    "                model.train()\n",
    "                data = data_list[0]\n",
    "                v_pred = model(data)\n",
    "                batch_loss = torch.mean(telescopeMSE2(data[:,0:48], v_pred))\n",
    "                all_train_losses.append(batch_loss.item())\n",
    "                losses.append(batch_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            scheduler.step(batch_loss)\n",
    "\n",
    "            data_test = test\n",
    "            model.eval()\n",
    "            test_pred = model(data_test)\n",
    "            batch_test = torch.mean(telescopeMSE2(data_test[:,0:48], test_pred))\n",
    "            all_test_losses.append(batch_test.item())\n",
    "            running_loss = np.mean(losses)\n",
    "            runningtest_loss = batch_test.item()\n",
    "\n",
    "            if epoch % 25 == 0:\n",
    "                print('Epoch {}, lr {}'.format(\n",
    "                    epoch, optimizer.param_groups[0]['lr']))\n",
    "                print(f\"Epoch {epoch}: Train Tele {batch_loss:.3f}, Test Tele {batch_test:.3f}\")\n",
    "                print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "    #     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    else:\n",
    "        print('dataset too small to train')\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9767402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_condAE(model, dt_train, dt_test,label,cur_directory,path, loss = new_loss,num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    size_train = len(dt_train)\n",
    "    size_test = len(dt_test)\n",
    "    if size_train !=0 and size_test != 0:\n",
    "\n",
    "        print(f'TRAINING INFO:')\n",
    "        print(f'Total Dataset Size: {size_train + size_test}')\n",
    "        mean = 0\n",
    "        std = 1\n",
    "\n",
    "        train_loc =dt_train\n",
    "        test_loc = dt_test\n",
    "\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "        test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "        train_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_loc_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test_loc)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        train = dt_train[:,0:48]\n",
    "        test = dt_test[:,0:48]\n",
    "        # train = torch.vstack([train,largest[0:1000]] )\n",
    "        train=train[torch.randperm(train.size()[0])]\n",
    "        test=test[torch.randperm(test.size()[0])]\n",
    "        train_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(train)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        test_d1_flat = DataLoader(\n",
    "            TensorDataset(torch.Tensor(test)),\n",
    "            batch_size=batch,\n",
    "            shuffle=False\n",
    "        )\n",
    "        #fine-tune autoencoder\n",
    "        #batch 500\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "\n",
    "        all_test_losses = []\n",
    "        all_train_losses = []\n",
    "        # train\n",
    "        running_loss = float(\"inf\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            for i, data_list in enumerate(train_d1_flat):\n",
    "\n",
    "                model.train()\n",
    "                data = data_list[0]\n",
    "                v_pred = model(data)\n",
    "                batch_loss = loss(data[:,0:48], v_pred,epoch,mean,std)\n",
    "                all_train_losses.append(batch_loss.item())\n",
    "                losses.append(batch_loss.item())\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            scheduler.step(batch_loss)\n",
    "\n",
    "            data_test = test\n",
    "            model.eval()\n",
    "            test_pred = model(data_test)\n",
    "            batch_test = loss(data_test[:,0:48], test_pred,epoch,mean,std)\n",
    "            all_test_losses.append(batch_test.item())\n",
    "            running_loss = np.mean(losses)\n",
    "            runningtest_loss = batch_test.item()\n",
    "\n",
    "            if epoch % 25 == 0:\n",
    "                print('Epoch {}, lr {}'.format(\n",
    "                    epoch, optimizer.param_groups[0]['lr']))\n",
    "                print(f\"Epoch {epoch}: Train {AE_MSE(v_pred, data):.3f}, Test {AE_MSE(test_pred, data_test):.3f}\")\n",
    "                print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "    #     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    else:\n",
    "        print('dataset too small to train')\n",
    "        torch.save(model,f'{cur_directory}/{label}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb123c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass a path to the folde\n",
    "def train_condAEs(data_path,\n",
    "                  dt_files,\n",
    "                 batch = 100,\n",
    "                 override = True,\n",
    "                 model_params = [],\n",
    "                 loss = new_loss,\n",
    "                 tele = False,\n",
    "                 path_1 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 path_2 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 path_3 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 append_ReLU = False,\n",
    "                 max_dt_size = 40000,\n",
    "                 epochs = 100,\n",
    "                 lr = 0.001,\n",
    "                 dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\",\n",
    "                                           time.gmtime()),\n",
    "                 path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "        for f in dt_files:\n",
    "            os.mkdir(os.path.join(path, f))\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for d in dt_files:\n",
    "        low = torch.load(os.path.join(data_path,d+'_low'))\n",
    "        high = torch.load(os.path.join(data_path,d+'_high'))\n",
    "        data.append(cond_storage(d,low,high, max_dt_size))\n",
    "\n",
    "    for d in data:\n",
    "        \n",
    "        params = []\n",
    "        if override == True: \n",
    "            if model_params == []:\n",
    "                if int(d.name[10:]) < 12:\n",
    "                    mt = torch.load(path_1)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d.name[10:]) >= 12:\n",
    "                    mt = torch.load(path_2)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load(path_3)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                \n",
    "                params.append(Naive_DAE(model_params))\n",
    "            params.append(d)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            \n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "        else:\n",
    "            if not d[1] in trained_models:\n",
    "                \n",
    "                if model_params == []:\n",
    "                    if int(d[1][8]) == 1:\n",
    "                        params.append(torch.load(path_1))\n",
    "                    elif int(d[1][8]) == 2:\n",
    "                        params.append(torch.load(path_2))\n",
    "                    else:\n",
    "                        params.append(torch.load(path_3))\n",
    "                else:\n",
    "                    params.append(Naive_DAE(model_params))\n",
    "                \n",
    "                params.append(dt)\n",
    "                \n",
    "                #Limiting size of training/testing to limit runtime\n",
    "                if len(dt) <= max_dt_size:\n",
    "                    params.append(int(0.8*len(dt)))\n",
    "                    params.append(int(0.19*len(dt)))\n",
    "                else:\n",
    "                    params.append(int(0.8*max_dt_size))\n",
    "                    params.append(int(0.19*max_dt_size))\n",
    "                \n",
    "                params.append(d[1])\n",
    "                params.append(dir_label)\n",
    "                \n",
    "                train_info.append(params)\n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        i = i+1\n",
    "        print(f'TRAINING MODEL {params[1].name}: {i}/{len(train_info)}')\n",
    "        #Training model\n",
    "        if tele:\n",
    "            print('LOW')\n",
    "            train_condAE_tele(params[0].to(device),params[1].low_train.to(device),params[1].low_test.to(device),params[1].name+'_low',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs,loss = loss, lr = lr)\n",
    "            print('HIGH')\n",
    "            train_condAE_tele(params[0].to(device),params[1].high_train.to(device),params[1].high_test.to(device),params[1].name+'_high',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs, loss = loss,lr = lr)\n",
    "\n",
    "        else:\n",
    "            print('LOW')\n",
    "\n",
    "            train_condAE(params[0].to(device),params[1].low_train.to(device),params[1].low_test.to(device),params[1].name+'_low',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs,loss = loss, lr = lr)\n",
    "            print('HIGH')   \n",
    "            train_condAE(params[0].to(device),params[1].high_train.to(device),params[1].high_test.to(device),params[1].name+'_high',os.path.join(path,params[1].name),path, batch = batch, num_epochs = epochs, loss = loss,lr = lr)\n",
    "\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e009f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cond_storage():\n",
    "    \n",
    "    def __init__(self,name, low, high,max_dt_size):\n",
    "        self.name = name\n",
    "        \n",
    "        if len(low) < max_dt_size:\n",
    "            self.low_train = low[0: int(0.8*len(low))]\n",
    "            self.low_test = low[int(0.8*len(low)):]\n",
    "        else:\n",
    "            self.low_train = low[0: int(0.8*max_dt_size)]\n",
    "            self.low_test = low[int(0.8*max_dt_size):]\n",
    "        \n",
    "        if len(high) < max_dt_size:\n",
    "            self.high_train = low[0: int(0.8*len(high))]\n",
    "            self.high_test = low[int(0.8*len(high)):]\n",
    "        else:\n",
    "            self.high_train = low[0: int(0.8*max_dt_size)]\n",
    "            self.high_test = low[int(0.8*max_dt_size):]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_telescope(data,\n",
    "                 batch_size = 100,\n",
    "                 override = True,\n",
    "                 model_params = [],\n",
    "                 path_1 = 'models/MIP_dt_1_450_250_greater_1',\n",
    "                 path_2 = 'models/MIP_dt_1_450_250_greater_2',\n",
    "                 path_3 = 'models/MIP_dt_1_450_250_greater_3',\n",
    "                 append_ReLU = False,\n",
    "                 max_dt_size = 40000,\n",
    "                 epochs = 100,\n",
    "                 lr = 0.001,\n",
    "                 dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\",\n",
    "                                           time.gmtime()),\n",
    "                 path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    \n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        params = []\n",
    "        if override == True: \n",
    "            if model_params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    mt = torch.load(path_1)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load(path_2)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load(path_3)\n",
    "                    if append_ReLU:\n",
    "                        mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                \n",
    "                params.append(Naive_DAE(model_params))\n",
    "            params.append(dt)\n",
    "\n",
    "            #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "        else:\n",
    "            if not d[1] in trained_models:\n",
    "                if model_params == []:\n",
    "                    if int(d[1][8]) == 1:\n",
    "                        params.append(torch.load('models/dt_1_greater_0_450_250_100_dif_2_tele'))\n",
    "                    elif int(d[1][8]) == 2:\n",
    "                        params.append(torch.load('models/dt_2_greater_0_450_250_100_dif_2_tele'))\n",
    "                    else:\n",
    "                        params.append(torch.load('models/dt_3_greater_0_450_250_100_dif_2_tele'))\n",
    "                else:\n",
    "                    \n",
    "                    params.append(Naive_DAE(model_params))\n",
    "                params.append(dt)\n",
    "\n",
    "                #Limiting size of training/testing to limit runtime\n",
    "                if len(dt) <= max_dt_size:\n",
    "                    params.append(int(0.8*len(dt)))\n",
    "                    params.append(int(0.19*len(dt)))\n",
    "                else:\n",
    "                    params.append(int(0.8*max_dt_size))\n",
    "                    params.append(int(0.19*max_dt_size))\n",
    "\n",
    "                params.append(d[1])\n",
    "                params.append(dir_label)\n",
    "                train_info.append(params)\n",
    "\n",
    "    for params in train_info:\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        #Training model\n",
    "        train_model(params[0].to(device),params[1].to(device),params[2],params[3],params[4],params[5],path,batch = batch_size, num_epochs = epochs, lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def retrain_models_telescope(data,mse,mse_threshold = 5, batch = 100,override = True, model_params = [],max_dt_size = 40000,epochs = 100,lr = 0.001, dir_label = time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime()), path = '/uscms/home/nswood/nobackup/Notebooks/AE_Dev/models/batched_models'):\n",
    "    path = os.path.join(path, dir_label)\n",
    "    \n",
    "    if not (os.path.exists(path) and os.path.isdir(path)):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    trained_models = os.listdir(path)\n",
    "    mse_to_retrain = []\n",
    "    device = 'cuda'\n",
    "    train_info = []\n",
    "    i = 0\n",
    "    for d in data:\n",
    "        dt = d[0]\n",
    "        cur_mse = mse[i]\n",
    "        i = i + 1\n",
    "        params = []\n",
    "        if cur_mse > mse_threshold:\n",
    "            mse_to_retrain.append(cur_mse)\n",
    "            print(f'Model {d[1]} has Telescope of {cur_mse} and is being retrained')\n",
    "            if params == 'retrain':\n",
    "                params.append(torch.load(os.path.join(path,d[1])))\n",
    "            elif params == []:\n",
    "                if int(d[1][8]) == 1:\n",
    "                    print('models/high_layer_tele')\n",
    "                    mt = torch.load('models/dt_1_greater_0_450_250_100_dif_2_tele')\n",
    "#                     mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                elif int(d[1][8]) == 2:\n",
    "                    mt = torch.load('models/dt_2_greater_0_450_250_100_dif_2_tele')\n",
    "#                     mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "                else:\n",
    "                    mt = torch.load('models/dt_3_greater_0_450_250_100_dif_2_tele')\n",
    "#                     mt.decoders.add_module('ReLU',nn.ReLU())\n",
    "                    params.append(mt)\n",
    "            else:\n",
    "                print(f'created model {d[1]} with {model_params}')\n",
    "                params.append(Naive_DAE(model_params))\n",
    "               \n",
    "\n",
    "            params.append(dt)\n",
    "\n",
    "                #Limiting size of training/testing to limit runtime\n",
    "            if len(dt) <= max_dt_size:\n",
    "                params.append(int(0.8*len(dt)))\n",
    "                params.append(int(0.19*len(dt)))\n",
    "            else:\n",
    "                params.append(int(0.8*max_dt_size))\n",
    "                params.append(int(0.19*max_dt_size))\n",
    "\n",
    "            params.append(d[1])\n",
    "            params.append(dir_label)\n",
    "            train_info.append(params)\n",
    "            \n",
    "    i = 0\n",
    "    for params in train_info:\n",
    "        cur_mse = mse_to_retrain[i]\n",
    "        print(f'TRAINING MODEL {params[4]}')\n",
    "        i=i+1\n",
    "        #Training mode\n",
    "        retrain_model_telescope(params[0].to(device),\n",
    "                      cur_mse,\n",
    "                      params[1].to(device),\n",
    "                      params[2],\n",
    "                      params[3],\n",
    "                      params[4],\n",
    "                      params[5],\n",
    "                      path,batch = batch,\n",
    "                      num_epochs = epochs,\n",
    "                      lr = lr)\n",
    "        #clear cuda after training each model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def retrain_model_telescope(model, mse,dt, size_train, size_test,label,cur_directory,path,num_epochs = 200, lr = 0.001,batch = 100):\n",
    "    print(f'TRAINING INFO:')\n",
    "    print(f'Total Dataset Size: {size_train + size_test}')\n",
    "    mean = 0\n",
    "    std = 1\n",
    "    train_loc = dt[0:size_train]\n",
    "    test_loc = dt[-size_test:]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train_loc=train_loc[torch.randperm(train_loc.size()[0])]\n",
    "    test_loc=test_loc[torch.randperm(test_loc.size()[0])]\n",
    "    train_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_loc_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test_loc)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train = dt[0:size_train,0:48]\n",
    "    test = dt[-size_test:,0:48]\n",
    "    # train = torch.vstack([train,largest[0:1000]] )\n",
    "    train=train[torch.randperm(train.size()[0])]\n",
    "    test=test[torch.randperm(test.size()[0])]\n",
    "    train_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(train)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    test_d1_flat = DataLoader(\n",
    "        TensorDataset(torch.Tensor(test)),\n",
    "        batch_size=batch,\n",
    "        shuffle=False\n",
    "    )\n",
    "    #fine-tune autoencoder\n",
    "    #batch 500\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr,weight_decay=5e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor = 0.5)\n",
    "    \n",
    "    all_test_losses = []\n",
    "    all_train_losses = []\n",
    "    # train\n",
    "    running_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        losses = []\n",
    "\n",
    "        for i, data_list in enumerate(train_d1_flat):\n",
    "            \n",
    "            model.train()\n",
    "            data = data_list[0]\n",
    "            v_pred = model(data)\n",
    "\n",
    "            \n",
    "            batch_loss = torch.mean(telescopeMSE2(data[:,0:48].cpu(), v_pred.cpu()))\n",
    "            all_train_losses.append(batch_loss.item())\n",
    "            losses.append(batch_loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        scheduler.step(batch_loss)\n",
    "        \n",
    "        data_test = test\n",
    "        model.eval()\n",
    "        test_pred = model(data_test)\n",
    "        batch_test = torch.mean(telescopeMSE2(data_test[:,0:48].cpu(), test_pred.cpu()))\n",
    "        all_test_losses.append(batch_test.item())\n",
    "        running_loss = np.mean(losses)\n",
    "        runningtest_loss = batch_test.item()\n",
    "        if epoch % 25 == 0:\n",
    "            print('Epoch {}, lr {}'.format(\n",
    "                epoch, optimizer.param_groups[0]['lr']))\n",
    "            print(f\"Epoch {epoch}: Train {AE_MSE(v_pred, data):.3f}, Test {AE_MSE(test_pred, data_test):.3f}\")\n",
    "            print(f\"MSE NON-NORMALIZED: Train MSE {running_loss*3.5280**2:.3f}, Test MSE {runningtest_loss*3.5280**2:.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "#     AE_Stats.gen_all_stats(model(dt[-size_test:,0:48]).cpu(),dt[-size_test:,0:48].cpu(),dt[-size_test:].cpu())\n",
    "    if running_loss*3.5280**2 < mse:\n",
    "        torch.save(model,f'{path}/{label}')\n",
    "    else:\n",
    "        print(f'Telescope of {running_loss*3.5280**2} was larger than initial of {mse} and was not saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
